## nftl和ftl

###  修订记录
| 修订说明 | 日期 | 作者 | 额外说明 |
| --- |
| 初版 | 2018/04/10 | 员清观 |  |


## 1 FTL的

**闪存文件系统**<br>
闪存文件系统比FTL具有更高的性能，一般用于固定的、非插拔的NAND闪存管理．当前针对闪存文件系统的研究主要包括文件系统日志管理、文件系统快速初始化、文件系统崩溃恢复、垃圾收集和页面分配技术等。
大多数闪存文件系统都借鉴了日志文件系统的设计思想，实践证明，这种设计非常适用于闪存存储设备。在传统的、不是基于日志的文件系统中，为了提高文件系统的效率，通常采用缓冲区来进行数据的读写。但是，这种方式也存在一定的隐患，比如，当发生断电事故时，如果缓冲区中的数据还没有全部刷新写入到磁盘，就会引起部分数据的丢失，而且这种数据丢失是无法恢复的。缓冲区中可能会包含一些关键数据，比如文件系统的管理信息，丢失这些数据会导致文件系统数据组织的混乱，甚至引起文件系统的崩溃。日志文件系统可以很好地克服上述缺陷，它会在磁盘中维护一个日志文件，所有数据更新都以追加的方式写入到日志中，每个更新操作都对应于日志中的一条记录。日志文件的尾部包含了文件系统中的最新数据。当系统发生断电事故后，只需要扫描日志就可以恢复还原文件。


最新设计的算法中, 64sect中1个sect用于标识是否free，这主要是为了加速启动过程．否则，每次启动都需要读取所有的sect才能清除哪一个chunk已经不包含有效信息．

空间换时间．可以考虑禁止这样的方式，那么每次启动之后，自动update_sectmap然后drop掉冗余的chunk就好．然后，相应的算法也需要调整一下．add_free()的时候，就不需要erase并新写入一个sect了，而是在alloc_free()函数中erase

FTL算法的进一步开发应该移植到pc平台更加方便一些，除了调试更加方便快捷，还可以更加方便的制造各种测试场景。

硬件基础知识和坏块管理:

  https://blog.csdn.net/xjzlq/article/details/24369701
  http://dblab.xmu.edu.cn/post/4548/

**FTL的一些前世今生的历史**<br>
读史可以明智。在SLC时代，与其一起成长的软件技术有YAFFS/YAFFS2，它是第一个专门为Nand设计的，并具有WL和POR功能的嵌入式文件系统；以及JFFS/JFFS2，建立可以挂载Nand Driver的MTD设备之上的日志文件系统。随着容量的增加，MLC技术的到来，JFFS2的继往开来者UBIFS/UBI也随之到来，还有与其形成竞争关系的LogFS。尽管百花齐放，它们都是建立在MTD之上将FTL理念和文件系统概念融合的产物，或者说FTL是它们剥离的产物，但它们的局限性也很明显，越来越难以匹配Nand技术的发展

**固态存储相对于传统磁盘特点**<br>
１．异处更新(out-of-place Update)；２．P/E次数有限制；３．随机读性能更好；４．读、写速度不一致

回归到文件系统上，趋于智能设备发展的大势，传统文件系统并非碌碌无为，也在不断针对非机械存储盘增加新特性。以ext4来说，增加Trim功能，支持大Block Size（>4KB,<=64KB）;再如Btrfs针对SSD进行专门的优化，其COW（Copy-on-Write）技术避免了对同一个物理单元反复写动作，尽量减少SSD内部FTL GC的发生，同时优化底层块分配，尽量分配到大Size（比如2MB）的连续块进行操作，方便了SSD内部FTL对数据的组织，进而优化性能。甚至还诞生了如F2fs这种专门为Nand设计的新型文件系统，充分考虑了Nand特性，充分考虑了FTL及Flash几何结构参数感知，特别优化了移动设备的大量小文件读写操作，扩大了随机写区域，使得在小文件操作上有得天独厚的优势，并支持后台Cleaning及Greedy和转移数据最少的Section选择算法，支持Multi-head Logs用于动态和静态Hot与Cold数据分离等等需要应用在Nand Flash上的考量问题。

尽管如此多的针对性优化和新特性，但是大多建立在具有FTL的设备之上。因此FTL作为Nand底层核心技术不容小觑。该项技术也成为很多厂商不愿外人所知的技术信息，其所具有的挑战伴随着Nand发展一直存在，厂商也因此可以用新技术Nand相关设备抢占市场；而Linux Mainline却因为MTD停滞不前，也给了学术界生存的土壤，对此方面研究的Paper层峦叠起，可见FTL的优劣直接影响到Nand设备的寿命、可靠性和性能上的优劣。本系列文章将从概念上对FTL那些事娓娓道来

### 1.1 文件系统常用命令

**e2fsck检查extx文件系统**<br>
e2fsck用于检查ext 2/ext 3/ext 4系列文件系统。对于使用日志的ext 3和ext 4文件系统，如果系统在没有任何错误的情况下被不干净地关闭，通常在日志中重播提交的事务之后，文件系统应该标记为干净。因此，对于使用日志的文件系统，e2fsck通常会重播日志并退出，除非它的超级块表明需要进一步检查。
  https://blog.csdn.net/wj78080458/article/details/83623846?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~top_click~default-1-83623846.nonecase&utm_term=e2fsck&spm=1000.2123.3001.4430
`e2fsck  [-pacnyrdfkvtDFV]  [-b superblock]  [-B  blocksize] [-l|-L bad_blocks_file ]  [-C fd]  [-j external-journal]  [-E extended_options ]  device`

!!! 可以考虑重写这个命令,实现恢复点feature.

常用选项,其他看上述网页就好:
- -b superblock
- -n 只读方式打开文件系统,对所有问题回答no.允许非交互地使用e2fsck。此选项不能与-p或-y选项同时指定。我感觉和nftl部分的恢复点feature很搭,e2fsck返回错误的话就使用最新的恢复点
- -B blocksize
- -a | -p 自动修复
- -f 强制修复
- -E discard/nodiscard

e2fsck可以返回以下值：0，没有错误。1，文件系统错误更正。2，文件系统错误更正，系统应该重启。4，文件系统错误没有更正。8，操作错误。16，语法错误。32，用户取消了操作。128，共享库错误.是否可以用"-n"选项启动,如果返回不是"0/1",就认定需要使用恢复点呢?

### 1.2 技术模块
**技术模块**<br>
- Interface Adapter：在内部FTL中主要关联eMMC/SCSI/SATA/PCIe/NVMe等接口，而在外部FTL中主要关联Linux Block Device。
- Address Translation：俗称Mapping机制，负责逻辑地址到物理地址的映射，接下来的很多技术模块都以该机制为核心进行。众所周知，Nand Flash具有写时擦除以及后来顺序写的”顽疾”，因此不得不异地更新。逆映射关系存放在Flash页的冗余空间中，需要通过完整扫描Flash来建立映射关系，所以速度慢
- 增量空间的供给（OP）
- Garbage Collection：简称GC，上述”顽疾”的延续，回收异地更新产生的脏数据所占空间的回收工作。
- Wear Leveling：简称WL，磨损均衡，避免某一个Nand Block很快坏去，使所有Block的PE Cycle均衡发展。包括冷/热数据的交换处理。
- Power off Recovery：简称POR，掉电恢复，因为某些人为或自然外力的原因导致数据没有成功写入到Nand中，决定系统要从可能出现ECC或具有Log系统功能中恢复到掉电前的安全状态。
- Parallelization and Load Balancing：在Multi-Die甚至Multi-Plane的Nand系统中，在并行处理中确保性能的完美展现。
- Cache Manager：Cache不仅可以存放用户数据，也可以存放FTL Metadata，对系统的整体性能有着天然的优势。
- Error Handler：处理读写操作中遇到的Fatal Error或ECC Error状况，以及Bad Block或Weak Block的管理。
- 加密和压缩，以及重复数据的删除。

**FTL优劣的因素**<br>
综合来看就是时空效率和稳定性的Trade-off，空间是指RAM的使用量和Flash的利用率，时间是指读写操作性能，稳定性主要体现在寿命和容错上，具体来说分为五大因素：
- Translation Performance：涉及到Cache管理和Mapping Table/Tree的组织方式，导致的查找效率高低，这将直接影响到读写速度。
- RAM Overhead：涉及到SRAM/DRAM的资源占用多少，以及Cache管理不当导致命中失效进而频繁地从Flash处更新Mapping Table镜像信息，进而影响性能。特别是SRAM和熊猫一样不像柴米油盐随意。
- Block Utilization：这不仅关系到Block在Erase之前写入的数据量，也关系到另一个叫Write Amplification（简称WA，写放大）的概念，一方面因为设计不当导致的每笔用户数据写入会产生额外开销，另外一方面由GC或WL等FTL行为导致数据迁移所致。
- Garbage Collection Performance：在写过程中触发GC会导致写速度产生波动现象，特别是频繁的GC或者每次GC数据量较大。
- Fault Tolerance：错误产生的类型有很多，比如Bad Block、突然掉电、Bitflips、UECC等，如何能容错不至于变成”砖头”变的尤为重要。

**映射方式**<br>
（1）页映射，每个逻辑扇区被映射到一个对应的物理扇区
（2）块映射，物理块内的物理扇区的偏移量与逻辑块中的逻辑扇区的偏移量相同。
（3）混合映射

### 1.2 地址映射方式
一般分为块映射 页映射 混合映射
(1)Block Mapping
将块映射地址分为两部分：块地址和块内偏移地址(offset)。映射表只保存块的地址，逻辑块和物理块的块内偏移地址对应相同。因此，在块映射机制中，只保存块的映射关系。
优点：映射表size小，对于读操作非常简单。
缺点：不利于random write,random write会产生大量的数据update。如果一个block中的某个page需要update，需要将这个块的数据复制到另一个物理block中，然后擦除old block,将更新的数据写入新分配的物理block中。导致flash block的空间利用率非常低，且会引起频繁的erase block 和valid page copying。

(2)Page Mapping
LPN->PPN,每个Page对应一个mapping项。
优点：GC时能明显减少valid page copy
缺点：mapping size 大，当flash容量增大时，mapping table 会迅速增大。

(3)Hybrid Mapping
综合page mapping和block mapping的优点，在block mapping的基础上，对频繁更新的data采用page mapping。由于通常只有小部分数据的更新比较频繁，大部分数据的更新是很少的，所以用block mapping机制减少mapping table size，而对更新频繁的data采用page mapping。
Hybrid mapping把flash分为Data Block和Log Block。Data block用于存储数据，采用block mapping，Log Block用于存储对于Data block更新后的数据，即相当于写缓存来处理更新操作，Log Block采用page mapping。

## 2 infotm nftl
### 2.1 nftl_core.c

phy_chunk_node->valid_sects和phy_chunk_node->clear_sects在代码中，基本保持成对使用，如果不是，请确认是否错误．

```cpp
int infotm_nftl_initialize(infotm_nftl_blk_t *infotm_nftl_blk)

```
## 3 yaffs2文件系统

git clone git://www.aleph1.co.uk/yaffs2
网页：　https://yaffs.net/get-yaffs    http://www.aleph1.co.uk/

**检查点机制**<br>
检查点机制非常简单。一系列的数据会被写入到一个块的集合，这个块集合被标记为专门用来保存检查点数据，重要的运行时状态会被写入到数据流。并非所有的状态都需要被保存，只有需要用来重构运行时数据结构的状态才需要被保存。例如，文件元数据就不需要保存，因为，很容易通过―延迟加载‖进行加载。

检查点采用以下顺序存储数据：开始标记（包括检查点格式编号）、Yaffs_Device设备信息、块信息、厚片标记、对象（包括文件结构）、结尾标记、校验和。检查点的有效性是通过下面的机制来保证的：（1）采用任何可用的ECC机制来存储数据；（2）开始标记包含了一个检查点的版本信息，从而使得一个废弃的检查点（如果这个检查点代码发生了变化），不会被读取；（3）数据被写入到数据流，作为结构化的记录，每个记录都具有类型和大小；（4）当需要的时候，结束标记必须被读取；（5）在整个检查点数据集合中，需要维护两个校验和。



## 4 jffs2文件系统

对于JFFS2，需要特别指出的是，为了实现块的均衡磨损，garbageCollection()函数在选择要擦除的块时，会以99%的概率从脏块链表中选择一个块，以1%的概率从干净块链表中选择一个块
```cpp
function garbageCollection()
{
  产生一个随机数random；
  if（random mod 100不等于0）then
    从脏块链表中选中链表头部的块；
  else
    从干净块链表中选中链表头部的块；
    擦除选中的区块；
}
```

**JFFS2文件系统的不足**<br>
JFFS2文件系统的不足之处包括以下几个方面：
  具有较长的挂载时间：JFFS2的挂载过程需要从头到尾扫描闪存块，需要耗费较长的时间。
  磨损平衡具有随机性：JFFS2在选择要擦除的块时，会以99%的概率从脏块链表中选择一个块，以1%的概率从干净块链表中选择一个块。这种概率的方法，很难保证磨损的均衡性。在某些情况下，甚至可能造成对块的不必要的擦除操作，或者引起磨损平衡调整的不及时。
  可扩展性较差：JFFS2在挂载时，需要扫描整个闪存空间，因此，挂载时间和闪存空间大小成正比。另外，JFFS2对内存的占用量也和闪存块数成正比。在实际应用中，JFFS2最大能用在128MB的闪存上。此外，JFFS2在NAND闪存上可能无法取得很好的性能，主要原因包括：（1）NAND闪存设备通常要比NOR闪存设备的容量大许多，因此，在和闪存管理相关的数据结构方面，前者也会比后者大许多。而且，JFFS2的挂载时间和闪存空间大小成正比，因此，对于容量普遍较大的NAND闪存设备，JFFS2的扫描时间会较长，无法取得好的性能。（2）NAND闪存设备是以页为单位访问数据，如果只想访问页中的一部分数据，也必须顺序读取整个页的全部数据，而无法跳过不需要的数据。这就减慢了扫描和挂载的过程。



## 5 当前驱动中遗留问题和潜在优化

mx35_spinand.c中缓存定义的太大了： #define BUFSIZE (10 * 64 * 2048)
gigaflash 0xc8 0xf1 : 最后一个块0x400无法擦除和写入，读取的内容是块0x3ff的内容，会导致相关代码逻辑判断混乱。
spinand_read_id         函数两个芯片有所不同
spinand_program_page    函数的实现，应该是bug，再次确认下。
spinand_read_from_cache
spinand_program_data_to_cache

### 5.1 优化结果
小蚁： 4线： 升级时间从19s减少到10s

阔展平台：
  单线驱动最高支持的频率 42666666Hz ：
    [    1.113333] imap-ssp imap-ssp.0: SSP Target Frequency is: 50000000, Effective Frequency is 42666666Hz
  最高频率对应的时钟配置(clk.c)：
    DEV_CLK_INFO(SSP_CLK_SRC, 0, DPLL, 0, 17, DISABLE),
  四线驱动最高支持的频率：
    [    1.049999] [spimul] srcclk(90352941Hz), target(50000000Hz), actual(45176470Hz), clkdiv(2).
  最高频率对应的时钟配置(clk.c)：
    DEV_CLK_INFO(SSP_CLK_SRC, 0, DPLL, 0, 16, DISABLE),
