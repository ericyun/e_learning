# dma子系统

## 0 修订记录
| 修订说明 | 日期 | 作者 | 额外说明 |
| --- |
| 初版 | 2018/04/10 | 员清观 |  |


---
## 1 pl330模块
配置`DMA_LLI_MODE`，并且`struct pl330_thread`中`struct _pl330_req req[2]`如果可以定义多个请求，那么在`pl330_update()`函数中，是否可以逐个request切换，一直到处理完毕呢？这是一个request的链表．这个请求可以定义为可变个数的队列．

### 1.1 基础
1-6字节长度的变长微指令,每个channel一个PC,这些指令,存放在DDR,DMA控制器包含有cache优化访问.每个通道的cache如果能够保证一个循环体的话,性能应该不错,但是现在可以么?
关于cacheline,多长/多少,以及多个thread之间独立还是共享

**readqueue/writequeue/MFIFO**<br>
queue保存读写指令序列,MFIFO保存读写到的数据
pl022支持burst方式传输数据到dma么?
mem  -- mem 的拷贝, 使用的burst还是single,Always又是什么东西

`struct dma_pl330_dmac->struct dma_pl330_chan->struct dma_chan`用于资源初始化和管理，`struct pl330_dmac->struct pl330_thread->struct pl330_req`用于运行时通道管理.　驱动中添加`struct dma_pl330_desc`链接到通道`struct dma_pl330_chan`的`work_list`，然后启动通道时填充到`struct pl330_req`．

```cpp
struct pl330_info {
	struct device *dev;	 /* Owning device */
	unsigned mcbufsz;	 /* Size of MicroCode buffers for each channel. */
	void __iomem	*base; /* ioremap'ed address of PL330 registers. */
	void	*client_data; /* Client can freely use it. */
	void	*pl330_data;	 /* PL330 core data, Client must not touch it. */
	struct pl330_config	pcfg; /* Populated by the PL330 core driver during pl330_add */
	/* If the DMAC has some reset mechanism, then the client may want to provide pointer to the method. */
	void (*dmac_reset)(struct pl330_info *pi);
};
struct pl330_xfer {
	u32 src_addr, dst_addr, bytes, total_size, numf;
	u16 src_icg, dst_icg;
	struct pl330_xfer *next;
};
struct pl330_req {
	enum pl330_reqtype rqtype;
	unsigned peri:5; /* Index of peripheral for the xfer. */
	void *token; /* Unique token for this xfer, set by the client. */
	void (*xfer_cb)(void *token, enum pl330_op_err err); //Callback to be called after xfer
	struct pl330_reqcfg *cfg; /* If NULL, req will be done at last set parameters. */
	struct pl330_xfer *x; /* Pointer to first xfer in the request. */
	struct list_head rqd; /* Hook to attach to DMAC's list of reqs with due callback */
};
struct _pl330_req {
	u32 mc_bus;	void *mc_cpu;
	u32 mc_len; /* Number of bytes taken to setup MC for the req */
	struct pl330_req *r;
};
struct pl330_thread {
	u8 id; 	int ev;
	bool free; /* If the channel is not yet acquired by any client */
	struct pl330_dmac *dmac; /* Parent DMAC */
	struct _pl330_req req[2]; /* Only two at a time */
	unsigned lstenq; /* Index of the last enqueued request */
	int req_running; /* Index of the last submitted request or -1 if the DMA is stopped */
	enum _dma_mode mode; /* dma transfer mode */
};
struct pl330_dmac {
	spinlock_t		lock;
	struct list_head	req_done; /* Holds list of reqs with due callbacks */
	struct pl330_info	*pinfo; /* Pointer to platform specific stuff */
	int			events[32]; /* Maximum possible events/irqs */
	u32			mcode_bus; /* BUS address of MicroCode buffer */
	void			*mcode_cpu; /* CPU address of MicroCode buffer */
	struct pl330_thread	*channels; /* List of all Channel threads */
	struct pl330_thread	*manager;/* Pointer to the MANAGER thread */
	struct tasklet_struct	tasks;/* To handle bad news in interrupt */
	struct _pl330_tbd	dmac_tbd;
	enum pl330_dmac_state	state;/* State of DMAC operation */
};
struct dma_pl330_chan {
	struct tasklet_struct task;/* Schedule desc completion */
	struct dma_chan chan;/* DMA-Engine Channel */
	struct list_head work_list; /* List of to be xfered descriptors */
	struct dma_pl330_dmac *dmac;
	spinlock_t lock;
	enum pl330_byteswap byteswap;
	void *pl330_chid;
	int burst_sz, burst_len; /* the peripheral fifo width, and the number of burst */
	dma_addr_t fifo_addr;
	enum desc_type desc_type; /* for pl330 descritor operation type */
	/*for status flags*/
	bool pause;
	int len;
};
struct dma_pl330_dmac {
	struct pl330_info pif;
	struct dma_device ddma; /* DMA-Engine Device */
	struct list_head desc_pool;/* Pool of descriptors available for the DMAC's channels */
	spinlock_t pool_lock;/* To protect desc_pool manipulation */
	struct dma_pl330_chan *peripherals; //Peripheral channels connected to this DMAC
};
struct dma_pl330_desc {
	struct list_head node;/* To attach to a queue as child */
	struct dma_async_tx_descriptor txd;/* Descriptor for the DMA Engine API */
	struct pl330_xfer px[PL330_MAX_SG_NUM];/* Xfer for PL330 core */
	struct pl330_reqcfg rqcfg;
	struct pl330_req req;
	enum desc_status status;
	struct dma_pl330_chan *pchan; //The channel which currently holds this desc
};
```

### 1.1 初始化和probe
```cpp
//devices.c文件中设备的定义
static u8 imap_pl330_peri[] = {
	IMAPX_PCM0_TX,	IMAPX_PCM1_TX,	IMAPX_PCM0_RX,	IMAPX_PCM1_RX,	IMAPX_SSP0_TX,	IMAPX_SSP1_TX,	IMAPX_SSP0_RX,	IMAPX_SSP1_RX,
	IMAPX_AC97_TX,	IMAPX_AC97_RX,	IMAPX_UART0_TX,	IMAPX_UART1_TX,	IMAPX_UART2_TX,	IMAPX_UART3_TX,	IMAPX_UART0_RX,	IMAPX_UART1_RX,
	IMAPX_UART2_RX,	IMAPX_UART3_RX,	IMAPX_I2S_SLAVE_TX,	IMAPX_I2S_SLAVE_RX,	IMAPX_I2S_MASTER_TX,	IMAPX_I2S_MASTER_RX, IMAPX_TOUCHSCREEN_RX,
};
static struct dma_pl330_platdata imap_pl330_platdata = {
	.nr_valid_peri = 23, 	.peri_id = imap_pl330_peri, 	.mcbuf_sz = 512,
};
struct amba_device imap_dma_device = {
	.dev = {		.init_name = "dma-pl330",		.platform_data = &imap_pl330_platdata,		.coherent_dma_mask = ~0,	},
	.res = {		.start = IMAP_GDMA_BASE,		.end = IMAP_GDMA_BASE + IMAP_GDMA_SIZE - 1,		.flags = IORESOURCE_MEM,		},
	.irq = {GIC_DMA0_ID, GIC_DMA1_ID,	GIC_DMA2_ID, GIC_DMA3_ID,	GIC_DMA4_ID, GIC_DMA5_ID,	GIC_DMA6_ID, GIC_DMA7_ID,
		GIC_DMA8_ID, GIC_DMA9_ID,	GIC_DMA10_ID, GIC_DMA11_ID,	GIC_DMABT_ID, NO_IRQ},//GDMA支持最多6个通道，提供了12个中断，每个通道两个．应该是根据驱动申请的通道号，自动关联．
	.periphid = 0x00041330,
};

struct dma_pl330_chan {
	struct tasklet_struct task;/* Schedule desc completion */
	struct dma_chan chan;/* DMA-Engine Channel */
	struct list_head work_list;/* List of to be xfered descriptors */
	struct dma_pl330_dmac *dmac; /* Pointer to the DMAC that manages this channel, NULL if the channel is available to be acquired. As the parent, this DMAC also provides descriptors to the channel. */

	spinlock_t lock;/* To protect channel manipulation */
	enum pl330_byteswap byteswap;

	void *pl330_chid;/* Token of a hardware channel thread of PL330 DMAC NULL if the channel is available to be acquired. */

	/* For D-to-M and M-to-D channels */
	int burst_sz, burst_len; /* the peripheral fifo width, and the number of burst */
	dma_addr_t fifo_addr;
	enum desc_type desc_type;/* for pl330 descritor operation type */

	bool pause;
	int len;
};

struct pl330_thread {/* A DMAC Thread */
	u8 id;
	int ev;
	bool free;/* If the channel is not yet acquired by any client */
	struct pl330_dmac *dmac;/* Parent DMAC */
	struct _pl330_req req[2];/* Only two at a time */
	unsigned lstenq;/* Index of the last enqueued request */
	int req_running;/* Index of the last submitted request or -1 if the DMA is stopped */
	enum _dma_mode mode;/* dma transfer mode */
};


int pl330_update(const struct pl330_info *pi)
  void __iomem *regs = pi->base;
	struct pl330_dmac *pl330 = pi->pl330_data;
  val = readl(regs + FSM) & 0x1;//读取管理通道的fsm寄存器，判断是否需要复位管理通道
  if (val)
		pl330->dmac_tbd.reset_mngr = true;
  else
		pl330->dmac_tbd.reset_mngr = false;
  val = readl(regs + FSC) & ((1 << pi->pcfg.num_chan) - 1);  //读取普通通道的fsc寄存器，判断哪些通道需要复位
  pl330->dmac_tbd.reset_chan |= val;    i = -1;
  while (val && (++i < pi->pcfg.num_chan))  //循环复位相应的通道(线程)
    if (val & (1 << i))   _stop(&pl330->channels[i]);
  val = readl(regs + ES); //读取es寄存器，检查发生的事件
  if (pi->pcfg.num_events < 32 && val & ~((1 << pi->pcfg.num_events) - 1))
		pl330->dmac_tbd.reset_dmac = true; return;
  for (ev = 0; ev < pi->pcfg.num_events; ev++)
    if (   ( val & (1 << ev )) || ( ( ev == pl022_sync.pl022_ev_tx ) &&( pl022_sync.pl022_pages>0 ) &&(pl022_sync.pl022_pages%2 == 1) )  ) //循环处理已经发生的事件
      u32 inten = readl(regs + INTEN);
      if (inten & (1 << ev))  writel(1 << ev, regs + INTCLR); //如果中断使能并且事件发生，清除中断．
      id = pl330->events[ev]; thrd = &pl330->channels[id]; //获取事件对应的通道id和dmac通道处理线程
      active = thrd->req_running;//获取当前正在处理的请求
      rqdon//初始化pl330 dmac的struct tasklet_struct软中断处理，该操作主要处理一些dmac出现的错误e = thrd->req[active].r; //获取已经处理的请求
      mark_free(thrd, active); //将该dmac处理线程的请求标记为free
      _start(thrd);//继续该dmac处理线程
      list_add_tail(&rqdone->rqd, &pl330->req_done);//将该请求加入到已经处理完成链表
  list_for_each_entry_safe(rqdone, tmp, &pl330->req_done, rqd)//处理pl330 dmac中的已经完成处理链表的每个成员
    list_del(&rqdone->rqd);     _callback(rqdone, PL330_ERR_NONE);//调用请求中的回调处理该完成请求。
  if (pl330->dmac_tbd.reset_dmac|| pl330->dmac_tbd.reset_mngr	|| pl330->dmac_tbd.reset_chan)
    tasklet_schedule(&pl330->tasks);

void pl330_tasklet(unsigned long data)
  struct dma_pl330_chan *pch = (struct dma_pl330_chan *)data;
  struct dma_pl330_desc *desc, *_dt; //另外本函数会释放已经done的desc
  list_for_each_entry_safe(desc, _dt, &pch->work_list, node)
    if (desc->status == DONE)
			dma_cookie_complete(&desc->txd);		list_move_tail(&desc->node, &list);
  |--> fill_queue(pch);//void fill_queue(struct dma_pl330_chan *pch)
    struct dma_pl330_desc *desc;
    list_for_each_entry(desc, &pch->work_list, node)
      |--> ret = pl330_submit_req(pch->pl330_chid,&desc->req); //int pl330_submit_req(void *ch_id, struct pl330_req *r)
        if (_queue_full(thrd))  return -EAGAIN;
				idx = IS_FREE(&thrd->req[0]) ? 0 : 1;	xs.ccr = ccr;	xs.r = r;
				ret = _setup_req(1, thrd, idx, &xs);//预演
				thrd->lstenq = idx;
        |--> thrd->req[idx].mc_len = _setup_req(0, thrd, idx, &xs);//int _setup_req(unsigned dry_run, struct pl330_thread *thrd,unsigned index, struct _xfer_spec *pxs)，dry_run是演练的意思
          struct _pl330_req *req = &thrd->req[index];　　
          u8 *buf = req->mc_cpu;　//好像是微指令开始地址，因此setup函数就是填写微指令给dmac
          int off = 0; off += _emit_FLUSHP(dry_run, &buf[off], pxs->r->peri);
          off += _emit_MOV(dry_run, &buf[off], CCR, pxs->ccr);
          x = pxs->r->x;
          while (x)//从上层看，应该只包含一个，fill_px()函数中next==NULL.
            pxs->x = x; off += _setup_xfer(dry_run, &buf[off], pxs); x = x->next;
          off += _emit_SEV(dry_run, &buf[off], thrd->ev); off += _emit_END(dry_run, &buf[off]); return off;
				thrd->req[idx].r = r;
        return 0;
  |--> pl330_chan_ctrl(pch->pl330_chid, PL330_OP_START);//int pl330_chan_ctrl(void *ch_id, enum pl330_chan_op op)
    struct pl330_thread *thrd = ch_id;
    pl330 = thrd->dmac; active = thrd->req_running;
    switch (op)
      case PL330_OP_FLUSH:
      case PL330_OP_ABORT:
        _stop(thrd); ...
      case PL330_OP_START:
        _start(thrd); ...
  free_desc_list(&list);//void free_desc_list(struct list_head *list)
    list_for_each_entry(desc, list, node)
      pch = desc->pchan;
      dma_async_tx_callback callback = desc->txd.callback;
      param = desc->txd.callback_param;//驱动中调用dmaengine_prep_slave_sg()函数获取desc,然后直接赋值这两项参数
      callback(param);

void *pl330_request_channel(const struct pl330_info *pi)
  struct pl330_dmac *pl330 = pi->pl330_data;
  int chans = pi->pcfg.num_chan;
  for (i = 0; i < chans; i++)
    thrd = &pl330->channels[i];
    if ((thrd->free) && (!_manager_ns(thrd) || _chan_ns(pi, i)))
      thrd->ev = _alloc_event(thrd);
      thrd->free = false;			thrd->lstenq = 1;
      thrd->req[0].r = NULL;	mark_free(thrd, 0);			thrd->req[1].r = NULL;			mark_free(thrd, 1);
int pl330_add(struct pl330_info *pi)
  struct pl330_dmac *pl330 = kzalloc(sizeof(*pl330), GFP_KERNEL);
  pl330->pinfo = pi;  pi->pl330_data = pl330; spin_lock_init(&pl330->lock); INIT_LIST_HEAD(&pl330->req_done);
  |--> ret = dmac_alloc_resources(pl330);//int dmac_alloc_resources(struct pl330_dmac *pl330)
    struct pl330_info *pi = pl330->pinfo; int chans = pi->pcfg.num_chan;
    pl330->mcode_cpu = dma_alloc_coherent(pi->dev, chans * pi->mcbufsz, &pl330->mcode_bus, GFP_KERNEL);
    |--> ret = dmac_alloc_threads(pl330);//int dmac_alloc_threads(struct pl330_dmac *pl330)
      struct pl330_info *pi = pl330->pinfo; int chans = pi->pcfg.num_chan;
      pl330->channels = kzalloc((1 + chans) * sizeof(*thrd), GFP_KERNEL);
      for (i = 0; i < chans; i++)
        thrd = &pl330->channels[i];thrd->id = i;thrd->dmac = pl330;_reset_thread(thrd);thrd->free = true;
      thrd = &pl330->channels[chans];thrd->id = chans;thrd->dmac = pl330;thrd->free = false;pl330->manager = thrd;//MANAGER is indexed at the end
  tasklet_init(&pl330->tasks, pl330_dotask, (unsigned long) pl330);//pl330_dotask()函数主要处理一些dmac出现的错误

int add_desc(struct dma_pl330_dmac *pdmac, gfp_t flg, int count)
  struct dma_pl330_desc *desc = kmalloc(count * sizeof(*desc), flg);
  for (i = 0; i < count; i++)
    |--> _init_desc(&desc[i]);//void _init_desc(struct dma_pl330_desc *desc)
      desc->req.xfer_cb = dma_pl330_rqcb();//void dma_pl330_rqcb(void *token, enum pl330_op_err err)
          <--  void pl330_dotask(unsigned long data)
            <--  int pl330_update(const struct pl330_info *pi)//好像是启动传输：_start(thrd);
              <--  irqreturn_t pl330_irq_handler(int irq, void *data)//应该是fifo中数据满足传输要求，产生中断给dma
          tasklet_schedule(&pch->task);
      desc->txd.tx_submit = pl330_tx_submit();
    list_add_tail(&desc[i].node, &pdmac->desc_pool);

static int pl330_probe(struct amba_device *adev, const struct amba_id *id)
  set_secure_para(1); //module_power_on(SYSMGR_GDMA_BASE); writel(val, IO_ADDRESS(SYSMGR_GDMA_BASE) + 0x20);
	pdat = adev->dev.platform_data; //devices.c文件中定义的 imap_pl330_platdata
  struct clk *clk = clk_get_sys("dma-pl330","dma-pl330"); ret = clk_prepare_enable(clk);
  struct dma_pl330_dmac *pdmac = devm_kzalloc(&adev->dev, sizeof(*pdmac), GFP_KERNEL);
  struct pl330_info *pi = &pdmac->pif;  pi->base = devm_ioremap_resource(&adev->dev, res);
  amba_set_drvdata(adev, pdmac);//-->dev_set_drvdata(&adev->dev, pdmac)--> dev->p->driver_data = pdmac;
  while (adev->irq[j] != -1)
    ret = request_irq(adev->irq[j], pl330_irq_handler, 0,dev_name(&adev->dev), pi); j++;
  |--> ret = pl330_add(pi); //
  num_chan = max_t(int, pdat->nr_valid_peri, pi->pcfg.num_chan); //设备定义，23
  pdmac->peripherals = kzalloc(num_chan * sizeof(*pch), GFP_KERNEL);
  for (i = 0; i < num_chan; i++) //初始化所有的通道
    pch = &pdmac->peripherals[i];
    INIT_LIST_HEAD(&pch->work_list);
    spin_lock_init(&pch->lock);
    pch->pl330_chid = NULL;
    pch->chan.device = pd;
    pch->dmac = pdmac;
    list_add_tail(&pch->chan.device_node, &pd->channels);
  struct dma_device *pd;
  pd = &pdmac->ddma;  	INIT_LIST_HEAD(&pd->channels);
  //定义pl330所有的ops接口
  |--> pd->device_alloc_chan_resources = pl330_alloc_chan_resources();//申请通道 static int pl330_alloc_chan_resources(struct dma_chan *chan)
    struct dma_pl330_chan *pch = to_pchan(chan);
    dma_cookie_init(chan);  pch->desc_type = NORMAL;
    pch->pl330_chid = pl330_request_channel(&pdmac->pif);
    tasklet_init(&pch->task, pl330_tasklet, (unsigned long) pch);
	|--> pd->device_free_chan_resources = pl330_free_chan_resources();//释放通道 struct dma_pl330_chan *pch = to_pchan(chan);
    struct dma_pl330_chan *pch = to_pchan(chan);
    tasklet_kill(&pch->task);
    pl330_release_channel(pch->pl330_chid);
  |--> pd->device_prep_slave_sg = pl330_prep_slave_sg();//struct dma_async_tx_descriptor *pl330_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len, enum dma_transfer_direction direction, unsigned long flg, void *context)
    struct dma_pl330_chan *pch = to_pchan(chan);  addr = pch->fifo_addr;
    for_each_sg(sgl, sg, sg_len, i)　//每个sg一般对应一个page,对应到一个desc.
      |--> desc = pl330_get_desc(pch);//struct dma_pl330_desc *pl330_get_desc(struct dma_pl330_chan *pch)
        struct dma_pl330_desc *desc = pluck_desc(pdmac);//从desc_pool中摘下一个desc
        desc->pchan = pch;  desc->txd.cookie = 0; async_tx_ack(&desc->txd);
        desc->req.peri = peri_id ? pch->chan.chan_id : 0;
        desc->rqcfg.pcfg = &pch->dmac->pif.pcfg;
        dma_async_tx_descriptor_init(&desc->txd, &pch->chan);
      if (direction == DMA_MEM_TO_DEV)
  			desc->rqcfg.src_inc = 1; desc->rqcfg.dst_inc = 0; desc->req.rqtype = MEMTODEV;
  			|--> fill_px(&desc->px, addr, sg_dma_address(sg), sg_dma_len(sg), sg_dma_len(sg));//void fill_px(struct pl330_xfer *px, dma_addr_t dst, dma_addr_t src, size_t len, size_t total_size)
          px->next = NULL;	px->bytes = len;	px->dst_addr = dst;	px->src_addr = src;	px->total_size = total_size;
  		else
  			desc->rqcfg.src_inc = 0; desc->rqcfg.dst_inc = 1; desc->req.rqtype = DEVTOMEM;
  			fill_px(&desc->px, sg_dma_address(sg), addr, sg_dma_len(sg), sg_dma_len(sg));
      desc->rqcfg.brst_size = pch->burst_sz; desc->rqcfg.brst_len = pch->burst_len; desc->rqcfg.swap = pch->byteswap;
    desc->txd.flags = flg;    return &desc->txd;
  |--> pd->device_control = pl330_control();//int pl330_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd, unsigned long arg)
    struct dma_pl330_chan *pch = to_pchan(chan);  struct dma_pl330_dmac *pdmac = pch->dmac;
    switch (cmd)
      case DMA_TERMINATE_ALL:
        pl330_chan_ctrl(pch->pl330_chid, PL330_OP_FLUSH);
        list_for_each_entry_safe(desc, _dt, &pch->work_list , node)
          desc->status = DONE;  list_move_tail(&desc->node, &list);
        list_splice_tail_init(&list, &pdmac->desc_pool);
      case DMA_SLAVE_CONFIG:
        slave_config = (struct dma_slave_config *)arg;  pch->byteswap = slave_config->byteswap;
        //then update : pch->fifo_addr  pch->burst_sz  pch->burst_len
	|--> pd->device_issue_pending = pl330_issue_pending();//void pl330_issue_pending(struct dma_chan *chan)
    struct dma_pl330_chan *pch = to_pchan(chan);
		fill_lli_queue(pch); pl330_chan_ctrl(pch->pl330_chid, PL330_OP_START);
		if ((pch->desc_type == CYCLIC) || (pch->desc_type == LLI))	handle_cyclic_desc_list(&list);
		else free_desc_list(&list);
  |--> pd->device_dma_getposition = pl330_dma_getposition();//int pl330_dma_getposition(struct dma_chan *chan, dma_addr_t *src, dma_addr_t *dst)
    struct dma_pl330_chan *pch = to_pchan(chan);
    int ret = pl330_chan_status(pch->pl330_chid, &status);
    *src = status.src_addr; *dst = status.dst_addr;
	pd->device_prep_dma_memcpy = pl330_prep_dma_memcpy();
	pd->device_prep_dma_cyclic = pl330_prep_dma_cyclic();
  pd->device_prep_dma_lli = pl330_prep_dma_lli();
  pd->device_prep_slave_sg_pl022 = pl330_prep_slave_sg_pl022();
  pd->device_tx_status = pl330_tx_status();
  |--> ret = dma_async_device_register(pd/*struct dma_device *device*/);//为每个dma通道注册生成devices, /sys/devices/dma-pl330/dma/ 目录下,但是因为没有dmaengine功能支持,没有实际意义
    list_for_each_entry(chan, &device->channels, device_node) //struct dma_chan* chan;
      chan->local = alloc_percpu(typeof(*chan->local));
      chan->dev = kzalloc(sizeof(*chan->dev), GFP_KERNEL);
      chan->chan_id = chancnt++;
      rc = device_register(&chan->dev->device);
    device->chancnt = chancnt;
		list_add_tail_rcu(&device->global_node, &dma_device_list);
		dma_channel_rebalance();

static int pl330_remove(struct amba_device *adev)
  struct dma_pl330_dmac *pdmac = amba_get_drvdata(adev);
  dma_async_device_unregister(&pdmac->ddma);
  amba_set_drvdata(adev, NULL);
  list_for_each_entry_safe(pch, _p, &pdmac->ddma.channels, chan.device_node)
    list_del(&pch->chan.device_node);
    pl330_control(&pch->chan, DMA_TERMINATE_ALL, 0);
    |--> pl330_free_chan_resources(&pch->chan);//void pl330_free_chan_resources(struct dma_chan *chan)
      struct dma_pl330_chan *pch = to_pchan(chan);
      tasklet_kill(&pch->task);
      pl330_release_channel(pch->pl330_chid); pch->pl330_chid = NULL;
    struct pl330_info *pi = &pdmac->pif;
    |--> pl330_del(pi);//void pl330_del(struct pl330_info *pi)
      struct pl330_dmac *pl330 = pi->pl330_data;     pl330->state = UNINIT;
      tasklet_kill(&pl330->tasks);
      |--> dmac_free_resources(pl330);//void dmac_free_resources(struct pl330_dmac *pl330)
        struct pl330_info *pi = pl330->pinfo;
        |--> dmac_free_threads(pl330);//int dmac_free_threads(struct pl330_dmac *pl330)
          struct pl330_info *pi = pl330->pinfo;
          for (i = 0; i < chans; i++)
            thrd = &pl330->channels[i];
            |--> pl330_release_channel((void *)thrd);//停止线程,执行两个req回调,释放event
              struct pl330_thread *thrd = ch_id;  struct pl330_dmac *pl330 = thrd->dmac;
              _stop(thrd);
              _callback(thrd->req[1 - thrd->lstenq].r, PL330_ERR_ABORT);
              _callback(thrd->req[thrd->lstenq].r, PL330_ERR_ABORT);
              _free_event(thrd, thrd->ev);    //-->pl330->events[ev] = -1;
              thrd->free = true;
          kfree(pl330->channels);
        dma_free_coherent(pi->dev, chans * pi->mcbufsz,pl330->mcode_cpu, pl330->mcode_bus);
      kfree(pl330); pi->pl330_data = NULL;
    j = -1; while (adev->irq[++j] != -1)  free_irq(adev->irq[j], pi);

static struct amba_driver pl330_driver = {
	.drv = {		.owner = THIS_MODULE,		.name = "dma-pl330", }, .id_table = pl330_ids, 	.probe = pl330_probe, 	.remove = pl330_remove, };

static int __init pl330_init(void)
  return amba_driver_register(&pl330_driver);
static void __exit pl330_exit(void)
  amba_driver_unregister(&pl330_driver);
```

### 1.2 DMA mask
```cpp
struct device {
	...;
	u64		*dma_mask;	// dma mask (if dma'able device)
	u64		coherent_dma_mask; // Like dma_mask, but for alloc_coherent mappings as not all hardware supports 64 bit addresses for consistent allocations such descriptors.
	...;
};
//通过调用dma_set_mask()来通知内核dma访问寻址能力限制：
int dma_set_mask(struct device *dev, u64 mask);
//通过调用dma_set_coherent_mask()通知内核一致性内存分配的限制。这里dev为一个指向设备的指针，掩码显示与设备寻址能力对应的位。如果使用指定的mask时DMA能正常工作，则返回零。通常来说，设备数据结构是内嵌到特定总线的设备结构中的。比如一个指向PCI设备的指针为pdev->dev（pdev指向PCI设备）。
//一般的设定是 DMA_BIT_MASK(32) 或 ~0 或 0xffffffffUL
int dma_set_coherent_mask(struct device *dev, u64 mask);

static u64 sdmmc0_dma_mask = DMA_BIT_MASK(32);
struct platform_device imap_mmc0_device = {
	.dev = {
		.platform_data = &imap_mmc0_platdata,
		.dma_mask = &sdmmc0_dma_mask,
		.coherent_dma_mask = DMA_BIT_MASK(32),
		},
};
```

## 2 dmaengine
**dmaengine.c中核心函数**<br>
`dma_async_device_register()` `__dma_request_channel()` ，负责dma通道的注册和申请，前者在`/sys/class/dma`目录下，创建了24个通道设备`dma0chanx`

`pl330_tx_submit()`函数中，不知道是否每次`last->node`都是空的,`pch->work_list`是否可以用来构建多request的LLI工作方式
`fill_lli_queue()`函数中，`pch->work_list`中的每个request调用`pl330_submit_lli_req()`

所以，我的理解是，可以多次调用`dmaengine_prep_slave_sg()``dmaengine_submit()`
从`struct device`反向解析次序:  struct device device --> struct dma_chan_dev --> struct dma_chan

### 2.1 数据结构
**用于上层接口API**<br>
```cpp
struct dma_slave_config {
	enum dma_transfer_direction direction;
	dma_addr_t src_addr, dst_addr;
	enum dma_slave_buswidth src_addr_width, dst_addr_width;
	enum dma_slave_byteswap byteswap;
	u32 src_maxburst, dst_maxburst;
	bool device_fc; unsigned int slave_id;
};
struct dma_async_tx_descriptor {
	dma_cookie_t cookie; //一个整型数，用于追踪本次传输
	enum dma_ctrl_flags flags; /* not a 'long' to pack with cookie */
	dma_addr_t phys;
	struct dma_chan *chan;
	dma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx); //controller driver提供的回调函数，用于把该描述符提交到待传输列表
	dma_async_tx_callback callback;
	void *callback_param;
};
```
**用于内部管理**<br>
```cpp
struct dma_device {
	unsigned int chancnt, privatecnt;
	struct list_head channels, global_node;
	dma_cap_mask_t  cap_mask;
	unsigned short max_xor, max_pq;
	u8 copy_alignm, xor_align, pq_align, fill_align;
	//#define DMA_HAS_PQ_CONTINUE (1 << 15)

	int dev_id;
	struct device *dev;
	int(void) (*device_alloc_chan_resources)(*device_free_chan_resources)(struct dma_chan *chan);

	struct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,	size_t len, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_xor)(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,	unsigned int src_cnt, size_t len, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(struct dma_chan *chan, dma_addr_t *src,	unsigned int src_cnt,	size_t len, enum sum_check_flags *result, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_pq)(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,unsigned int src_cnt, const unsigned char *scf,	size_t len, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_pq_val)(	struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,	unsigned int src_cnt, const unsigned char *scf, size_t len,	enum sum_check_flags *pqres, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(	struct dma_chan *chan, dma_addr_t dest, int value, size_t len,unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(struct dma_chan *chan, unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(struct dma_chan *chan,struct scatterlist *dst_sg, unsigned int dst_nents,	struct scatterlist *src_sg, unsigned int src_nents,	unsigned long flags);
	struct dma_async_tx_descriptor *(*device_prep_slave_sg)(struct dma_chan *chan, struct scatterlist *sgl,	unsigned int sg_len, enum dma_transfer_direction direction,	unsigned long flags, void *context);

  /**added by cls for dma lli mode */
	struct dma_async_tx_descriptor *(*device_prep_dma_lli)(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,　size_t period_len, enum dma_transfer_direction direction);
	struct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,	size_t period_len, enum dma_transfer_direction direction,unsigned long flags, void *context);
	struct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(struct dma_chan *chan, struct dma_interleaved_template *xt,unsigned long flags);
	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,	unsigned long arg);

	enum dma_status (*device_tx_status)(struct dma_chan *chan, dma_cookie_t cookie, struct dma_tx_state *txstate);
	void (*device_issue_pending)(struct dma_chan *chan);
	int (*device_dma_getposition)(struct dma_chan *chan, dma_addr_t *src, dma_addr_t *dst);
};

struct dma_chan_dev {
	struct dma_chan *chan;
	struct device device;
	int dev_id;
	atomic_t *idr_ref;
};
struct dma_chan {
	struct dma_device *device;
	dma_cookie_t cookie, completed_cookie;
	/* sysfs */
	int chan_id;
	struct dma_chan_dev *dev;
	struct list_head device_node;
	struct dma_chan_percpu __percpu *local;
	int client_count, table_count;
	void *private;
};
```
### 2.2 dmaengine core

```cpp
static DEFINE_MUTEX(dma_list_mutex);
static DEFINE_IDR(dma_idr);
static LIST_HEAD(dma_device_list);
static long dmaengine_ref_count;

//
struct dma_chan *dev_to_dma_chan(struct device *dev);

```

### 2.3 dmaengine API
```cpp
//#define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param) //fn如pl022_filter()
  |--> chan = private_candidate(mask, device, fn, fn_param);//根据filter函数找到匹配的chan
		list_for_each_entry(chan, &dev->channels, device_node) if (fn && fn(chan, fn_param)) return chan;
	dma_chan_get(chan);//-->chan->device->device_alloc_chan_resources(chan); 为channel分配资源

int  dmaengine_slave_config(struct dma_chan *chan, struct dam_slave_config *config) //申请到一个dma channel之后，根据实际情况，对该channel进行参数配置 //return chan->device->device_control(chan, DMA_SLAVE_CONFIG,(unsigned long)config);

//逆向解析引用::
int dmaengine_device_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd, unsigned long arg)
  <-- dmaengine_slave_config()
  <-- dmaengine_terminate_all()
  <-- int dmaengine_pause(struct dma_chan *chan)
  <-- int dmaengine_resume(struct dma_chan *chan)

struct dma_async_tx_descriptor *dmaengine_prep_slave_single(struct dma_chan *chan, dma_addr_t buf, size_t len,enum dma_transfer_direction dir, unsigned long flags); //获取一个传输描述符
struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,	unsigned int sg_len, enum dma_transfer_direction dir, unsigned long flags);
struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,	size_t period_len, enum dma_transfer_direction dir,unsigned long flags);

dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc) //一旦传输描述符准备好并且回调函数也加入后，该函数把传输描述符加入到DMA engine驱动的等待队列，但不会启动DMA操作　--> pl330_tx_submit()-->list_add_tail(&last->node, &pch->work_list); //将请求desc加入worklist
void dma_async_issue_pending(struct dma_chan *chan) //该函数启动DMA传输，此时如果通道是空闲的，等待队列中的第一个传输描述符将会启动DMA操作。

enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx); //等待结束,好像没怎么调用
int dmaengine_terminate_all(struct dma_chan *chan);
void dma_release_channel(struct dma_chan *chan) //释放通道


//其他一些不常用的
dma_cookie_t dma_async_memcpy_buf_to_buf(struct dma_chan *chan, void *dest,	void *src, size_t len)
	dma_addr_t dma_src = dma_map_single(dev->dev, src, len, DMA_TO_DEVICE);
	dma_addr_t dma_dest = dma_map_single(dev->dev, dest, len, DMA_FROM_DEVICE);
	flags = DMA_CTRL_ACK |	DMA_COMPL_SRC_UNMAP_SINGLE |	DMA_COMPL_DEST_UNMAP_SINGLE;
	tx = dev->device_prep_dma_memcpy(chan, dma_dest, dma_src, len, flags);
	tx->callback = NULL;　cookie = tx->tx_submit(tx);
	preempt_disable();　__this_cpu_add(chan->local->bytes_transferred, len);　__this_cpu_inc(chan->local->memcpy_count);　preempt_enable();
dma_cookie_t dma_async_memcpy_buf_to_pg(struct dma_chan *chan, struct page *page,	unsigned int offset, void *kdata, size_t len)//基本同上，使用dma_map_page()而已
dma_cookie_t dma_async_memcpy_pg_to_pg(struct dma_chan *chan, struct page *dest_pg, unsigned int dest_off, struct page *src_pg, unsigned int src_off, size_t len)
//同步方式等待传输结束，但是期间并没有释放CPU给其他进程使用，效率很低，尽量不要使用
enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie) //
	dma_async_issue_pending(chan);
	while (1)
		status = dma_async_is_tx_complete(chan, cookie, NULL, NULL);
		if (time_after_eq(jiffies, dma_sync_wait_timeout))  return DMA_ERROR;
		if (status != DMA_IN_PROGRESS)	break;
		cpu_relax();

```

具体上来讲，内核的其他模块使用dma engine的步骤是:
- 使用`dma_request_channel()`先申请一个dma channel，之后的dma请求都基于这个申请的dma channel。
- 调用`dma_dev->device_prep_dma_memcpy(chan, dst, src, len, flag)`把dma操作的参数传给dma子系统。同时返回一个从chan申请的异步传输描述符: struct dma_async_tx_descriptor.可以把用户的回调函数设置在上面的描述符里。通常这里的回调函数里是一个complete函数，用来在传输完成后通知用户业务流程里的wait等待。
- tx->tx_submit(tx) 把请求提交。
- dma_submit_error 判断提交的请求是否合法。
- dma_async_issue_pending() 触发请求真正执行。
- 如上，在发送请求之后，一般可以在这里wait等待，通过上面注册的回调函数在dma执行完成后通知这里的wait。
- dma_async_is_tx_complete() 查看请求的状态。
- 做完dma操作之后使用dma_release_channel()释dma_request_channel放申请的dma channel。

```cpp
//attrs里面的功能,好像没有实际生效 : cat /sys/class/dma/dma0chan0~10/bytes_transferred,全0
static struct device_attribute dma_attrs[] = {
	__ATTR(memcpy_count, S_IRUGO, show_memcpy_count, NULL),	__ATTR(bytes_transferred, S_IRUGO, show_bytes_transferred, NULL),
	__ATTR(in_use, S_IRUGO, show_in_use, NULL),	__ATTR_NULL
};
static struct class dma_devclass = {
	.name		= "dma",	.dev_attrs	= dma_attrs,	.dev_release	= chan_dev_release,
};
int __init dma_bus_init(void) //return class_register(&dma_devclass);
arch_initcall(dma_bus_init);
```

### 2.4 dma_mapping
**相关的主要文件**<br>
`include/asm-generic/dma-mapping-common.h` 和 `arch/arm/mm/dma-mapping.c`　和　`arch/arm/include/asm/glue-proc.h`
`arch/arm/mm/proc-v7.S` `arch/arm/mm/proc-v7-2level.S`
```cpp
//include/asm-generic/dma-mapping-common.h
	//#define _CACHE v7
	#define __cpuc_flush_icache_all		__glue(_CACHE,_flush_icache_all)
	#define __cpuc_flush_kern_all		__glue(_CACHE,_flush_kern_cache_all)
	#define __cpuc_flush_kern_louis		__glue(_CACHE,_flush_kern_cache_louis)
	#define __cpuc_flush_user_all		__glue(_CACHE,_flush_user_cache_all)
	#define __cpuc_flush_user_range		__glue(_CACHE,_flush_user_cache_range)
	#define __cpuc_coherent_kern_range	__glue(_CACHE,_coherent_kern_range)
	#define __cpuc_coherent_user_range	__glue(_CACHE,_coherent_user_range)
	#define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)

	#define dmac_map_area    __glue(_CACHE,_dma_map_area)    // v7_dma_map_area
	#define dmac_unmap_area  __glue(_CACHE,_dma_unmap_area)  // v7_dma_unmap_area
	#define dmac_flush_range __glue(_CACHE,_dma_flush_range) // v7_dma_flush_range

//arch/arm/include/asm/glue-proc.h
	//#define CPU_NAME cpu_v7
	#define cpu_proc_init			__glue(CPU_NAME,_proc_init)			//cpu_v7_proc_init
	#define cpu_proc_fin			__glue(CPU_NAME,_proc_fin)			//cpu_v7_proc_fin
	#define cpu_reset			__glue(CPU_NAME,_reset)							//cpu_v7_reset
	#define cpu_do_idle			__glue(CPU_NAME,_do_idle)					//cpu_v7_do_idle
	#define cpu_dcache_clean_area		__glue(CPU_NAME,_dcache_clean_area)	//cpu_v7_dcache_clean_area
	#define cpu_do_switch_mm		__glue(CPU_NAME,_switch_mm)		//cpu_v7_switch_mm
	#define cpu_set_pte_ext			__glue(CPU_NAME,_set_pte_ext)	//cpu_v7_set_pte_ext
	#define cpu_suspend_size		__glue(CPU_NAME,_suspend_size)//cpu_v7_suspend_size
	#define cpu_do_suspend			__glue(CPU_NAME,_do_suspend)	//cpu_v7_do_suspend
	#define cpu_do_resume			__glue(CPU_NAME,_do_resume)   	//cpu_v7_do_resume

struct dma_map_ops arm_dma_ops = {
	.alloc = arm_dma_alloc,	.free = arm_dma_free,	.mmap	= arm_dma_mmap,	.get_sgtable = arm_dma_get_sgtable,
	.map_page	= arm_dma_map_page,	.unmap_page	= arm_dma_unmap_page,	.map_sg = arm_dma_map_sg,	.unmap_sg = arm_dma_unmap_sg,
	.sync_single_for_cpu = arm_dma_sync_single_for_cpu,	.sync_single_for_device	= arm_dma_sync_single_for_device,
	.sync_sg_for_cpu = arm_dma_sync_sg_for_cpu,	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
	.set_dma_mask = arm_dma_set_mask,
};

static int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr, void *data) //--> struct page *page = virt_to_page(addr); pgprot_t prot = *(pgprot_t *)data; set_pte_ext(pte, mk_pte(page, prot), 0); --> cpu_v7_set_pte_ext() 清除页表映射中cache标记，cpu访问内存的时候，就不会经由cache了

void *__alloc_from_contiguous(struct device *dev, size_t size, pgprot_t prot, struct page **ret_page, const void *caller)
	struct page *page = dma_alloc_from_contiguous(dev, count, order);//从伙伴系统中分配连续内存
	|--> __dma_clear_buffer(page, size);
		void *ptr = page_address(page); memset(ptr, 0, size);
		dmac_flush_range(ptr, ptr + size); //v7_dma_flush_range : clean & invalidate cache
		outer_flush_range(__pa(ptr), __pa(ptr) + size); //如果有二级cache
	|--> __dma_remap(page, size, prot);
		apply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot); dsb();
		flush_tlb_kernel_range(start, end);
	ptr = page_address(page);

//#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, NULL)
dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr, size_t size, enum dma_data_direction dir, struct dma_attrs *attrs)
	struct dma_map_ops *ops = get_dma_ops(dev); //-->return &arm_dma_ops;
	|--> addr = ops->map_page(dev, virt_to_page(ptr), (unsigned long)ptr & ~PAGE_MASK, size, dir, attrs);//-->arm_dma_map_page()
		__dma_page_cpu_to_dev(page, offset, size, dir); //-->dma_cache_maint_page(page, off, size, dir, dmac_map_area); 对所有的page调用 dmac_map_area()
		return pfn_to_dma(dev, page_to_pfn(page)) + offset;

//#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, NULL)
void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr, size_t size, enum dma_data_direction dir, struct dma_attrs *attrs)
		struct dma_map_ops *ops = get_dma_ops(dev); //-->return &arm_dma_ops;
		ops->unmap_page(dev, addr, size, dir, attrs); //-->arm_dma_unmap_page()-->__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)), ahandle & ~PAGE_MASK, size, dir);--> dma_cache_maint_page(page, off, size, dir, dmac_unmap_area);

//和上面single的两个函数没有什么区别，不过这个只有一个page罢了，不做解析
dma_addr_t dma_map_page(struct device *dev, struct page *page,unsigned long offset, size_t size,enum dma_data_direction direction);
void dma_unmap_page(struct device *dev, dma_addr_t dma_address,size_t size, enum dma_data_direction direction);

//#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, NULL)
int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg, int nents, enum dma_data_direction dir, struct dma_attrs *)
	struct dma_map_ops *ops = get_dma_ops(dev);
	|--> return ents = ops->map_sg(dev, sg, nents, dir, attrs); //-->arm_dma_map_sg()
		for_each_sg(sg, s, nents, i)
			s->dma_address = ops->map_page(dev, sg_page(s), s->offset, s->length, dir, attrs);
		return nents;
//#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, NULL)
void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg, int nents, enum dma_data_direction dir, struct dma_attrs *attrs)

dma_sync_single_for_cpuvoid dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size, enum dma_data_direction dir)//-->arm_dma_sync_single_for_cpu()
	unsigned int offset = handle & (PAGE_SIZE - 1); 	struct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));
	|--> __dma_page_dev_to_cpu(page, offset, size, dir);
		unsigned long paddr = page_to_phys(page) + off; dma_cache_maint_page(page, off, size, dir, dmac_unmap_area);
		if (dir != DMA_TO_DEVICE && off == 0 && size >= PAGE_SIZE)	set_bit(PG_dcache_clean, &page->flags);

void dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size, enum dma_data_direction dir)//-->
	unsigned int offset = handle & (PAGE_SIZE - 1);
	struct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));
	|--> __dma_page_cpu_to_dev(page, offset, size, dir);
		unsigned long paddr = page_to_phys(page) + off;	dma_cache_maint_page(page, off, size, dir, dmac_map_area);
```

### 2.5 spi-imapx应用
```cpp
//step 1:
static inline bool pl022_filter(struct dma_chan *chan, void *param)
       u8 *peri_id;       peri_id = chan->private;       return *peri_id == (unsigned)param;

int imapx_spi_dma_probe(struct imapx_spi *host)
	dma_cap_mask_t mask;
	dma_cap_zero(mask); 	dma_cap_set(DMA_SLAVE, mask); //标准使用方式
	host->dma_rx_channel = dma_request_channel(mask,host->master_info->dma_filter/* 如pl022_filter() */,host->master_info->dma_rx_param);
	//dma_filter这个函数主要是查找你的dma传输的设备的请求信号线，其具体是在注册时填写的。这里会根据这个函数返回的真假来判断已经注册在总线上的dma slave的
	host->dma_tx_channel = dma_request_channel(mask,host->master_info->dma_filter,host->master_info->dma_tx_param);
	host->buffer = dma_alloc_coherent(host->dev, SCATTER_SIZE, &host->buffer_dma_addr, GFP_KERNEL);
//step 2:
int imapx_spi_configure_dma(struct imapx_spi *host)
	struct dma_chan *rxchan = host->dma_rx_channel;
	struct dma_async_tx_descriptor *rxdesc;
	struct dma_slave_config rx_conf = {	.src_addr = host->phy_base + SPI_FDR,	.direction = DMA_DEV_TO_MEM,　.device_fc = false,　};
	rx_conf.src_maxburst = 32;	rx_conf.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;
	rx_conf.byteswap = DMA_SLAVE_BYTESWAP_32;
	dmaengine_slave_config(rxchan, &rx_conf);
	status = sg_alloc_table(&host->sgt_rx, sgs, GFP_ATOMIC);
	imapx_spi_setup_dma_scatter(host, host->buffer, host->cur_rx_len, 0, &host->sgt_rx);
	rx_sglen = dma_map_sg(rxchan->device->dev, host->sgt_rx.sgl, host->sgt_rx.nents, DMA_FROM_DEVICE);//通过这个函数来实现虚拟地址和物理地址的映射。
	//流式DMA映射：使用的内存来自于上层kmalloc()或者__get_free_pages(),而不是dma_alloc_coherent()，增加了cache的使无效和清除操作以解决cache一致性问题
	rxdesc = dmaengine_prep_slave_sg(rxchan, host->sgt_rx.sgl,rx_sglen, DMA_DEV_TO_MEM,DMA_PREP_INTERRUPT | DMA_CTRL_ACK);//return chan->device->device_prep_slave_sg(chan, sgl, sg_len, dir, flags, NULL);
	rxdesc->callback = imapx_spi_dma_callback;
	rxdesc->callback_param = host;
	dmaengine_submit(rxdesc);//return desc->tx_submit(desc);
//step 3:
int imapx_spi_start_dma(struct imapx_spi *host)
	dma_async_issue_pending(host->dma_rx_channel);//chan->device->device_issue_pending(chan);
//step 4:
void imapx_spi_dma_callback(void *data)
	dma_unmap_sg(host->dma_rx_channel->device->dev, host->sgt_rx.sgl,	host->sgt_rx.nents, DMA_FROM_DEVICE);//访问数据之前需要先unmap,否则需要调用dma_sync_sg_for_cpu();/dma_sync_sg_for_device();
	sg_free_table(&host->sgt_rx);
	//然后可以开始下一次传输
//step 5:
void imapx_spi_dma_remove(struct imapx_spi *host)
	imapx_spi_terminate_dma(host);
	dma_release_channel(host->dma_tx_channel); dma_release_channel(host->dma_rx_channel);
	dma_free_coherent(host->dev, SCATTER_SIZE, host->buffer, host->buffer_dma_addr);
```

## 3 cache机制
ARM/PPC/MIPS三款主流嵌入式处理器都是软件管理cache，即有专门的指令来进行cache操作，ARM的CP15协处理器也提供对cache的操作. 在kernel平台汇编代码中也封装了cache操作函数，这里以ARM v7处理器为例说明，在`arch/arm/mm/cache-v7.S`中就封装了cache的操作函数，其中`v7_dma_flush_range()`刷新函数是完成了写回和无效2种操作. Cache的操作有2种：写回和无效。写回操作是将cache中数据写回到DDR中，但cache和ddr之间的关联仍然在，无效操作是无效掉cache中原有数据，下次读取cache中数据时，需要重新建立cacheline和ddr间关联，从DDR中重新读取。这两种操作其实都是为了保证cache数据一致性。

**kernel中有2种情况是需要保证数据一致性的：**<br>
（1）寄存器地址空间--硬件上有保护，可忽略。寄存器是CPU与外设交流的接口，有些状态寄存器是由外设根据自身状态进行改变，这个操作对CPU是不透明的。有可能这次CPU读入该状态寄存器，下次再读时，该状态寄存器已经变了，但是CPU还是读取的cache中缓存的值。但是寄存器操作在kernel中是必须保证一致的，这是kernel控制外设的基础，IO空间通过ioremap进行映射到内核空间。ioremap在映射寄存器地址时页表是配置为uncached的。数据不走cache，直接由地址空间中读取。保证了数据一致性。
（2）DMA缓冲区的地址空间。DMA操作对于CPU来说也是不透明的，DMA导致内存中数据更新，对于CPU来说是完全不可见的。反之亦然，CPU写入数据到DMA缓冲区，其实是写到了cache，这时启动DMA，操作DDR中的数据并不是CPU真正想要操作的。

### 3.1 相关cache基本操作
```cpp
v7_dma_inv_range:
	dcache_line_size r2, r3
	sub	r3, r2, #1
	tst	r0, r3
	bic	r0, r0, r3
	mcrne	p15, 0, r0, c7, c14, 1		@ clean & invalidate D / U line 刷新buffer前边部分不对齐空间
	tst	r1, r3
	bic	r1, r1, r3
	mcrne	p15, 0, r1, c7, c14, 1		@ clean & invalidate D / U line 刷新buffer后边部分不对齐空间
1:
	mcr	p15, 0, r0, c7, c6, 1		@ invalidate D / U line
	add	r0, r0, r2
	cmp	r0, r1
	blo	1b
	dsb
	mov	pc, lr
ENDPROC(v7_dma_inv_range)

v7_dma_clean_range:
	dcache_line_size r2, r3
	sub	r3, r2, #1
	bic	r0, r0, r3
1:
	mcr	p15, 0, r0, c7, c10, 1		@ clean D / U line 只刷新
	add	r0, r0, r2
	cmp	r0, r1
	blo	1b
	dsb
	mov	pc, lr
ENDPROC(v7_dma_clean_range)

ENTRY(v7_dma_map_area)
	add	r1, r1, r0
	teq	r2, #DMA_FROM_DEVICE　 //DMA_FROM_DEVICE类型的话，清除cache内容，防止ddr数据被CPU在DMA过程中写回；否则，写回cache内容以保持同步
	beq	v7_dma_inv_range
	b	v7_dma_clean_range
ENDPROC(v7_dma_map_area)

ENTRY(v7_dma_unmap_area)
	add	r1, r1, r0
	teq	r2, #DMA_TO_DEVICE     //DMA_FROM_DEVICE类型的话，设么都不做；否则，清除cache内容，这样可以强制之后CPU读取的时候会从ddr中获取最新数据．
	bne	v7_dma_inv_range
	mov	pc, lr
ENDPROC(v7_dma_unmap_area)

ENTRY(v7_dma_flush_range)
	dcache_line_size r2, r3
	sub	r3, r2, #1
	bic	r0, r0, r3
1:
	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D / U line
	add	r0, r0, r2
	cmp	r0, r1
	blo	1b
	dsb
	mov	pc, lr
ENDPROC(v7_dma_flush_range)
```

### 3.2 一致性DMA缓存
```cpp
//#define dma_alloc_coherent(d, s, h, f) dma_alloc_attrs(d, s, h, f, NULL)
void *dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp);
	struct dma_map_ops *ops = get_dma_ops(dev); //-->return &arm_dma_ops;
	|--> return cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);//-->arm_dma_alloc()
		pgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel); //主要是uncached标记
		|--> return __dma_alloc(dev, size, handle, gfp, prot, false, __builtin_return_address(0));
			u64 mask = get_coherent_dma_mask(dev); gfp |= GFP_DMA; gfp &= ~(__GFP_COMP);
			if (is_coherent || nommu())				addr = __alloc_simple_buffer(dev, size, gfp, &page);
			else if (!(gfp & __GFP_WAIT))			addr = __alloc_from_pool(size, &page);
			else if (!IS_ENABLED(CONFIG_CMA))	addr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);
			else if (gfp&__GFP_NOWARN)				addr = __alloc_from_contiguous_noalign(dev, size, prot, &page, caller);
			else 															addr = __alloc_from_contiguous(dev, size, prot, &page, caller);

void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr, dma_addr_t handle);
void *dma_alloc_writecombine(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp); //用于分配一个写合并(writecombinbing)的DMA缓冲区
void dma_free_writecombine(struct device *dev, size_t size, void *cpu_addr, dma_addr_t dma_handle);//其实就是dma_free_conherent
```

### 3.3 流式DMA映射
在map的时候要指定一个参数，来指明数据的方向是从外设到内存还是从内存到外设：1. 从内存到外设：CPU会做cache的flush操作，将cache中新的数据刷到内存; 2. 从外设到内存：CPU将cache置无效，这样CPU读的时候不命中，就会从内存去读新的数据

流式映射有3种接口，单缓冲区映射/单页映射/分离聚合映射

不要发明新的体系结构相关的scatterlist；使用`<asm-generic/scatterlist.h>`就可以了。如果体系结构支持IOMMU（包括软件IOMMU），你就需要使能`CONFIG_NEED_SG_DMA_LENGTH`

### 3.4 scatterlist
`kernel/lib/scatterlist.c`

```cpp
//#define for_each_sg(sglist, sg, nr, __i)	for (__i = 0, sg = (sglist); __i < (nr); __i++, sg = sg_next(sg))
//#define sg_is_chain(sg)		((sg)->page_link & 0x01)
//#define sg_is_last(sg)		((sg)->page_link & 0x02)
//#define sg_chain_ptr(sg)	((struct scatterlist *) ((sg)->page_link & ~0x03))
//#define sg_dma_address(sg)	((sg)->dma_address)
//#define sg_dma_len(sg)		((sg)->length)

struct sg_table {
	struct scatterlist *sgl;	/* the list */
	unsigned int nents;		/* number of mapped entries */
	unsigned int orig_nents;	/* original size of list */
};
struct scatterlist {//每一个内存段对应一个结构体来管理
	unsigned long	page_link; 	unsigned int	offset, length; dma_addr_t	dma_address;
};

void sg_init_table(struct scatterlist *sg, unsigned int nents); //memset(sgl, 0, sizeof(*sgl) * nents); sg_mark_end(&sgl[nents - 1]);
void sg_set_page(struct scatterlist *sg, struct page *page, unsigned int len, unsigned int offset); //将page中指定offset、指定长度的内存赋给指定的scatterlist（设置page_link、offset、len字段）
	|--> sg_assign_page(sg, page);//将page赋给指定的scatterlist（设置page_link字段）
		unsigned long page_link = sg->page_link & 0x3;
		sg->page_link = page_link | (unsigned long) page;
	sg->offset = offset;	sg->length = len;
void sg_set_buf(struct scatterlist *sg, const void *buf, unsigned int buflen); //sg_set_page(sg, virt_to_page(buf), buflen, offset_in_page(buf));　将指定长度的buffer赋给scatterlist（从虚拟地址中获得page指针、在page中的offset之后，再调用sg_set_page）
struct page *sg_page(struct scatterlist *sg) //-->return (struct page *)((sg)->page_link & ~0x3); 获取scatterlist所对应的page指针

struct scatterlist *sg_last(struct scatterlist *s, unsigned int);
struct scatterlist *sg_next(struct scatterlist *sg)
//将两个scatterlist 数组捆绑在一起
void sg_chain(struct scatterlist *prv, unsigned int prv_nents, struct scatterlist *sgl)
//获取某个scatterlist的物理或者虚拟地址
dma_addr_t sg_phys(struct scatterlist *sg) //return page_to_phys(sg_page(sg)) + sg->offset;
inline void *sg_virt(struct scatterlist *sg)//return page_address(sg_page(sg)) + sg->offset;
//获取sg个数
int sg_nents(struct scatterlist *sg);
//将某个scatterlist 标记（或者不标记）为the last one
void sg_mark_end(struct scatterlist *sg) //sg->page_link |= 0x02; sg->page_link &= ~0x01;
void sg_unmark_end(struct scatterlist *sg) //sg->page_link &= ~0x02;

//范例
void my_example()
	struct scatterlist *sg, *sg_list =  = kmalloc(sizeof(struct scatterlist) * PL022_MAX_SGL_NENTS, GFP_KERNEL);
	int i, nents;
	sg_init_table(sg_list, PL022_MAX_SGL_NENTS);
	for_each_sg(sg_list, sg, PL022_MAX_SGL_NENTS, i)
		sg_set_page(sg, virt_to_page(p_virt_buffer), len, 0); nents ++; if (have_detect_end) { sg_mark_end(sg); break; }
```
**其他table**<br>
```cpp
//#define SG_MAX_SINGLE_ALLOC		(PAGE_SIZE / sizeof(struct scatterlist)) //一个page可以保存的scl个数
void sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen) //sg_init_table(sg, 1);sg_set_buf(sg, buf, buflen);
int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask) //-->__sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC, gfp_mask, sg_kmalloc);
	memset(table, 0, sizeof(*table));
	while not_finished
		sg = alloc_fn(alloc_size, gfp_mask); //-->sg_kmalloc()分配若干个scatterlist的数组
		sg_init_table(sg, alloc_size); 	table->nents = table->orig_nents += sg_size;
		if (prv) sg_chain(prv, max_ents, sg); else  table->sgl = sg;　//链接两个数组
		if (!left)	sg_mark_end(&sg[sg_size - 1]);
		prv = sg;
void sg_free_table(struct sg_table *table)
```
