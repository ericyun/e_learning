  ##

###  修订记录
| 修订说明 | 日期 | 作者 | 额外说明 |
| --- |
| 初版 | 2018/04/10 | 员清观 |  |

## 0 计划完成

移植`fsck`程序来实现fat32文件系统的分析，学习，改进．百度搜索: read()/write()的生命旅程

!!!! 使用延时分配策略,所有的簇分配集中进行,会产生资源的burst. 另外,分配簇需要读取fat表,fat表中一个扇区对应512k的数据区,2m的数据,对应4个扇区,可以考虑一次读取4个扇区应该可以减少一点点时间吧

多个文件并行写入的过程就是不同文件的元数据和数据交叉写入的过程，而元数据和数据在存储设备上位于不同的位置，优化的目标就是尽量安排好他们的次序，以符合效率两原则。
数据就是文件内容数据，而元数据主要包括有：1. fsinfo扇区(空闲帧数等信息)　2. 目录项信息　3. fat表簇号

buffer相关的大量无法索引到的symbol，实际上是一些宏而已，在buffer_head.h中定义；linux大部分文件中定义的变量和函数是static的，所以，我们只要关注文件中被 EXPORT 或者被注册的ops就好.再结合数据结构，架构就很清晰了。

Writeback机制的好处总结起来主要是两点：
1.加快write()的响应速度。因为media的读写相对于内存访问是较慢的。如果每个write()都访问media，势必很慢。将较慢的media访问交给writeback thread，而write()本身的thread里只在内存里操作数据，将数据交到writeback queue即返回。
2.便于合并和排序 (merge and sort) 多个write，merge是将多个少量数据的write合并成几个大量数据的write，减少访问media的次数；sort是将无序的write按照其访问media上的block的顺序排序，减少磁头在media上的移动距离。

自下而上，函数调用关系逆向跟踪的例子，有时候很方便解析函数调用逻辑：

## 1. IO系统架构

![io子系统架构](pic_dir/io子系统架构.png)



## 3. io数据流


![数据流写入硬盘的过程](pic_dir/数据流写入硬盘的过程.png)
```cpp
//malloc的buf对于图层中的application buffer,即应用程序的buffer;
char *buf = malloc(MAX_BUF_SIZE);
strncpy(buf, src, MAX_BUF_SIZE);
//fwrite是系统提供的最上层接口,也是最常用的接口。它在用户进程空间开辟一个buffer,将多次小数据量相邻写操作先缓存起来,合并,最终调用write函数一次性写入(或者将大块数据分解多次write调用)。调用fwrite后,把数据从application buffer 拷贝到了 CLib buffer,即C库标准IObuffer。fwrite返回后,数据还在CLib buffer,如果这时候进程core掉。这些数据会丢失。没有写到磁盘介质上。
fwrite(buf, MAX_BUF_SIZE, 1, fp);
//除了fclose方法外,还有一个主动刷新操作fflush函数,不过fflush函数只是把数据从CLib buffer 拷贝到page cache 中,并没有刷新到磁盘上,从page cache刷新到磁盘上可以通过调用fsync函数完成。
fflush(fp);
fsync(fp);
//当调用fclose的时候,fclose调用会把这些数据刷新到磁盘介质上。
fclose(fp);
```
**非缓冲式操作与缓冲式操作**<br>
Linux下的文件I/O操作分为非缓冲式操作与缓冲式操作，非缓冲式操作的函数主要是由系统调用提供的，缓冲式I/O操作主要是由C语言的标准输入输出库来提供的。非缓冲式适用于小文件的读写，实时性强，而非缓冲式I/O操作适用于非实时性任务，大量数据的读写，能够减小系统开销。上图中的完整流程对应的是缓冲式文件I/O操作,主要对应的函数有:fopen,fclose,fread,fwrite,fflush,fscanf,fprintf,fgetpos,fsetpos,ftell,rewind,fgets,fputs.缓冲区是为程序分配的内存块，在进行数据比较大且不要求实时性的I/O操作时，一部分数据被放于缓冲区中，只有当数据块的长度要超过缓冲区的长度或时间周期到达时，才把这些数据送到指定的位置。基于缓冲区的设备操作是为了减少读写的次数，从而减小系统开销，主要适用于大量数据的读写操作。而非缓冲式文件操作对于小规模文件的读写，或者是实时设备，执行非缓冲式文件操作，应用程序能够立即得到数据。非缓冲式文件操作的函数主要是由系统调用提供，有以下几个函数,read(),write(),lseek(). 这些操作避开了clib的buffer.

完整的写流程:
- 1. fwrite是系统提供的最上层接口，也是最常用的接口。它在用户进程空间开辟一个buffer，将多次小数据量相邻写操作先缓存起来，合并，最终调用write函数一次性写入（或者将大块数据分解多次write调用）
- 2. 内核的Write函数通过调用系统调用接口,将数据从应用层copy到内核层,所以write会触发内核态/用户态切换。当数据到达page cache后,内核并不会立即把数据往下传递。而是返回用户空间。数据什么时候写入硬盘,有内核IO调度决定,所以write是一个异步调用。这一点和read不同,read调用是先检查page cache里面是否有数据,如果有,就取出来返回用户,如果没有,就同步传递下去并等待有数据,再返回用户,所以read是一个同步过程。当然你也可以把write的异步过程改成同步过程,就是在open文件的时候带上O_SYNC标记。
- 3. 数据到了page cache后,内核有pdflush线程在不停的检测脏页,判断是否要写回到磁盘中。
- 4. 把需要写回的页提交到IO队列——即IO调度队列。IO调度队列调度策略调度何时写回。IO队列有2个主要任务。一是合并相邻扇区的,而是排序。合并相信很容易理解,排序就是尽量按照磁盘选择方向和磁头前进方向排序。因为磁头寻道时间是和昂贵的。这里IO队列和我们常用的分析工具IOStat关系密切。IOStat中rrqm/s wrqm/s表示读写合并个数。avgqu-sz表示平均队列长度。内核中有多种IO调度算法。当硬盘是SSD时候,没有什么磁道磁头,人家是随机读写的,加上这些调度算法反而画蛇添足。OK,刚好有个调度算法叫noop调度算法,就是什么都不错(合并是做了)。刚好可以用来配置SSD硬盘的系统。
- 5. 从IO队列出来后,就到了驱动层(当然内核中有更多的细分层,这里忽略掉),驱动层通过DMA,将数据写入磁盘cache。
- 6. 磁盘cache时候写入磁盘介质,那是磁盘控制器自己的事情。如果想要睡个安稳觉,确认要写到磁盘介质上。就调用fsync函数吧。可以确定写到磁盘上了

内核使用文件描述符来索引打开的文件。文件描述符是一个非负整数，每当打开一个存在的文件或创建一个新文件的时候，内核会向进程返回一个文件描述符，当对文件进行相应操作的时候，使用文件描述符作为参数传递给相应的函数。

**提升性能:**<br>
1. 上面的系统调用write会触发用户态/内核态切换。这时候该mmap出场了，mmap把page cache地址空间映射到用户空间，应用程序像操作应用层内存一样，写文件。省去了系统调用开销。
2. 如果想绕过page cache，直接把数据送到磁盘设备上,可以通过open文件带上O_DIRECT参数，这是write该文件。就是直接写到设备上。
3. 如果继续较劲，直接写扇区有没有办法。这就是所谓的RAW设备写，绕开了文件系统，直接写扇区，想fdsik，dd，cpio之类的工具就是这一类操作。

**O_DIRECT 和 RAW**<br>
两者都要通过驱动层读写；在系统引导启动，还处于实模式的时候，可以通过bios接口读写raw设备。

O_DIRECT 和 RAW设备最根本的区别是O_DIRECT是基于文件系统的，也就是在应用层来看，其操作对象是文件句柄，内核和文件层来看，其操作是基于inode和数据块，这些概念都是和ext2/3的文件系统相关，写到磁盘上最终是ext3文件。一般基于O_DIRECT来设计优化自己的文件模块，是不满系统的cache和调度策略，自己在应用层实现这些，来制定自己特有的业务特色文件读写。但是写出来的东西是ext3文件，该磁盘卸下来，mount到其他任何linux系统上，都可以查看。

而基于RAW设备的设计系统，一般是不满现有ext3的诸多缺陷，设计自己的文件系统。而RAW设备写是没有文件系统概念，操作的是扇区号，操作对象是扇区，写出来的东西不一定是ext3文件（如果按照ext3规则写就是ext3文件）。自己设计文件布局和索引方式。举个极端例子：把整个磁盘做一个文件来写，不要索引。这样没有inode限制，没有文件大小限制，磁盘有多大，文件就能多大。这样的磁盘卸下来，mount到其他linux系统上，是无法识别其数据的。

!!! O_DIRECT的方式能够应用到nftl上来获得高收益么?

**page cache五个flush触发点**<br>
1.pdflush(内核线程)定期flush;
2.系统和其他进程需要内存的时候触发它flush;
3.用户手工sync,外部命令触发它flush;
4.proc内核接口触发flush,”echo 3 >/proc/sys/vm/drop_caches;
5.应用程序内部控制flush。


## 4. 文件系统相关命令

**强制同步文件更新的方法**<br>
  echo 3>/proc/sys/vm/drop_caches
  文件write的异步过程改成同步过程,就是在open文件的时候带上O_SYNC标记
  int  posix_fadvise( int  fd, off_t offset, off_t len,  int  advice); //advice可以定义直接清理该文件的cache

## 5 vfat文件主要系统调用层次分析

![磁盘和目录和文件系统](pic_dir/磁盘和目录和文件系统.png)

**虚拟文件系统主要模块:**<br>
- 1、超级块（super_block），用于保存一个文件系统的所有元数据，相当于这个文件系统的信息库，为其他的模块提供信息。因此一个超级块可代表一个文件系统。文件系统的任意元数据修改都要修改超级块。基于从超级块读取的控制信息生成的超级块对象是常驻内存并被缓存的,
- 2、目录项模块，管理路径的目录项。比如一个路径 /home/foo/hello.txt，那么目录项有home, foo, hello.txt。目录项的块，存储的是这个目录下的所有的文件的inode号和文件名等信息。其内部是树形结构，操作系统检索一个文件，都是从根目录开始，按层次解析路径中的所有目录，直到定位到文件。
- 3、inode模块，管理一个具体的文件，是文件的唯一标识，一个文件对应一个inode。通过inode可以方便的找到文件在磁盘扇区的位置。同时inode模块可链接到address_space模块，方便查找自身文件数据是否已经缓存。
- 4、打开文件列表模块，包含所有内核已经打开的文件。已经打开的文件对象由open系统调用在内核中创建，也叫文件句柄。打开文件列表模块中包含一个列表，每个列表表项是一个结构体struct file，结构体中的信息用来表示打开的一个文件的各种状态参数。
- 5、file_operations模块。这个模块中维护一个数据结构，是一系列函数指针的集合，其中包含所有可以使用的系统调用函数，例如open、read、write、mmap等。每个打开文件（打开文件列表模块的一个表项）都可以连接到file_operations模块，从而对任何已打开的文件，通过系统调用函数，实现各种操作。
- 6、address_space模块，它表示一个文件在页缓存中已经缓存了的物理页。它是页缓存和外部设备中文件系统的桥梁。如果将文件系统可以理解成数据源，那么address_space可以说关联了内存系统和文件系统。

![vfs相关模块间关系图](pic_dir/vfs相关模块间关系图.png)

![ Linux 文件系统组件的体系结构](pic_dir/linux文件系统组件的体系结构.png)
这个图的主要价值是, inode和dentry和VFS是一层的,独立于具体的文件系统,而buffercache依托于具体的文件系统.

**从进程看**<br>

![从进程的角度看](pic_dir/从进程的角度看.png)

```cpp
struct files_struct {//打开的文件集
  atomic_t count;              /*结构的使用计数*/
  ……
  int max_fds;                 /*文件对象数的上限*/
  int max_fdset;               /*文件描述符的上限*/
  int next_fd;                 /*下一个文件描述符*/
  struct file ** fd;           /*全部文件对象数组*/
  ……
 };

struct fs_struct {//建立进程与文件系统的关系
  atomic_t count;              /*结构的使用计数*/
  rwlock_t lock;               /*保护该结构体的锁*/
  int umask；                  /*默认的文件访问权限*/
  struct dentry * root;        /*根目录的目录项对象*/
  struct dentry * pwd;         /*当前工作目录的目录项对象*/
  struct dentry * altroot；    /*可供选择的根目录的目录项对象*/
  struct vfsmount * rootmnt;   /*根目录的安装点对象*/
  struct vfsmount * pwdmnt;    /*pwd的安装点对象*/
  struct vfsmount * altrootmnt;/*可供选择的根目录的安装点对象*/
};
```



系统调用统一定义在`calls.S`文件中. 可以参考网页: https://www.cnblogs.com/pengdonglin137/p/3714981.html

```cpp
//kernel/arch/arm/kernel/calls.s 文件中
 CALL(sys_restart_syscall)
 CALL(sys_exit)
 ...
//kernel/arch/arm/kernel/entry_common.s 文件中
 .equ NR_syscalls,0
 #define CALL(x) .equ NR_syscalls,NR_syscalls+1
 #include "calls.S"
//到这里,是为了统计calls的个数: NR_syscalls
 #define CALL(x) .long x
 ...
  .type	sys_call_table, #object
 ENTRY(sys_call_table)
 #include "calls.S"
 //到这里,定义了系统调用列表
 ENTRY(vector_swi) //通过软异常执行系统调用
  sub	sp, sp, #S_FRAME_SIZE
  ...
  get_thread_info tsk
  adr	tbl, sys_call_table
  ...
  ldrcc	pc, [tbl, scno, lsl #2]		@ call sys_* routine  !!执行!!
  bcs	arm_syscall
	b	sys_ni_syscall
 ENDPROC(vector_swi)
```

### 5.0 简化流程
以后是否每个章节都增加一个简化流程呢?
```cpp
//fs/fat/inode.c
fat_writepage()
    block_write_full_page()
        block_write_full_page_endio()
            __block_write_full_page()
                submit_bh()
                    _submit_bh()
                        submit_bio()
//block/blk-core.c
submit_bio()
    generic_make_request(bio)
        q->make_request_fn(q, bio);

//块设备的queue的初始化
blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)
    blk_init_queue_node()
        blk_init_allocated_queue()
            q->request_fn = rfn;()
            blk_queue_make_request(q, blk_queue_bio);
                q->make_request_fn = mfn;

blk_queue_bio()
    add_acct_request()
        __elv_add_request()
            q->elevator->type->ops.elevator_add_req_fn(q, rq);
    __blk_run_queue()
        __blk_run_queue_uncond()
            q->request_fn()
```


### 5.1 mount和open

linux vfat没有存储inode,需要根据dentry信息调用`fat_build_inode()`函数动态生成．

```cpp
struct dentry {/* RCU lookup touched fields */
	unsigned int d_flags;		/* protected by d_lock */
	seqcount_t d_seq;		/* per dentry seqlock */
	struct hlist_bl_node d_hash;	/* lookup hash list */
	struct dentry *d_parent;	/* parent directory */
	struct qstr d_name;
	struct inode *d_inode;		/* Where the name belongs to - NULL is negative */
	unsigned char d_iname[DNAME_INLINE_LEN];	/* small names */

	/* Ref lookup also touches following */
	unsigned int d_count;		/* protected by d_lock */
	spinlock_t d_lock;		/* per dentry lock */
	const struct dentry_operations *d_op;
	struct super_block *d_sb;	/* The root of the dentry tree */
	unsigned long d_time;		/* used by d_revalidate */
	void *d_fsdata;			/* fs-specific data */

	struct list_head d_lru;		/* LRU list */
	/* d_child and d_rcu can share memory */
	union {
		struct list_head d_child;	/* child of parent list */
	 	struct rcu_head d_rcu;
	} d_u;
	struct list_head d_subdirs;	/* our children */
	struct hlist_node d_alias;	/* inode alias list */
};
struct inode_operations {
	struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int);
	void * (*follow_link) (struct dentry *, struct nameidata *);
	int (*permission) (struct inode *, int);
	struct posix_acl * (*get_acl)(struct inode *, int);
	int (*readlink) (struct dentry *, char __user *,int);
	void (*put_link) (struct dentry *, struct nameidata *, void *);
	int (*create) (struct inode *,struct dentry *, umode_t, bool);
	int (*link) (struct dentry *,struct inode *,struct dentry *);
	int (*unlink) (struct inode *,struct dentry *);
	int (*symlink) (struct inode *,struct dentry *,const char *);
	int (*mkdir) (struct inode *,struct dentry *,umode_t);
	int (*rmdir) (struct inode *,struct dentry *);
	int (*mknod) (struct inode *,struct dentry *,umode_t,dev_t);
	int (*rename) (struct inode *, struct dentry *, struct inode *, struct dentry *);
	int (*setattr) (struct dentry *, struct iattr *);
	int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat *);
	int (*setxattr) (struct dentry *, const char *,const void *,size_t,int);
	ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t);
	ssize_t (*listxattr) (struct dentry *, char *, size_t);
	int (*removexattr) (struct dentry *, const char *);
	int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start, u64 len);
	int (*update_time)(struct inode *, struct timespec *, int);
	int (*atomic_open)(struct inode *, struct dentry *,
			   struct file *, unsigned open_flag, umode_t create_mode, int *opened);
} ____cacheline_aligned;

struct super_operations {
  struct inode *(*alloc_inode)(struct super_block *sb);
	void (*destroy_inode)(*drop_inode)(*evict_inode)(struct inode *);
  void (*dirty_inode) (struct inode *, int flags);
	int (*write_inode) (struct inode *, struct writeback_control *wbc);
	void (*put_super) (*umount_begin) (struct super_block *);
	int (*sync_fs)(struct super_block *sb, int wait);
	int (*freeze_fs)(*unfreeze_fs) (struct super_block *);
	int (*statfs) (struct dentry *, struct kstatfs *);
	int (*remount_fs) (struct super_block *, int *, char *);
	int (*show_options)(*show_devname)(*show_path)(*show_stats)(struct seq_file *, struct dentry *);
	int (*bdev_try_to_free_page)(struct super_block*, struct page*, gfp_t);
	int (*nr_cached_objects)(struct super_block *);
	void (*free_cached_objects)(struct super_block *, int);
};
struct file_system_type {
	const char *name; int fs_flags;
	struct dentry *(*mount) (struct file_system_type *, int, const char *, void *);
	void (*kill_sb) (struct super_block *);	struct module *owner;	struct file_system_type * next;	struct hlist_head fs_supers;
	struct lock_class_key s_lock_key, s_umount_key, s_vfs_rename_key, s_writers_key[SB_FREEZE_LEVELS];
	struct lock_class_key i_lock_key, i_mutex_key, i_mutex_dir_key;
};
struct super_block {
  struct list_head s_list; /* Keep this first */
  dev_t s_dev; /* search index; _not_ kdev_t */
  unsigned char s_blocksize_bits;
  unsigned long s_blocksize;
  loff_t s_maxbytes; /* Max file size */ //初始化为0xffffffff
  struct file_system_type *s_type; //存放本超级块对象对应的文件系统类型
  const struct super_operations *s_op;//超级块函数集
  const struct dquot_operations *dq_op;
  const struct quotactl_ops *s_qcop;
  const struct export_operations *s_export_op;
  unsigned long s_flags; //|=MS_NODIRATIME，不需要更新目录更新时间
  unsigned long s_magic; //MSDOS_SUPER_MAGIC,fat32文件系统
  struct dentry *s_root;     ／／根目录的dentry，链接"/"和对应boot扇区0的inode
  struct rw_semaphore s_umount;
  int s_count;
  atomic_t s_active;
  const struct xattr_handler **s_xattr;

  struct list_head s_inodes; /* all inodes */
  struct hlist_bl_head s_anon; /* anonymous dentries for (nfs) exporting */
  struct list_head s_files;
  struct list_head s_mounts; /* list of mounts; _not_ for fs use */
  /* s_dentry_lru, s_nr_dentry_unused protected by dcache.c lru locks */
  struct list_head s_dentry_lru; /* unused dentry lru */
  int s_nr_dentry_unused; /* # of dentry on lru */

  /* s_inode_lru_lock protects s_inode_lru and s_nr_inodes_unused */
  spinlock_t s_inode_lru_lock ____cacheline_aligned_in_smp;
  struct list_head s_inode_lru; /* unused inode lru */
  int s_nr_inodes_unused; /* # of inodes on lru */
  struct block_device *s_bdev;  struct backing_dev_info *s_bdi;  struct mtd_info *s_mtd;  struct hlist_node s_instances;
  struct quota_info s_dquot; /* Diskquota specific options */
  struct sb_writers s_writers;
  char s_id[32]; /* Informational name */
  u8 s_uuid[16]; /* UUID */
  void *s_fs_info; /* Filesystem private info */     //指向struct msdos_sb_info结构，fat32的管理信息
  unsigned int s_max_links;
  fmode_t s_mode;
  u32 s_time_gran; /* Granularity of c/m/atime in ns. Cannot be worse than a second */

  /* The next field is for VFS *only*. No filesystems have any business even looking at it. You had been warned. */
  struct mutex s_vfs_rename_mutex; /* Kludge */
  char *s_subtype;  /* Filesystem subtype. If non-empty the filesystem type field in /proc/mounts will be "type.subtype" */
  char __rcu *s_options; /* Saved mount options for lazy filesystems using generic_show_options() */
  const struct dentry_operations *s_d_op; /* default d_op for dentries */
  int cleancache_poolid; /* Saved pool identifier for cleancache (-1 means none) */
  struct shrinker s_shrink; /* per-sb shrinker handle */
  atomic_long_t s_remove_count; /* Number of inodes with nlink == 0 but still referenced */
  int s_readonly_remount; /* Being remounted read-only */
};

const struct file_operations fat_dir_operations = {
    .llseek = generic_file_llseek, .read = generic_read_dir, .readdir = fat_readdir,
    .unlocked_ioctl    = fat_dir_ioctl, .fsync = fat_file_fsync,};
static const struct inode_operations vfat_dir_inode_operations = {
    .create = vfat_create, .lookup = vfat_lookup, .unlink = vfat_unlink, .mkdir = vfat_mkdir,
    .rmdir = vfat_rmdir,　.rename = vfat_rename, .setattr = fat_setattr, .getattr = fat_getattr,
};
static const struct super_operations fat_sops = {
    .alloc_inode    = fat_alloc_inode,　  .destroy_inode    = fat_destroy_inode,
    .write_inode    = fat_write_inode,  //用于文件缓存的回写回调
    .evict_inode    = fat_evict_inode,    .put_super    = fat_put_super,
　 .statfs        = fat_statfs,     .remount_fs    = fat_remount,     .show_options    = fat_show_options,
};
struct inode *fat_alloc_inode(struct super_block *sb)
  struct msdos_inode_info *ei　= kmem_cache_alloc(fat_inode_cachep, GFP_NOFS);
  init_rwsem(&ei->truncate_lock);　return &ei->vfs_inode;
int fat_write_inode(struct inode *inode, struct writeback_control *wbc)
  struct super_block *sb = inode->i_sb;
  if (inode->i_ino == MSDOS_FSINFO_INO)
		mutex_lock(&MSDOS_SB(sb)->s_lock); err = fat_clusters_flush(sb); mutex_unlock(&MSDOS_SB(sb)->s_lock);
	|--> else __fat_write_inode(inode, wbc->sync_mode == WB_SYNC_ALL);
    i_pos = fat_i_pos_read(sbi, inode); fat_get_blknr_offset(sbi, i_pos, &blocknr, &offset);
    bh = sb_bread(sb, blocknr); raw_entry = &((struct msdos_dir_entry *) (bh->b_data))[offset];
    raw_entry->size = cpu_to_le32(inode->i_size); raw_entry->attr = fat_make_attrs(inode);
    fat_set_start(raw_entry, MSDOS_I(inode)->i_logstart);
    mark_buffer_dirty(bh); sync_dirty_buffer(bh);
int vfat_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)
  cluster = fat_alloc_new_dir(dir, &ts);
    fat_alloc_clusters(dir, &cluster, 1); blknr = fat_clus_to_blknr(sbi, cluster); bhs[0] = sb_getblk(sb, blknr);
    de = (struct msdos_dir_entry *)bhs[0]->b_data;//开始在新分配的cluster上填写目录信息
    memcpy(de[0].name, MSDOS_DOT, MSDOS_NAME); memcpy(de[1].name, MSDOS_DOTDOT, MSDOS_NAME); ...;
    fat_set_start(&de[0], cluster); //.指向新分配的的开始cluster
    fat_set_start(&de[1], MSDOS_I(dir)->i_logstart); //..指向父目录的开始cluster
    de[0].size = de[1].size = 0; //基于第一个block创建并初始化.和..两个目录项，包括设置start cluster
    memset(de + 2, 0, sb->s_blocksize - 2 * sizeof(*de)); set_buffer_uptodate(bhs[0]);
    mark_buffer_dirty_inode(bhs[0], dir);
    fat_zeroed_cluster(dir, blknr, 1, bhs, MAX_BUF_PER_PAGE); //零初始化cluster剩余的空间
    return cluster;
  vfat_add_entry(dir, &dentry->d_name, 1, cluster, &ts, &sinfo); dir->i_version++; inc_nlink(dir);
  inode = fat_build_inode(sb, sinfo.de, sinfo.i_pos); brelse(sinfo.bh); inode->i_version++; set_nlink(inode, 2);
  inode->i_mtime = inode->i_atime = inode->i_ctime = ts; dentry->d_time = dentry->d_parent->d_inode->i_version;
  d_instantiate(dentry, inode);

int fat_get_entry(struct inode *dir, loff_t *pos, struct buffer_head **bh, struct msdos_dir_entry **de)
  //如果老的扇区的目录项没有遍历到结尾
  if (*bh && *de && (*de - (struct msdos_dir_entry *)(*bh)->b_data) <	MSDOS_SB(dir->i_sb)->dir_per_block - 1)
		*pos += sizeof(struct msdos_dir_entry); (*de)++; return 0;
  //否则优先使用当前目录已经分配的簇中的空闲或者标记为删除的位置，可能会需要找到连续多个位置才能满足需求
  |--> return fat__get_entry(dir, pos, bh, de);
    *bh = NULL; iblock = *pos >> sb->s_blocksize_bits;
    fat_bmap(dir, iblock, &phys, &mapped_blocks, 0);//逻辑iblock换算成物理phys,称之为block mapping
    fat_dir_readahead(dir, iblock, phys); //预读phys所在整个簇的目录项
    *bh = sb_bread(sb, phys);//获取phys扇区的buffer head
    offset = *pos & (sb->s_blocksize - 1);
    *pos += sizeof(struct msdos_dir_entry); *de = (struct msdos_dir_entry *)((*bh)->b_data + offset); return 0;
int vfat_create(struct inode *dir, struct dentry *dentry, umode_t mode,bool excl)//创建并且初始化inode,更新dentry信息
  |--> vfat_add_entry(dir, &dentry->d_name, 0, 0, &ts, &sinfo);
    slots = kmalloc(sizeof(*slots) * MSDOS_SLOTS, GFP_NOFS);//分配临时缓冲区来保存目录信息
    |--> vfat_build_slots(dir, qname->name, len, is_dir, cluster, ts, slots, &nr_slots);//在缓存区中填充短名信息 struct msdos_dir_entry
      //初始化若干msdos_dir_slot，如果是短名，nr_slots为１,msdos_dir_entry就够不需要额外的msdos_dir_slot定义
      //初始化新的目录文件的内容，之后写入
      de->attr = ..;de->lcase = lcase;;de->time = de->ctime = time;
      de->date = de->cdate = de->adate = date; de->ctime_cs = time_cs; de->size = 0;
      fat_set_start(de, cluster);     //vfat_build_slots函数对短名只有设置地址这个重要的动作，在slots指针返回一个目录项
    //step 1:找到一个空闲的dir_entry, step 2:把缓冲区中目录信息拷贝过去 step 3:在sinfo中返回关键控制信息
    |--> fat_add_entries(dir, slots, nr_slots, sinfo);//只考虑短名字的场景，nr_slots这里为１
      while (fat_get_entry(dir, &pos, &bh, &de) > -1) //遍历寻找空闲的dir_entry
        if (IS_FREE(de->name))　get_bh(bh); bhs[nr_bhs] = prev = bh; nr_bhs++; free_slots++; break;
      int offset = pos & (sb->s_blocksize - 1);
      memcpy(bhs[i]->b_data + offset, slots, copy); //写入目录项
      mark_buffer_dirty_inode(bhs[i], dir);
      if (IS_DIRSYNC(dir)) err = sync_dirty_buffer(bhs[i]);
      sinfo->slot_off = pos; sinfo->de = de; sinfo->bh = bh;
      sinfo->i_pos = fat_make_i_pos(sb, sinfo->bh, sinfo->de);//计算目录项指针偏移索引，单位为目录项(32byte)；表示这是分区上第几个目录项
    dir->i_ctime = dir->i_mtime = dir->i_atime = *ts;
    if (IS_DIRSYNC(dir)) fat_sync_inode(dir); else mark_inode_dirty(dir);
  |--> inode = fat_build_inode(sb, sinfo.de, sinfo.i_pos);
    //为目录文件创建并且初始化inode，主要是几个ops，链接到hash中
    inode = fat_iget(sb, i_pos);  if(inode == null)  inode = new_inode(sb);//找到已有的inode，否则新建一个
    |--> fat_fill_inode(inode, de);               //用de目录项的内容填充Ｉｎｏｄｅ信息
      inode->i_op = &fat_file_inode_operations;
      inode->i_fop = &fat_file_operations;
      inode->i_mapping->a_ops = &fat_aops;
      MSDOS_I(inode)->i_start = fat_get_start(sbi, de); //ｆｉｌｌ文件开始位置和长度以及ｏｐｓ操作函数集
      //inode并没有实际在磁盘上存放．从磁盘上文件目录项中获取开始簇信息，存在inode中．
      inode->i_size = le32_to_cpu(de->size);　以及其他信息的填充
    fat_attach(inode, i_pos); insert_inode_hash(inode);
  inode->i_version++; inode->i_mtime = inode->i_atime = inode->i_ctime = ts;
  dentry->d_time = dentry->d_parent->d_inode->i_version;
  |--> d_instantiate(dentry, inode);
    hlist_add_head(&dentry->d_alias, &inode->i_dentry); //inode通过自己的i_dentry节点链接到dentry
    dentry->d_inode = inode; //可以通过dentry索引到inode

void setup(struct super_block *sb)
  MSDOS_SB(sb)->dir_ops = &vfat_dir_inode_operations;
	if (MSDOS_SB(sb)->options.name_check != 's') sb->s_d_op = &vfat_ci_dentry_ops;
	else sb->s_d_op = &vfat_dentry_ops;
//设定ｆａｔ３２文件系统加载超级快的函数为vfat_fill_super
static int vfat_fill_super(struct super_block *sb, void *data, int silent)
  return fat_fill_super(sb, data, silent, 1, setup);
    struct msdos_sb_info* sbi = kzalloc(sizeof(struct msdos_sb_info), GFP_KERNEL); //分配超级块扩展fs信息
    sb->s_fs_info = sbi; //超级块私有信息设定为fat32结构体
    sb->s_op = &fat_sops; //fat32超级块函数集
    sb->s_export_op = &fat_export_ops;　//fat32 nfs?
    setup(sb); sb_min_blocksize(sb, 512); bh = sb_bread(sb, 0);　读取超级快管理信息并用bh的内容初始化sbi结构;
    struct fat_boot_sector *b = (struct fat_boot_sector *) bh->b_data;
    media = b->media; logical_sector_size = get_unaligned_le16(&b->sector_size); sbi->sec_per_clus = b->sec_per_clus;
    sbi->cluster_size = sb->s_blocksize * sbi->sec_per_clus; sbi->cluster_bits = ffs(sbi->cluster_size) - 1;
    sbi->fats = b->fats; sbi->fat_start = le16_to_cpu(b->reserved); sbi->fat_length = le16_to_cpu(b->fat_length);
    sbi->root_cluster = sbi->fat_bits = sbi->free_clus_valid = 0; sbi->free_clusters = -1;
    sbi->prev_free = FAT_START_ENT; sb->s_maxbytes = 0xffffffff;
    if (!sbi->fat_length && b->fat32.length)
      struct fat_boot_fsinfo *fsinfo; struct buffer_head *fsinfo_bh;
      sbi->fat_bits = 32; sbi->fat_length = le32_to_cpu(b->fat32.length); sbi->root_cluster = le32_to_cpu(b->fat32.root_cluster);
      sbi->fsinfo_sector = le16_to_cpu(b->fat32.info_sector);
      if (sbi->fsinfo_sector == 0)	sbi->fsinfo_sector = 1;
      fsinfo_bh = sb_bread(sb, sbi->fsinfo_sector); fsinfo = (struct fat_boot_fsinfo *)fsinfo_bh->b_data;
      if (sbi->options.usefree)	sbi->free_clus_valid = 1;
      sbi->free_clusters = le32_to_cpu(fsinfo->free_clusters); sbi->prev_free = le32_to_cpu(fsinfo->next_cluster);
      brelse(fsinfo_bh);
    sbi->dir_per_block = sb->s_blocksize / sizeof(struct msdos_dir_entry);
    sbi->dir_per_block_bits = ffs(sbi->dir_per_block) - 1;
    sbi->dir_start = sbi->fat_start + sbi->fats * sbi->fat_length;
    sbi->dir_entries = get_unaligned_le16(&b->dir_entries);
    rootdir_sectors = sbi->dir_entries * sizeof(struct msdos_dir_entry) / sb->s_blocksize;
    sbi->data_start = sbi->dir_start + rootdir_sectors; total_sectors = get_unaligned_le16(&b->sectors);
    if (total_sectors == 0)	total_sectors = le32_to_cpu(b->total_sect);
    total_clusters = (total_sectors - sbi->data_start) / sbi->sec_per_clus;
    sbi->dirty = b->fat32.state & FAT_STATE_DIRTY;
    fat_clusters = calc_fat_clusters(sb); total_clusters = min(total_clusters, fat_clusters - FAT_START_ENT);
    sbi->max_cluster = total_clusters + FAT_START_ENT;
    brelse(bh); fat_hash_init(sb); dir_hash_init(sb); |--> fat_ent_access_init(sb);
      sbi->fatent_shift = 2;          //表示cluster的index占用４个字节
      sbi->fatent_ops = &fat32_ops;     //fat表函数集
    fat_set_state(sb, 1, 0);     //DPB, 分区的ｂｏｏｔ信息只有一个扇区，直接通过buffer机制操作
      bh = sb_bread(sb, 0);　b = (struct fat_boot_sector *) bh->b_data;
      b->fat32.state |= FAT_STATE_DIRTY; or b->fat32.state &= ~FAT_STATE_DIRTY;
      mark_buffer_dirty(bh);     sync_dirty_buffer(bh);
    fat_inode = new_inode(sb); MSDOS_I(fat_inode)->i_pos = 0; sbi->fat_inode = fat_inode;
    fsinfo_inode = new_inode(sb); fsinfo_inode->i_ino = MSDOS_FSINFO_INO; sbi->fsinfo_inode = fsinfo_inode;
    insert_inode_hash(fsinfo_inode);
    root_inode = new_inode(sb); root_inode->i_ino = MSDOS_ROOT_INO; root_inode->i_version = 1;
    |--> fat_read_root(root_inode); //root inode初始化
      MSDOS_I(inode)->i_pos = MSDOS_ROOT_INO; inode->i_uid = sbi->options.fs_uid;
      inode->i_gid = sbi->options.fs_gid; inode->i_version++; inode->i_generation = 0;
      inode->i_mode = fat_make_mode(sbi, ATTR_DIR, S_IRWXUGO); inode->i_op = sbi->dir_ops; inode->i_fop = &fat_dir_operations;
      MSDOS_I(inode)->i_start = sbi->root_cluster; fat_calc_dir_size(inode);
      inode->i_blocks = ((inode->i_size + (sbi->cluster_size - 1)) & ~((loff_t)sbi->cluster_size - 1)) >> 9;
      MSDOS_I(inode)->i_logstart = 0;　MSDOS_I(inode)->mmu_private = inode->i_size;
    insert_inode_hash(root_inode); fat_attach(root_inode, 0);
    sb->s_root = d_make_root(root_inode);//生成根目录对应的inode和dentry并初始化，"/"对应１扇区

static struct dentry *vfat_mount(struct file_system_type *fs_type,...) //in namei_vfat.c文件中
  return mount_bdev(fs_type, flags, dev_name, data, vfat_fill_super);
    struct block_device * bdev = blkdev_get_by_path(dev_name, mode, fs_type);//找到块设备
    struct super_block *s= sget(fs_type, test_bdev_super, set_bdev_super, flags | MS_NOSEC, bdev);//分配并初始化超级块
      s = alloc_super(type, flags);  s->s_type = type;     //文件系统struct file_system_type *fs_type
      strlcpy(s->s_id, type->name, sizeof(s->s_id));
      list_add_tail(&s->s_list, &super_blocks);     //添加到超级块链表
      hlist_add_head(&s->s_instances, &type->fs_supers);　//添加到文件系统链表
      get_filesystem(type);      register_shrinker(&s->s_shrink);
    s->s_mode = mode;　          strlcpy(s->s_id, bdevname(bdev, b), sizeof(s->s_id));
    sb_set_blocksize(s, block_size(bdev));
    s->s_flags |= MS_ACTIVE;     //继续mount_bdev函数
    bdev->bd_super = s;
    return dget(s->s_root);

//sys_mount() namespace.c文件
SYSCALL_DEFINE5(mount, char __user* dev_name, char __user* dir_name,char __user * type, unsigned long flags, void __user* data)
  copy_mount_string(type, &kernel_type); struct filename *kernel_dir = getname(dir_name);
  copy_mount_string(dev_name, &kernel_dev); copy_mount_options(data, &data_page); ((char *)data_page)[PAGE_SIZE - 1] = 0;
  |--> do_mount(kernel_dev, kernel_dir->name, kernel_type, flags,(void *) data_page);
    kern_path(dir_name, LOOKUP_FOLLOW, &path);
    |--> do_new_mount(&path, type_page, flags, mnt_flags,dev_name, data_page);
      struct file_system_type *type = get_fs_type(fstype);//获取文件系统登记的信息，主要是ops操作函数集,包括vfat_mount()函数
      |--> mnt = vfs_kern_mount(type, flags, name, data);
        mnt = alloc_vfsmnt(name);     //分配vfs的mount节点
        |--> root = mount_fs(type, flags, name, data);     //读取磁盘上超级块信息，初始化文件系统和超级块
          root = type->mount(type, flags, name, data);   //调用 vfat_mount()
          sb = root->d_sb;
        mnt->mnt_mountpoint = mnt->mnt.mnt_root = root; mnt->mnt.mnt_sb = root->d_sb; mnt->mnt_parent = mnt;
        br_write_lock(&vfsmount_lock); list_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);//添加到超级块的mount节点链表中
      put_filesystem(type);
      |--> do_add_mount(real_mount(mnt), path, mnt_flags);//把新mount节点挂载到路径对应的父节点上
        parent = real_mount(path->mnt);         //获得路径对应的parent挂载点
        newmnt->mnt.mnt_flags = mnt_flags;
        err = graft_tree(newmnt, parent, mp); //把newmnt挂载到parent的树

long sys_mknod(const char __user *filename, umode_t mode,unsigned dev);//SYSCALL_DEFINE3(mknod
  |--> return sys_mknodat(AT_FDCWD, filename, mode, dev);     //SYSCALL_DEFINE4(mknodat
    dentry = user_path_create(dfd, filename, &path, lookup_flags); //创建Ｐａｔｈ和页目录项

//在open的时候会将inode中结构体struct file_operations i_fop保存的函数指针赋值给file中的结构体struct file_operations f_op，以后read，write等函数调用的都是这里的struct file_operations结构体中的回调函数。
SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) //sys_open()-->do_sys_open
long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)
  int lookup = build_open_flags(flags, mode, &op);
  fd = get_unused_fd_flags(flags); //取回一个未被使用的文件描述符（每次都会选取最小的未被使用的文件描述符）
  |--> f = do_filp_open(dfd, getname(filename), &op, lookup);//取出和该文件相关的 dentry 和 inode,然后调用 do_dentry_open() 函数创建新的 file 对象，并用 dentry 和 inode 中的信息初始化 file 对象
    filp = path_openat(dfd, pathname, &nd, op, flags | LOOKUP_RCU);
      file = get_empty_filp(); path_init(dfd, pathname->name, flags | LOOKUP_PARENT, nd, &base); link_path_walk(pathname->name, nd);
      |--> do_last(nd, &path, file, op, &opened, pathname);
        |--> lookup_open(nd, path, file, op, got_write, opened);//Look up and maybe create and open the last component.
          struct dentry *dentry = lookup_dcache(&nd->last, dir, nd->flags, &need_lookup);
          //先lookup文件，找不到的话可能需要新建文件
          vfs_create(dir->d_inode, dentry, mode, nd->flags & LOOKUP_EXCL);//-->dir->i_op->create()-->vfat_create()
          path->dentry = dentry; path->mnt = nd->path.mnt;
        |--> finish_open(file, nd->path.dentry, NULL, opened);
          file->f_path.dentry = dentry;
          |--> do_dentry_open(file, open, current_cred());
            f->f_mode = OPEN_FMODE(f->f_flags) | FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE;
            path_get(&f->f_path); inode = f->f_path.dentry->d_inode; f->f_mapping = inode->i_mapping;
            file_sb_list_add(f, inode->i_sb);
            f->f_op = fops_get(inode->i_fop); f->f_op->open(inode, f);//获取ops并调用open()接口
            f->f_flags &= ~(O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC); file_ra_state_init(&f->f_ra, f->f_mapping->host->i_mapping);
      return file;
  fsnotify_open(f);
  fd_install(fd, f); //以文件描述符为索引，关联当前进程描述符和上述的 file 对象，为之后的 read 和 write 等操作作准备
  putname(tmp); return fd

```

### 5.2 cache.c和inode.c和fatent.c

fatent应该是管理fat表中数据的，`sbi->fat_start`对应fat表的开始扇区

cache.c文件主要提供下面两个函数接口：
- `int fat_get_cluster(struct inode *inode, int cluster, int *fclus, int *dclus)`，获取指定cluster对应的物理cluster
- `int fat_bmap(struct inode *inode, sector_t sector, sector_t *phys, unsigned long *mapped_blocks, int create)`，

misc.c中提供两个打印vfat调试信息的接口：
- `void __fat_fs_error(struct super_block *sb, int report, const char *fmt, ...)`
- `void fat_msg(struct super_block *sb, const char *level, const char *fmt, ...)`

misc.c中提供unix和vfat时间格式的转化：
- `void fat_time_fat2unix(struct msdos_sb_info *sbi, struct timespec *ts, __le16 __time, __le16 __date, u8 time_cs)`
- `void fat_time_unix2fat(struct msdos_sb_info *sbi, struct timespec *ts, __le16 *time, __le16 *date, u8 *time_cs)`

file.c中主要提供`struct file_operations fat_file_operations`的定义，`struct inode_operations fat_file_inode_operations`的定义暂时不解析

```cpp
//part 1 :: fatent.c 解析，只考虑32bits的vfat
static struct fatent_operations fat32_ops = {
     .ent_blocknr = fat_ent_blocknr,　//计算fat表中簇entry对应4个byte的物理block号和block内offset
     .ent_set_ptr = fat32_ent_set_ptr,//簇对象指针改变块内offset位置
     .ent_bread = fat_ent_bread,      //簇对象根据物理位置信息获得buffer head信息，指针定位到offset
     .ent_get = fat32_ent_get,　      //读取簇对象内容，就是簇对象指针指向的内容
     .ent_put = fat32_ent_put,　      //设定簇对象内容，就是它的下一个簇的簇号
     .ent_next = fat32_ent_next,      //簇对象重置为下一个簇号
};
struct fat_entry {
     int entry;　                    // 代表当前的簇索引
     union {  　u8 *ent12_p[2];__le16 *ent16_p;__le32 *ent32_p;　　} u;// 簇索引表指针
     int nr_bhs;                    // buffer_head数目，可能是1也可能是2，FAT32是1
     struct buffer_head *bhs[2];          // FAT表的扇区的buffer_head
     struct inode *fat_inode;            //超级块的inode
};

int fat_alloc_clusters(struct inode *inode, int *cluster, int nr_cluster) //找到nr_cluster个空闲cluster,保存在cluster[]数组中
  struct buffer_head *bhs[MAX_BUF_PER_PAGE];
  BUG_ON(nr_cluster > (MAX_BUF_PER_PAGE / 2)); /* fixed limit 限制一次最多分配４个*/
  fatent_init(&prev_ent); fatent_init(&fatent); fatent_set_entry(&fatent, sbi->prev_free + 1);     //一个可能不错的开始点
  while (count < sbi->max_cluster)
    fat_ent_read_block(sb, &fatent);  //读取entry所在块
    do
      //跳过dirty的cluster;     //(ops->ent_get(&fatent) != FAT_ENT_FREE)
      ops->ent_put(&fatent, FAT_ENT_EOF); ops->ent_put(&prev_ent, entry); //标示当前entry为最后一个；上一个块，指向当前entry
      fat_collect_bhs(bhs, &nr_bhs, &fatent);//bhs[]数组保存dirty的buffer head指针,之后fat_sync_bhs　fat_mirror_bhs brelse使用
      sbi->prev_free = entry; cluster[idx_clus++] = entry; //cluster[]保存分配的几个entry
      prev_ent = fatent;
    while (fat_ent_next(sbi, &fatent));
  mark_fsinfo_dirty(sb);　//mark元数据为dirty
  if (inode_needs_sync(inode))
    |--> err = fat_sync_bhs(bhs, nr_bhs);
      for (i = 0; i < nr_bhs; i++)     write_dirty_buffer(bhs[i], WRITE);
      for (i = 0; i < nr_bhs; i++)     wait_on_buffer(bhs[i]);
  if (!err)               err = fat_mirror_bhs(sb, bhs, nr_bhs);

int fat_free_clusters(struct inode *inode, int cluster)  //free指定的以及之后的所有cluster
  fatent_init(&fatent);
  do
    cluster = fat_ent_read(inode, &fatent, cluster);//获取当前cluster的下一个cluster
    ops->ent_put(&fatent, FAT_ENT_FREE);
    sbi->free_clusters++;
    //一些bhs操作：　fat_sync_bhs　fat_mirror_bhs　fat_collect_bhs
  while (cluster != FAT_ENT_EOF); //一直到文件结束
  if (dirty_fsinfo)     mark_fsinfo_dirty(sb);

int fat_ent_write(struct inode *inode, struct fat_entry *fatent, int new, int wait)//设定fatent的下一个是new
     ops->ent_put(fatent, new);          //当前entry项next cluster设定为new
     if (wait)     err = fat_sync_bhs(fatent->bhs, fatent->nr_bhs);
     return fat_mirror_bhs(sb, fatent->bhs, fatent->nr_bhs);

int fat_count_free_clusters(struct super_block *sb)//解析 fat表获得空闲cluster个数
  fatent_init(&fatent);     fatent_set_entry(&fatent, FAT_START_ENT);     //初始化fatent到fat表起始位置
  fat_ent_reada(sb, &fatent, min(reada_blocks, rest));//多读点效率高,调用sb_breadahead()函数，提前读取磁盘数据到文件内存映射中
  |--> fat_ent_read_block(sb, &fatent);//如果fatent没有有效内容信息，就获取fatent.entry对应簇所在扇区
      ops->ent_blocknr(sb, fatent->entry, &offset, &blocknr);
      return ops->ent_bread(sb, fatent, offset, blocknr);
  do  //处理读取的一个块，对应一个buf_head，一般包含了512/4个cluster entry
    if (ops->ent_get(&fatent) == FAT_ENT_FREE) {
      free++; if (sbi->options.discard) sb_issue_discard(sb, fat_clus_to_blknr(sbi, fatent.entry), sbi->sec_per_clus, GFP_NOFS, 0);
    }
  while(fat_ent_next(sbi, &fatent));　//遍历同一个buffer_head管理的块上的fat表簇号区域
  sbi->free_clusters = free;  sbi->free_clus_valid = 1;
  mark_fsinfo_dirty(sb);
      __mark_inode_dirty(sbi->fsinfo_inode, I_DIRTY_SYNC);
  return free;
```

```cpp
//part 2 :: inode.c cache.c 解析
//实际上，为了更快从整体理解代码架构，先忽略cache的中间作用
static struct kmem_cache *fat_cache_cachep, *fat_inode_cachep;

int fat_file_release(struct inode *inode, struct file *filp)
  if ((filp->f_mode & FMODE_WRITE) && MSDOS_SB(inode->i_sb)->options.flush)
		|--> fat_flush_inodes(inode->i_sb, inode, NULL);
      writeback_inode(i1); writeback_inode(i2); //回写元数据和文件数据
        sync_inode_metadata(inode, 0); --> sync_inode　//回写元数据
        filemap_fdatawrite(inode->i_mapping); //-->__filemap_fdatawrite()-->__filemap_fdatawrite_range()
      filemap_flush(sb->s_bdev->bd_inode->i_mapping);
    congestion_wait(BLK_RW_ASYNC, HZ/10);


const struct file_operations fat_file_operations = {
	.llseek		= generic_file_llseek, .read		= do_sync_read,	.write		= do_sync_write,
	.aio_read	= generic_file_aio_read,	.aio_write	= generic_file_aio_write,	.mmap		= generic_file_mmap,
	.release	= fat_file_release,	.unlocked_ioctl	= fat_generic_ioctl,	.fsync		= fat_file_fsync,
	.splice_read	= generic_file_splice_read, };

static const struct address_space_operations fat_aops = {
	.readpage	= fat_readpage, //mpage_readpage(page, fat_get_block);
	.readpages = fat_readpages, //mpage_readpages(mapping, pages, nr_pages, fat_get_block);
	.writepage = fat_writepage, //block_write_full_page(page, fat_get_block, wbc);
	.writepages	= fat_writepages, //mpage_writepages(mapping, wbc, fat_get_block);
	.write_begin = fat_write_begin, //cont_write_begin(file, mapping, pos, len, flags,　pagep, fsdata,
          fat_get_block,&MSDOS_I(mapping->host)->mmu_private);
	.write_end = fat_write_end, //generic_write_end(file, mapping, pos, len, copied, pagep, fsdata);
	.direct_IO = fat_direct_IO, //blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs, fat_get_block);
	.bmap = _fat_bmap //return (blocknr = generic_block_bmap(mapping, block, fat_get_block));
};
static struct file_system_type vfat_fs_type = {
    .owner        = THIS_MODULE,    .name        = "vfat",    .mount        = vfat_mount,
    .kill_sb    = kill_block_super,    .fs_flags    = FS_REQUIRES_DEV,
};
int __init init_fat_fs(void)
  |--> err = fat_cache_init();
    fat_cache_cachep = kmem_cache_create("fat_cache", sizeof(struct fat_cache),	0, SLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD, init_once);
  |--> err = fat_init_inodecache();
    fat_inode_cachep = kmem_cache_create("fat_inode_cache", sizeof(struct msdos_inode_info), 0, ..., init_once);
module_init(init_fat_fs)
int __init init_vfat_fs(void)
  return register_filesystem(&vfat_fs_type);
module_init(init_vfat_fs)

//建立从文件块号到设备扇区号的映射，实际上就是验证指定的iblock是否对应有效的文件内容,并bh_result结构和设备对应
static int fat_get_block(struct inode *inode, sector_t iblock,struct buffer_head *bh_result, int create)
  unsigned long max_blocks = bh_result->b_size >> inode->i_blkbits;
  err = __fat_get_block(inode, iblock, &max_blocks, bh_result, create);
    err = fat_bmap(inode, iblock, &phys, &mapped_blocks, create);//获取文件内偏移iblock对应的物理扇区
    if (phys){map_bh(bh_result, sb, phys);               *max_blocks = min(mapped_blocks, *max_blocks);return 0;}
      set_buffer_mapped(bh); bh->b_bdev = sb->s_bdev;　bh->b_blocknr = block;bh->b_size = sb->s_blocksize;
    //申请的扇区包含在现有文件内，返回物理扇区信息；否则，为文件增加一个簇，再返回对应扇区
    offset = (unsigned long)iblock & (sbi->sec_per_clus - 1);
    |--> fat_add_cluster(inode); //如果新的扇区是新的簇的开始扇区，就给文件添加一个新的簇再重新建立映射
      fat_alloc_clusters(inode, &cluster, 1);     //找到1个空闲的cluster,保存在cluster变量中
      |--> fat_chain_add(inode, cluster, 1);     //把cluster添加到文件的最后一个节点
        fat_get_cluster(inode, FAT_ENT_EOF, &fclus, &dclus);　//找到文件最后一个cluster
        new_fclus = fclus + 1; last = dclus; fatent_init(&fatent);
        ret = fat_ent_read(inode, &fatent, last); //好像没有用
        int wait = inode_needs_sync(inode); fat_ent_write(inode, &fatent, new_dclus, wait); //文件cluster chain添加一项
        inode->i_blocks += nr_cluster << (sbi->cluster_bits - 9);
    err = fat_bmap(inode, iblock/sector, &phys, &mapped_blocks, create);//
      last_block = (i_size_read(inode) + (blocksize - 1)) >> blocksize_bits;//文件实际使用的最后一个块
      //根据文件长度计算文件当前需要占用多少扇区，文件长度是应用实际写入的数据的长度
      //长度为０,sector 0就能触发，所以sector是从０开始计数的．
      if (sector >= last_block){//要写的sector超出当前文件大小,计算是否可以从文件实际分配的簇中找到可用块
        last_block = (MSDOS_I(inode)->mmu_private + (blocksize - 1))>> blocksize_bits;
        if (sector >= last_block)     return 0;
      }
      cluster = sector >> (sbi->cluster_bits - sb->s_blocksize_bits);//计算文件cluster id,代表文件内部偏移簇位置
      offset = sector & (sbi->sec_per_clus - 1);                                        //计算cluster内偏移多少个sector
      cluster = fat_bmap_cluster(inode, cluster);    //返回文件逻辑簇号对应的磁盘物理cluster id
        |--> ret = fat_get_cluster(inode, cluster, &fclus, &dclus);
          fatent_init(&fatent);
          while (*fclus < cluster)
            nr = fat_ent_read(inode, &fatent, *dclus);  //获取当前entry: *dclus位置保存的next cluster的entry
      *phys = fat_clus_to_blknr(sbi, cluster) + offset;        //返回磁盘sector id
      mapped_blocks = sbi->sec_per_clus - offset;        　//当前簇中在*phys扇区之后还有多少块
    *max_blocks = min(mapped_blocks, *max_blocks);
  bh_result->b_size = max_blocks << sb->s_blocksize_bits;　//计算映射的实际bytes
  if (phys)     map_bh(bh_result, sb, phys);     //搜索到有效的扇区，设置bh->b_bdev为设备
```

### 5.3 write()

参考文档:　http://blog.sina.com.cn/s/blog_a558c25a0102vgid.html

`page_writeback.c`主要提供下面函数接口，提供：
- page_writeback_init(void) //初始化rate limit数据
- do_writepages()
- generic_writepages()
- write_cache_pages()
- balance_dirty_pages_ratelimited()

```cpp
struct bio {
  sector_t bi_sector; //要传输的第一个扇区。
  struct bio *bi_next;  //链接到请求队列的下一个bio
  struct block_device *bi_bdev;//指向块设备描述符的指针
  unsigned long bi_flags; //bio的状态标志
  unsigned long bi_rw;//IO操作标志 READ/WRITE
  unsigned short bi_vcnt; //bio中的bio_vec数组中当前元素的个数
  unsigned short bi_idx; //bio的bio_vec数组中段的当前索引值
  unsigned int bi_phys_segments; //合并后bio中的物理段的数目
  unsigned int bi_size;  //所需传输的数据字节数
  unsigned int bi_seg_front_size;
  unsigned int bi_seg_back_size;
  unsigned int bi_max_vecs; //bio的bio_vec数组中允许的最大段数
  unsigned int bi_comp_cpu; /* completion CPU*/
  atomic_t bi_cnt; //bio引用计数器
  struct bio_vec *bi_io_vec; /* the actual vec list*/
  bio_end_io_t *bi_end_io;//bio的IO操作结束时调用的方法
  void *bi_private;//通用块层和块设备驱动程序的IO完成方法使用的指针
  bio_destructor_t *bi_destructor; //释放bio时调用的析构方法
};
struct mpage_data {
	struct bio *bio;
	sector_t last_block_in_bio;
	get_block_t *get_block;
	unsigned use_writepage;
};
int mpage_writepage(struct page *page, get_block_t get_block, struct writeback_control *wbc)
  struct mpage_data mpd = {.bio = NULL, .last_block_in_bio = 0,	.get_block = get_block,	.use_writepage = 0,	};
  |--> __mpage_writepage(page, wbc, &mpd);
    bio = mpage_alloc(bdev, blocks[0] << (blkbits - 9), bio_get_nr_vecs(bdev), GFP_NOFS|__GFP_HIGH);
    bio_add_page(bio, page, length, 0);
    |--> bio = mpage_bio_submit(WRITE, bio);
      bio->bi_end_io = mpage_end_io; submit_bio(rw, bio);
int mpage_writepages(struct address_space *mapping, struct writeback_control *wbc, get_block_t get_block)
  struct mpage_data mpd = {.bio = NULL, .last_block_in_bio = 0,	.get_block = get_block,	.use_writepage = 1,	};
  blk_start_plug(&plug); //把紧密耦合的数据块先plug，全部request完毕后unplug，再一起加入到块设备的queue唤醒处理thread，request_queue线程取request需要spin_lock_irq(q->queue_lock);如果每个request进入queue也都要lock，必然导致频繁的竞争，所以通过plug的方式，减少竞争
  |--> write_cache_pages(mapping, wbc, __mpage_writepage, &mpd);//构建脏页链表，逐个写入
    nr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag, min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);
    for (i = 0; i < nr_pages; i++)
      struct page *page = pvec.pages[i];
      (*writepage)(page, wbc, data); //__mpage_writepage()
  blk_finish_plug(&plug); //阻塞调用

int fat_write_begin(struct file *file, struct address_space *mapping, loff_t pos, unsigned len, unsigned flags, struct page **pagep, void **fsdata)
  |--> cont_write_begin(file, mapping, pos, len, flags, pagep, fsdata, fat_get_block, &MSDOS_I(mapping->host)->mmu_private);
    |--> return block_write_begin(mapping, pos, len, flags, pagep, get_block);//--> __block_write_begin()
      //保证所有的buffer_head已经分配物理扇区，保证数据同步
      unsigned from = pos & (PAGE_CACHE_SIZE - 1), to = from + len; struct inode *inode = page->mapping->host;
      head = create_page_buffers(page, inode, 0); blocksize = head->b_size; bbits = block_size_bits(blocksize);
      block = (sector_t)page->index << (PAGE_CACHE_SHIFT - bbits);
      for(bh = head, block_start = 0; bh != head || !block_start; block++, block_start=block_end, bh = bh->b_this_page)
        block_end = block_start + blocksize;
        if (buffer_new(bh))	clear_buffer_new(bh);
		    if (!buffer_mapped(bh))
          get_block(inode, block, bh, 1);　
          if (buffer_new(bh)) unmap_underlying_metadata(bh->b_bdev,bh->b_blocknr); continue;
        if (!buffer_uptodate(bh) && !buffer_delay(bh) && !buffer_unwritten(bh) && (block_start < from || block_end > to))
          |--> ll_rw_block(READ, 1, &bh);//void ll_rw_block(int rw, int nr, struct buffer_head *bhs[])
            for (i = 0; i < nr; i++) bh = bhs[i]; get_bh(bh); submit_bh(rw, bh);
           *wait_bh++=bh; //make sure data ready in buffer
      while(wait_bh > wait) wait_on_buffer(*--wait_bh); //同步访问

void __mark_inode_dirty(struct inode *inode, int flags)
  bdi = inode_to_bdi(inode);
  int was_dirty = inode->i_state & I_DIRTY;//trace中，每writeback_interval，被清除一次，
  if (!was_dirty)
    inode->dirtied_when = jiffies; //inode最早page被写入的时间是inode的dirty时间
    list_move(&inode->i_wb_list, &bdi->wb.b_dirty); //添加到bdi的writeback数据结构脏inode链表中
    if (wakeup_bdi)     bdi_wakeup_thread_delayed(bdi); //5s后第一次启动回写线程
int fat_write_end(struct file *file, struct address_space *mapping,　loff_t pos, unsigned len, unsigned copied,　struct page *pagep, void *fsdata)
  |--> generic_write_end(file, mapping, pos, len, copied, pagep, fsdata);
    |--> copied = block_write_end(file, mapping, pos, len, copied, page, fsdata);
      struct inode *inode = mapping->host; start = pos & (PAGE_CACHE_SIZE - 1); flush_dcache_page(page);
      |--> __block_commit_write(inode, page, start, start+copied);
        if (写入不完整页) mark_buffer_dirty(bh);-->__set_page_dirty();-->__mark_inode_dirty();
        if (整个页都是最新) SetPageUptodate(page);
      return copied;
    if (pos+copied > inode->i_size) i_size_write(inode, pos+copied); mark_inode_dirty(inode);
    unlock_page(page); page_cache_release(page);
    return copied;

//大部分实质性的工作都是在aops.write_begin()和aops.write_end()里做的.write_begin()之后page就ready了，用户的数据就可以copy到page中了。
SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count)
  struct fd f = fdget(fd);  loff_t pos = file_pos_read(f.file);
  |--> vfs_write(f.file, buf, count, &pos);
    file_start_write(file);
    |--> file->f_op->write(file, buf, count, pos);//do_sync_write() --> filp->f_op->aio_write();-->generic_file_aio_write()
      |--> __generic_file_aio_write(iocb, iov, nr_segs, &iocb->ki_pos);
        |--> return generic_file_buffered_write(iocb, iov, nr_segs,	pos, ppos, count, written);
          |--> generic_perform_write(file, &i, pos);
            while (next page valid)
              a_ops->write_begin(file, mapping, pos, bytes, flags, &page, &fsdata); //fat32: fat_write_begin
              copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes); //完成从用户缓冲区到page cache的复制
              a_ops->write_end(file, mapping, pos, bytes, copied,page, fsdata); //fat32: fat_write_end
              |--> balance_dirty_pages_ratelimited(mapping);
                int ratelimit = current->nr_dirtied_pause; nr_pages_dirtied = min(*p, ratelimit - current->nr_dirtied);
                current->nr_dirtied += nr_pages_dirtied;
                if (unlikely(current->nr_dirtied >= ratelimit))
                  |--> balance_dirty_pages(mapping, current->nr_dirtied);
                    struct backing_dev_info *bdi = mapping->backing_dev_info;
                    bdi_start_background_writeback(bdi); //满足特定条件时调用
    inc_syscw(current); file_end_write(file);
  file_pos_write(f.file, pos); fdput(f);

```

### 5.4 read()
参考文档: http://blog.sina.com.cn/s/blog_a558c25a0102v7pq.html

```cpp
int mpage_readpage(struct page *page, get_block_t get_block)
  |--> do_mpage_readpage(NULL, page, 1, &last_block_in_bio, &map_bh, &first_logical_block, get_block);
     mpage_alloc(bdev, blocks[0] << (blkbits - 9), min_t(int, nr_pages, bio_get_nr_vecs(bdev)),	GFP_KERNEL);
     bio_add_page(bio, page, length, 0);
     mpage_bio_submit(READ, bio);
int mpage_readpages(struct address_space *mapping, struct list_head *pages,	unsigned nr_pages, get_block_t get_block)
  for (page_idx = 0; page_idx < nr_pages; page_idx++)
    struct page *page = list_entry(pages->prev, struct page, lru);
    prefetchw(&page->flags); list_del(&page->lru);
    for (page_idx = 0; page_idx < nr_pages; page_idx++)
      struct page *page = list_entry(pages->prev, struct page, lru);
      |--> add_to_page_cache_lru(page, mapping, page->index, GFP_KERNEL);
        |--> add_to_page_cache(page, mapping, offset, gfp_mask); //-->add_to_page_cache_locked()
          radix_tree_insert(&mapping->page_tree, offset, page); //插入rb树
          mapping->nrpages++;
        lru_cache_add_file(page);//page添加到lru中，方便之后的搜寻
      bio = do_mpage_readpage(bio, page, nr_pages - page_idx,	&last_block_in_bio, &map_bh, &first_logical_block, get_block);

SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
  struct fd f = fdget(fd); loff_t pos = file_pos_read(f.file);
  |--> vfs_read(f.file, buf, count, &pos);
    |--> file->f_op->read(file, buf, count, pos);//do_sync_read-->filp->f_op->aio_read()-->generic_file_aio_read()
      |--> do_generic_file_read(filp, ppos, &desc, file_read_actor);
        for(;;)
          page = find_get_page(mapping, index);
          mapping->a_ops->readpage(filp, page);//fat_readpage()
          actor(desc, page, offset, nr); //-->file_read_actor() 将读入内容传入用户空间
  file_pos_write(f.file, pos); fdput(f);
```

### 5.5 目录操作

```cpp
SYSCALL_DEFINE2(mkdir, const char __user *, pathname, umode_t, mode) //--> sys_mkdirat(AT_FDCWD, pathname, mode);
SYSCALL_DEFINE3(mkdirat, int, dfd, const char __user *, pathname, umode_t, mode)
  struct dentry *dentry = user_path_create(dfd, pathname, &path, lookup_flags);
  security_path_mkdir(&path, dentry, mode);
  |--> vfs_mkdir(path.dentry->d_inode, dentry, mode);
    dir->i_op->mkdir(dir, dentry, mode);
  done_path_create(&path, dentry);

```


### 5.6 sync()
- sync函数只是将所有修改过的块缓冲区排入写队列，然后就返回，它并不等待实际写磁盘操作结束。通常称为update的系统守护进程会周期性地（一般每隔30秒）调用sync函数。这就保证了定期冲洗内核的块缓冲区。命令sync(1)也调用sync函数。
- fsync函数只对由文件描述符filedes指定的单一文件起作用，并且等待写磁盘操作结束，然后返回。fsync可用于数据库这样的应用程序，这种应用程序需要确保将修改过的块立即写到磁盘上。
- fdatasync函数类似于fsync，但它只影响文件的数据部分。而除数据外，fsync还会同步更新文件的属性
- msync 如果采用内存映射文件的方式进行文件IO（使用mmap，将文件的page cache直接映射到进程的地址空间，通过写内存的方式修改文件），可以使用msync()函数．msync需要指定同步的地址区间，如此细粒度的控制似乎比fsync更加高效（因为应用程序通常知道自己的脏页位置），但实际上（Linux）kernel中有着十分高效的数据结构，能够很快地找出文件的脏页，使得fsync只会同步文件的修改内容。

```cpp
//不需要回写线程的快速同步方式
SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)//-->vfs_fsync(vma->vm_file, 0);
SYSCALL_DEFINE1(fsync, unsigned int, fd) //--> do_fsync(fd, 0);
SYSCALL_DEFINE1(fdatasync, unsigned int, fd) //--> do_fsync(fd, 1);
int do_fsync(unsigned int fd, int datasync)//vfs_fsync(f.file, datasync);-->vfs_fsync_range(file, 0, LLONG_MAX, datasync);
  |--> file->f_op->fsync(file, start, end, datasync);//int fat_file_fsync(struct file *, loff_t , loff_t , int )
    |--> generic_file_fsync(filp, start, end, datasync);
      |--> filemap_write_and_wait_range(inode->i_mapping, start, end);//写入范围内所有page,然后等待所有page回写完成后再返回
        |-->__filemap_fdatawrite_range(mapping, lstart, lend, WB_SYNC_ALL);
          do_writepages(mapping, &wbc);
        |--> filemap_fdatawait_range(mapping, lstart, lend);
          struct pagevec pvec; pagevec_init(&pvec, 0);
          nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,...);
          for (i = 0; i < nr_pages; i++) wait_on_page_writeback(pvec.pages[i]);//遍历等待范围内所有page回写完毕
      sync_mapping_buffers(MSDOS_SB(inode->i_sb)->fat_inode->i_mapping);//文件系统级别回写同步
      |--> sync_inode_metadata(inode, 1);
        |-->sync_inode(inode, &wbc);
          |-->writeback_single_inode(inode, &inode_to_bdi(inode)->wb, wbc);  //__writeback_single_inode()
            |--> do_writepages(mapping, wbc);
              mapping->a_ops->writepages(mapping, wbc);//fat_writepages()
            if (wbc->sync_mode == WB_SYNC_ALL) filemap_fdatawait(mapping);//等待完成，下面判断是否写inode
            if (inode->i_state&(I_DIRTY|I_DIRTY_SYNC|I_DIRTY_DATASYNC)) write_inode(inode, wbc);
              // inode->i_sb->s_op->write_inode(inode, wbc); //fat_write_inode
    |--> sync_mapping_buffers(MSDOS_SB(inode->i_sb)->fat_inode->i_mapping);
      |--> fsync_buffers_list(&buffer_mapping->private_lock, &mapping->private_list);
        foreach buf_head write_dirty_buffer(bh, WRITE_SYNC);//调用write_dirty_buffer()写入所有buf_head
        osync_buffers_list(lock, list); //对每一个bh,　wait_on_buffer(bh);　等待回写结束
```

### 5.7 mmap()

mmap()系统调用在调用进程的虚拟地址空间中创建一个新的内存映射。munmap()系统调用执行逆操作，即从进程的地址空间删除一个映射。

映射可以分为两种：基于文件的映射和匿名映射。文件映射将一个文件区域中的内容映射到进程的虚拟地址空间中。匿名映射（通过使用MAP_ANONYMOUS）标记或映射/dev/zero来创建）并没有对应的文件区域，该映射中的字节会被初始化为0。

映射既可以是私有的（MAP_PRIVATE），也可以是共享的（MAP_SHARED）。这种差别确定了在共享内存上发生的变更的可见性，对于文件映射来讲，这种差别还确定了内核是否会将映射内容上发生的变更传递到底层文件上。当一个进程使用MAP_PAIVATE映射一个文件之后，在映射内容上发生的变更对其他进程是不可见的，并且也不会反应到映射文件上。MAP_SHARED文件映射的做法则相反——在映射上发生的变更对其他进程可见并且会反应到映射文件上。

在访问一个映射的内容时可能会遇到两个信号。如果在访问映射时违反了映射之上的保护规则（或访问一个当前未被映射的地址），那么就会产生一个SIGSEGV信号。对于基于文件的映射来讲，如果访问的映射部分在文件中没有相关区域与之对应（即映射大于底层文件，但仍算映射了，而不是未映射），那么就会产生一个SIGBUS信号。

交换空间过度利用允许系统给进程分配比实际可用的RAM与交换空间之和更多的内存。过度利用之所以可能是因为所有进程都不会全部用完为其分配的内存。使用MAP_NORESERVE标记可以控制每个mmap()调用的过度利用情况，而是用/proc文件则可以控制整个系统的过度利用情况。

mremap()系统调用允许调整一个既有映射的大小。remap_file_pages()系统调用允许创建非线性文件映射

对于mmap()映射文件和read()文件来说，read()在读次数少的情况下比mmap()快。因为虽然mmap()不需要read()的那一次内存拷贝，但是硬件的发展，使得内存拷贝消耗时间极大降低了。而且mmap()的开销在于缺页中断，处理任务比较多。并且，mmap之后，再有读操作不会经过系统调用，所以在LRU比较最近使用的页时并不占优势，容易被换出内存。所以，普通读的情况下（排除反复读），read()通常比mmap()来得更快。

**malloc**<br>
用户程序申请内存空间时，如果库函数本身的内存池不能满足分配，会调用brk系统调用向系统申请扩大堆空间。但此时扩大的只是线性空间，直到真正使用到那块线性空间时，系统才会通过data abort分配物理页面。所以，malloc返回不为NULL只能说明得到了线性空间的资源，真正物理内存分配失败时，进程还是会以资源不足为由，直接退出。

```cpp
int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)//触发vma区域文件回写，并等待回写
  file_update_time(vma->vm_file);
  |--> set_page_dirty(page); //-->__set_page_dirty_buffers(page);
    foreach buf_head_in_this_page set_buffer_dirty(bh);
    |--> __set_page_dirty(page, mapping, 1);
      radix_tree_tag_set(&mapping->page_tree,	page_index(page), PAGECACHE_TAG_DIRTY);
      __mark_inode_dirty(mapping->host, I_DIRTY_PAGES);
  wait_for_stable_page(page); //-->wait_on_page_writeback(page);
int generic_file_remap_pages(struct vm_area_struct *vma, addr, size, pgoff)//更新页表
  while (size>0)
    install_file_pte(vma->vm_mm, vma, addr, pgoff, vma->vm_page_prot);//-->set_pte_at(mm, addr, pte, pgoff_to_pte(pgoff));
    size -= PAGE_SIZE; addr += PAGE_SIZE; pgoff++;

//内核中，vma->vm_ops 的几个定义： 1. 文件映射主要对应线性映射　２．共享内存映射
const struct vm_operations_struct generic_file_vm_ops = {　
	.fault = filemap_fault,	.page_mkwrite	= filemap_page_mkwrite,	.remap_pages	= generic_file_remap_pages,};
//用于共享内存
const struct vm_operations_struct shm_vm_ops = {
	.open	= shm_open,	.close	= shm_close,　.fault	= shm_fault,};
const struct vm_operations_struct relay_file_mmap_ops = {
	.fault = relay_buf_fault,	.close = relay_file_mmap_close,};
const struct vm_operations_struct special_mapping_vmops = {
	.close = special_mapping_close,	.fault = special_mapping_fault,};
const struct vm_operations_struct mmap_mem_ops = {
	.access = generic_access_phys　};

struct vm_area_struct *　find_extend_vma(struct mm_struct* mm, unsigned long addr)
  struct vm_area_struct *vma = find_vma(mm,addr); if (!vma)　return NULL;
  start = vma->vm_start; if (vma->vm_start <= addr)　return vma;　//条件满足
  if (!(vma->vm_flags & VM_GROWSDOWN)) return NULL;//这个vma无法扩展
  |-->expand_stack(vma, addr); //-->expand_downwards(vma, address) 更改vma->vm_start和相关参数，实现扩展
    size = vma->vm_end - address; grow = (vma->vm_start - address) >> PAGE_SHIFT;
    anon_vma_interval_tree_pre_update_vma(vma);
    vma->vm_start = address; vma->vm_pgoff -= grow;
    anon_vma_interval_tree_post_update_vma(vma);　vma_gap_update(vma);　validate_mm(vma->vm_mm);
static struct vm_area_struct gate_vma = {
	.vm_start	= 0xffff0000,	.vm_end		= 0xffff0000 + PAGE_SIZE,	.vm_flags	= VM_READ | VM_EXEC | VM_MAYREAD | VM_MAYEXEC,};
int in_gate_area(struct mm_struct *mm, unsigned long addr)//addr 在指定的gate_vma范围内
	return (addr >= gate_vma.vm_start) && (addr < gate_vma.vm_end);

//CALL(sys_mmap2)-->sys_mmap_pgoff()-->SYSCALL_DEFINE6(mmap_pgoff, addr, len, prot, flags, fd, pgoff)
unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr, unsigned long len, ..., unsigned long pgoff)
  |--> ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff, &populate);
    addr = get_unmapped_area(file, addr, len, pgoff, flags);
    |--> return addr = mmap_region(file, addr, len, vm_flags, pgoff);
      vma = vma_merge(mm, prev, addr, addr + len, vm_flags, NULL, file, pgoff, NULL, NULL);
      if (vma) return addr; //如果可以和现有的vma合并，就不需要重新创建
      vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
      vma->vm_mm = mm; vma->vm_start = addr; vma->vm_end = addr + len; vma->vm_pgoff = pgoff; vma->vm_file = get_file(file);
      |--> file->f_op->mmap(file, vma); //-->generic_file_mmap()
        struct address_space *mapping = file->f_mapping;
        file_accessed(file); vma->vm_ops = &generic_file_vm_ops;
      vma_link(mm, vma, prev, rb_link, rb_parent); vm_stat_account(mm, vm_flags, file, len >> PAGE_SHIFT);
  |--> mm_populate(ret, populate); //-->__mm_populate()//如果是vm_locked属性，直接polute，一般应该是以一个个vma范围为单位吧
    for (nstart = start; nstart < end; nstart = nend)
      |--> __mlock_vma_pages_range(vma, nstart, nend, &locked);//-->__get_user_pages()
        vma = find_extend_vma(mm, start);
        if (!vma && in_gate_area(mm, start))//这是很特殊的处理了，可以忽略
          pte = pte_offset_map(pmd, pg);//获取pte信息
          vma = get_gate_vma(mm);　page = vm_normal_page(vma, start, *pte); pages[i] = page;
        else
          |--> page = follow_page_mask(vma, start,	foll_flags, &page_mask);
            ptep = pte_offset_map_lock(mm, pmd, address, &ptl); pte = *ptep;//获取pte信息
            page = vm_normal_page(vma, address, pte);//从pte获取page
          handle_mm_fault(mm, vma, start,	fault_flags);

unsigned long vm_brk(unsigned long addr, unsigned long len)
  struct mm_struct *mm = current->mm;
  do_brk(addr, len); mm_populate(addr, len);　//如果是vm_locked属性，直接polute

//SYSCALL_DEFINE1(brk, unsigned long, brk) //sys_brk
  newbrk = PAGE_ALIGN(brk); oldbrk = PAGE_ALIGN(mm->brk);
  if (brk <= mm->brk)
    do_munmap(mm, newbrk, oldbrk-newbrk); return;
  else
    |--> do_brk(oldbrk, newbrk-oldbrk);
      vma = vma_merge(mm, prev, addr, addr + len, flags, NULL, NULL, pgoff, NULL, NULL); //try merge
      vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
      vma->vm_mm = mm; vma->vm_start = addr; vma->vm_end = addr + len; vma->vm_pgoff = pgoff;
      vma_link(mm, vma, prev, rb_link, rb_parent); mm->total_vm += len >> PAGE_SHIFT;
    mm_populate(oldbrk, newbrk - oldbrk);　//如果是vm_locked属性，直接polute

SYSCALL_DEFINE5(remap_file_pages, start, size, prot, pgoff, flags) //fremap.c
  struct mm_struct *mm = current->mm; vma = find_vma(mm, start); file = get_file(vma->vm_file); mapping = file->f_mapping;
  if (mapping_cap_account_dirty(mapping))
    addr = mmap_region(file, start, size, vma->vm_flags, pgoff);
  else
    vma->vm_ops->remap_pages(vma, start, size, pgoff); //-->generic_file_remap_pages()
    mm_populate(start, size);　//如果是vm_locked属性，直接polute

SYSCALL_DEFINE5(mremap, addr, old_len, new_len, flags, new_addr) //mremap.c

SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)
  profile_munmap(addr);
  |--> vm_munmap(addr, len); //-->do_munmap(mm, start, len);
    vma = find_vma(mm, start);//返回第一个保证vma->vm_end > addr的vma
    |--> unmap_region(mm, vma, prev, start, end);
      lru_add_drain(); tlb_gather_mmu(&tlb, mm, 0); update_hiwater_rss(mm);
      unmap_vmas(&tlb, vma, start, end);
      free_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS, next ? next->vm_start : USER_PGTABLES_CEILING);
      tlb_finish_mmu(&tlb, start, end); remove_vma_list(mm, vma);
    remove_vma_list(mm, vma);

struct fsr_info {
	int	(*fn)(unsigned long addr, unsigned int fsr, struct pt_regs *regs);
	int	sig;	int	code;	const char *name; };
__dabt_svc()-->dabt_helper()-->bl	CPU_DABORT_HANDLER-->v7_early_abort()-->do_DataAbort()
  //|--> v7_early_abort()
    mrc	p15, 0, r1, c5, c0, 0		@ get FSR //mmu失效状态寄存器
  	mrc	p15, 0, r0, c6, c0, 0		@ get FAR //mmu失效地址寄存器
    |--> do_DataAbort(ro, r1);//void do_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
      const struct fsr_info *inf = fsr_info + fsr_fs(fsr);
      |--> inf->fn(addr, fsr & ~FSR_LNX_PF, regs); //do_translation_fault()
        if (addr < TASK_SIZE) //如果是用户空间fault ,#define TASK_SIZE	(0xB0000000UL)
        	|--> return do_page_fault(addr, fsr, regs);
            //|--> handle_mm_fault(mm, find_vma(mm, addr), addr & PAGE_MASK, flags);
            //|--> handle_pte_fault(mm, vma, address, pte_offset_map(pgd_offset(mm, address), address), pmd, flags);
              __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);//called by do_nonlinear_fault() or do_linear_fault()
              do_anonymous_page(mm, vma, address, pte, pmd, flags); //匿名page

              //do_swap_page()不需要关注，和交换机制相关的．
        pgd = cpu_get_pgd() + index; //直接读取mmu寄存器得到当前进程的pmd
        pgd_k = init_mm.pgd + index; //由内核init_mm得到内核pmd_k
        //如果是内核空间fault,直接拷贝内核页表
        pmd = pmd_offset(pgd, addr); pmd_k = pmd_offset(pgd_k, addr); copy_pmd(pmd, pmd_k);//每个进程都有自己的16k一级页表
        //如果是内核空间地址，会判断该地址对应的二级页表指针是否在init_mm中。如果在init_mm里面，www.linuxidc.com那么复制该二级页表指针到当前进程的一级页表；否则，调用do_bad_area处理（可能会调用到fixup）
int __init exceptions_init(void) //绑定了data_abort处理函数
  hook_fault_code(4, do_translation_fault, SIGSEGV, SEGV_MAPERR, "I-cache maintenance fault");
  hook_fault_code(3, do_bad, SIGSEGV, SEGV_MAPERR, "section access flag fault");
	hook_fault_code(6, do_bad, SIGSEGV, SEGV_MAPERR, "section access flag fault");
arch_initcall(exceptions_init);

//下面应该是在应对vmalloc() area的场景
  index = pgd_index(addr); pgd = cpu_get_pgd() + index; pgd_k = init_mm.pgd + index;
  pud = pud_offset(pgd, addr); pud_k = pud_offset(pgd_k, addr); pmd = pmd_offset(pud, addr); pmd_k = pmd_offset(pud_k, addr);
  copy_pmd(pmd, pmd_k);//#define copy_pmd(pmdpd,pmdps) do { pmdpd[0] = pmdps[0]; pmdpd[1] = pmdps[1];	flush_pmd_entry(pmdpd);	} while (0)
```

### 5.8 文件回写
文件缓存回写的时机<br>
- (1)周期性回写，周期为dirty_writeback_interval，默认5s；
- (2)块设备首次出现脏数据；
- (3)脏页达到限额，包括dirty_bytes、dirty_background_bytes、dirty_ratio、dirty_background_ratio；
- (4)剩余内存过少，唤醒所有回写线程；
- (5)syscall sync，同步所有超级块
- (6)syscall syncfs，同步某个超级块对应的文件系统
- (7)syscall fsync/fdatasync，同步某个文件
- (8)laptop模式，完成blk io request之后，启动laptop_mode_wb_timer；
- (9)arm_dma_alloc() 导致的强制回写部分page.这个需要继续跟踪．

通过`bdi_queue_work()`进入回写队列统一调度，或者通过`fsync`在用户进程中完成．

```cpp
LIST_HEAD(bdi_list); //所有的bdi通过 struct list_head bdi_list 成员链接在一起

//使用反向引用树，解析bdi_init()函数，
int bdi_init(struct backing_dev_info *bdi)
  //bdi_setup_and_register
  blk_alloc_queue_node()
    //blk_alloc_queue(gfp_t gfp_mask)
    blk_init_queue_node(rfn, lock, NUMA_NO_NODE);
      blk_init_queue(mtd_blktrans_request, &new->queue_lock);//对应了nor/nand等mtd设备
        add_mtd_blktrans_dev(struct mtd_blktrans_dev *new)
      blk_init_queue(mmc_request_fn, lock);//对应了sd卡等设备
        mmc_init_queue(struct mmc_queue *mq, struct mmc_card *card, spinlock_t *lock, const char *subname)
          mmc_blk_alloc_req()
            mmc_blk_alloc()
              mmc_blk_probe()
  bdi_init(&default_backing_dev_info);// default
  bdi_init(&noop_backing_dev_info);// noop
    default_bdi_init(void)
  bdi_init(&directly_mappable_cdev_bdi);　//字符设备
    chrdev_init(void)
  bdi_init(bdi);
    mtd_bdi_init(&mtd_bdi_unmappable, "mtd-unmap");
    mtd_bdi_init(&mtd_bdi_ro_mappable, "mtd-romap");
    mtd_bdi_init(&mtd_bdi_rw_mappable, "mtd-rwmap");
      init_mtd(void) //不了解什么情况
  bdi_init(&shmem_backing_dev_info);
    shmem_init(void) //shared memory

int bdi_init(struct backing_dev_info *bdi)
  |--> bdi_wb_init(&bdi->wb, bdi);
    INIT_DELAYED_WORK(&wb->dwork, bdi_writeback_workfn);

bdi_queue_work() //反向引用树，添加新的work到bdi的work_list
  sync_inodes_sb()  //.sync_mode = WB_SYNC_ALL,.sb = sb,.reason = WB_REASON_SYNC,
      //sync()系列接口调用
  writeback_inodes_sb_nr() //.sb = sb,.sync_mode = WB_SYNC_NONE, .tagged_writepages = 1,指定sb写入若干page，不等待io完成
    //sync()系列接口调用
    writeback_inodes_sb //写入get_nr_dirty_pages()个page，好像是所有的page
      sync_filesystem-->__sync_filesystem //reason: WB_REASON_SYNC
    try_to_writeback_inodes_sb //ext4调用
  __bdi_start_writeback() //sync_mode: WB_SYNC_NONE
    wakeup_flusher_threads
      free_more_memory //要求回写1024个page！！效率？range_cyclic：false
        alloc_page_buffers
        __getblk_slow-->__getblk() //短期大量写入，导致回写释放的内存无法满足需要，强制flush.
      SYSCALL_DEFINE0(sync)
      do_try_to_free_pages
        __alloc_pages_slowpath()
      //不需要关注laptop <--- laptop_mode_timer_fn
```

```cpp
struct bdi_writeback {//某个bdi的控制回写的数据结构，包含3个io链表，包含回写cb的work，并指向归属的bdi
  struct backing_dev_info *bdi; /* our parent bdi */
  unsigned int nr;
  unsigned long last_old_flush; /* last old data flush */
  struct delayed_work dwork; /* work item used for writeback *///实际控制回写timer和cb
  struct list_head b_dirty; //inode通过__mark_inode_dirty()进入此队列
  struct list_head b_io; //从b_dirty中取合适的(比如最老，指定文件，指定sb)放入此队列，回写过程只从这里读取
  struct list_head b_more_io; //还想写，但是配额已满，只好扔到这个队列，应该是要合并到下一次回写的
  spinlock_t list_lock; /* protects the b_* lists */
};
struct wb_writeback_work {//怎么没看到具体的文件信息？
  long nr_pages;
  struct super_block *sb;
  unsigned long *older_than_this;//有点疑问，为何指向stack中局部变量
  enum writeback_sync_modes sync_mode;
  unsigned int tagged_writepages:1, for_kupdate:1, range_cyclic:1, for_background:1;//ratio下限写
  enum wb_reason reason; /* why was writeback initiated */

  struct list_head list; /* pending work list 所有的work通过list建立链表加入到bdi的work_list */
  struct completion *done; //set if the caller waits
};
struct backing_dev_info {
  struct list_head bdi_list;//系统中所有bdi通过此node链接到global_bdi_list指针的链表
  unsigned long ra_pages; /* max readahead in PAGE_CACHE_SIZE units */
  unsigned long state; /* Always use atomic bitops on this */
  unsigned int capabilities; /* Device capabilities */
  congested_fn *congested_fn; /* Function pointer if device is md/dm */
  void *congested_data; /* Pointer to aux data for congested func */
  char *name;
  struct percpu_counter bdi_stat[NR_BDI_STAT_ITEMS];
  unsigned long bw_time_stamp, dirtied_stamp, written_stamp, write_bandwidth, avg_write_bandwidth;
  unsigned long dirty_ratelimit, balanced_dirty_ratelimit;
  struct fprop_local_percpu completions;
  int dirty_exceeded;
  unsigned int min_ratio, max_ratio, max_prop_frac;
  struct bdi_writeback wb; /* default writeback info for this bdi */控制回写的结构，定义io链表和回写操作
  spinlock_t wb_lock; /* protects work_list */
  struct list_head work_list; //wb_writeback_work通过list节点链接到链表。
  struct device *dev;
  struct timer_list laptop_mode_wb_timer;
};
//每个 struct backing_dev_info 包含一个 struct bdi_writeback wb, 后者又包含一个 struct delayed_work dwork，所以实现上使用mod_delay_work()来调整work执行的时间．但是一个work可以对应很多个inode，脏文件挂到 struct list_head b_dirty 队列上．

struct workqueue_struct *bdi_wq; //所有的回写work都queue到这个工作队列中
enum wb_reason {	WB_REASON_BACKGROUND, ... };
void wakeup_flusher_threads(long nr_pages, enum wb_reason reason)
  list_for_each_entry_rcu(bdi, &bdi_list, bdi_list)
    if (bdi_has_dirty_io(bdi))
      |--> __bdi_start_writeback(bdi, nr_pages, false, reason);
        struct wb_writeback_work *work = kzalloc(sizeof(*work), GFP_ATOMIC);
        work->sync_mode = WB_SYNC_NONE; work->nr_pages = nr_pages; work->range_cyclic = range_cyclic;  work->reason = reason;
        |--> bdi_queue_work(bdi, work);
          list_add_tail(&work->list, &bdi->work_list);
          mod_delayed_work(bdi_wq, &bdi->wb.dwork, 0);//之后立刻启动work

long wb_writeback(struct bdi_writeback *wb, struct wb_writeback_work *work)
  如果worklist非空，暂时不处理for_background和for_kupdate的work
  跳过不满足for_background阈值条件的for_background work
  if (work->for_kupdate) 强制设置dirty_expire_interval使能超时场景//怎么没有计算历史时间
    if (list_empty(&wb->b_io)) //b_io完毕，现在处理老龄数据和之前被延迟到b_more_io的inode(之前没有sync要求)
    list_splice_init(&wb->b_more_io, &wb->b_io); //合并b_more_io到b_io
    queue_io(wb, work);//如果上个b_io已清空，从b_dirty移动超时回写到b_io链表
      moved = move_expired_inodes(&wb->b_dirty, &wb->b_io, work);
  |--> if (work->sb) progress = __writeback_inodes_wb(wb, work);//一般情况下，b_io中内容是对应work的，之前刚刚添加。
    wrote += writeback_sb_inodes(sb, wb, work);//文件系统层面的sync()，sb是有效的
      foreach inode in b_io链表//如果inode锁定，可能想和其他inode一起回写提高效率，扔进b_more_io.
        if ((inode->i_state & I_SYNC) && wbc.sync_mode != WB_SYNC_ALL) requeue_io(inode, wb);
        if ((inode->i_state & I_SYNC) && wbc.sync_mode == WB_SYNC_ALL)
          inode_sleep_on_writeback(inode);continue;//解锁后重新检查回写inode
        inode->i_state |= I_SYNC;
        wbc.nr_to_write = write_chunk = writeback_chunk_size(wb->bdi, work);
        wbc.pages_skipped = 0;
        |--> __writeback_single_inode(inode, &wbc);//回写inode文件所有的脏页。
          ret = do_writepages(mapping, wbc);
          if (wbc->sync_mode == WB_SYNC_ALL) filemap_fdatawait(mapping);//等待完成，下面判断是否写inode
          if (inode->i_state&(I_DIRTY|I_DIRTY_SYNC|I_DIRTY_DATASYNC)) write_inode(inode, wbc);
            ret = inode->i_sb->s_op->write_inode(inode, wbc); //fat_write_inode
        if (time_is_before_jiffies(start_time + HZ / 10UL))    break; //最多写100ms就需要退出
        if (work->nr_pages <= 0) break;
   else progress = __writeback_inodes_wb(wb, work);//文件层次的操作

void bdi_writeback_workfn(struct work_struct *work)         //文件系统回写
  while (!list_empty(&bdi->work_list))
    |--> pages_written = wb_do_writeback(wb, 0);//多层循环，内层循环写入某类work,从而实现高优先级的work先写入
    //work_list上只有强制行的work,没有ratio和时间回写
      while ((work = get_next_work_item(bdi)) != NULL)
        wrote += wb_writeback(wb, work); complete(work->done);
      |--> wrote += wb_check_old_data_flush(wb);//
        struct wb_writeback_work work = {.nr_pages = nr_pages,.sync_mode = WB_SYNC_NONE,.for_kupdate = 1,.range_cyclic = 1,
              .reason = WB_REASON_PERIODIC, };//检查如果wb->last_old_flush为5s前的jifflies，启动回写
        return wb_writeback(wb, &work);
      |--> wrote += wb_check_background_flush(wb);
        if (over_bground_thresh(wb->bdi))
          struct wb_writeback_work work = {.nr_pages = LONG_MAX,.sync_mode = WB_SYNC_NONE,.for_background = 1,.range_cyclic = 1,
              .reason = WB_REASON_BACKGROUND,};//超过回写下限
          return wb_writeback(wb, &work);
      clear_bit(BDI_writeback_running, &wb->bdi->state);
  if (!list_empty(&bdi->work_list) ||(wb_has_dirty_io(wb) && dirty_writeback_interval))//可能再次定时启动
    queue_delayed_work(bdi_wq, &wb->dwork,msecs_to_jiffies(dirty_writeback_interval * 10));

```

## 6
### 6.1
