# spi_bus


## 0 修订记录
| 修订说明 | 日期 | 作者 | 额外说明 |
| --- |
| 初版 | 2018/04/10 | 员清观 |  |

## 5 编解码
### 5.1 libcodecs
```cpp
void *codec_open(codec_info_t *fmt)
	dev = (codec_t)malloc(sizeof(*dev));
	memset(dev, 0, sizeof(*dev)); memcpy(&dev->fmt, fmt, sizeof(codec_info_t));
	dev->codec_mode = codec_check_fmt(fmt);
```

### 5.2 ffmpeg
ffmpeg主目录下主要有libavcodec、libavformat和libavutil等子目录。其中
- libavcodec用于存放各个encode/decode模块，CODEC其实是Coder/Decoder的缩写，也就是编码解码器；用于各种类型声音/图像编解码
- libavformat用于存放muxer/demuxer模块，对音频视频格式的解析;用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能；
其中库 libavcodec，libavformat用于对媒体文件进行处理，如格式的转换；
- libavutil集项工具，包含一些公共的工具函数；用于存放内存操作等辅助性模块，是一个通用的小型函数库，该库中实现了CRC校验码的产生，128位整数数学，最大公约数，整数开方，整数取对数，内存分配，大端小端格式的转换等功能
- libavdevice：对输出输入设备的支持；
- libpostproc：用于后期效果处理；
- libswscale：用于视频场景比例缩放、色彩映射转换；
- ffmpeg：该项目提供的一个工具，可用于格式转换、解码或电视卡即时编码等；
- fsever：一个 HTTP 多媒体即时广播串流服务器；
- ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示
    ffmpeg软件包经编译过后将生成三个可执行文件，ffmpeg，ffserver，ffplay。其中ffmpeg用于对媒体文件进行处理，ffserver是一个http的流媒体服务器，ffplay是一个基于SDL的简单播放器。

muxer/demuxer和encoder/decoder在FFmpeg中的实现代码里，有许多相同的地方，而二者最大的差别是muxer和demuxer分别是不同的结构AVOutputFormat与AVInputFormat，而encoder和decoder都是用的AVCodec结构。二者都是在main()开始的`av_register_all()`函数内初始化的。二者都是以链表的形式保存在全局变量中的。muxer/demuxer是分别保存在全局变量AVOutputFormat *first_oformat与AVInputFormat *first_iformat中的。encoder/decoder都是保存在全局变量AVCodec *first_avcodec中的。
```cpp
//AVFormatContext是FFMpeg格式转换过程中实现输入和输出功能、保存相关数据的主要结构。每一个输入和输出文件，都在如下定义的指针数组全局变量中有对应的实体。
static AVFormatContext *output_files[MAX_FILES];
static AVFormatContext *input_files[MAX_FILES];
//对于输入和输出，因为共用的是同一个结构体，所以需要分别对该结构中如下定义的iformat或oformat成员赋值。对一个AVFormatContext来说，这二个成员不能同时有值，即一个AVFormatContext不能同时含有demuxer和muxer。在main( )函数开头的parse_options( )函数中找到了匹配的muxer和demuxer之后，根据传入的argv参数，初始化每个输入和输出的AVFormatContext结构，并保存在相应的output_files和input_files指针数组中。在av_encode( )函数中，output_files和input_files是作为函数参数传入后，在其他地方就没有用到了。
struct AVInputFormat *iformat;
struct AVOutputFormat *oformat;
//     AVCodecContext保存AVCodec指针和与codec相关数据，如video的width、height，audio的sample rate等。AVCodecContext中的codec_type，codec_id二个变量对于encoder/decoder的匹配来说，最为重要。codec_type保存的是CODEC_TYPE_VIDEO，CODEC_TYPE_AUDIO等媒体类型，codec_id保存的是CODEC_ID_FLV1，CODEC_ID_VP6F等编码方式。
enum CodecType codec_type;     /* see CODEC_TYPE_xxx */
enum CodecID codec_id;         /* see CODEC_ID_xxx */
//AVStream结构保存与数据流相关的编解码器，数据段等信息。比较重要的有如下二个成员，其中codec指针保存的就是encoder或decoder结构 priv_data指针保存的是和具体编解码流相关的数据
AVCodecContext *codec; /**< codec context */
void *priv_data;
```

SDL（Simple DirectMedia Layer）: 是一套开放源代码的跨平台多媒体开发库，使用C语言写成。SDL提供了数种控制图像、声音、输出入的函数，让开发者只要用相同或是相似的代码就可以开发出跨多个平台（Linux、Windows、Mac OS X等）的应用软件。目前SDL多用于开发游戏、模拟器、媒体播放器等多媒体应用领域。

**ffmpeg安装**<br>
配置C++编译开发环境
	sudo apt-get install build-essential
安装自动生成makefile的相关工具
	sudo apt-get install automake1.11
	sudo apt-get install libsdl2-dev
下载ffmpeg源码
	官方下载网址 http://ffmpeg.org/download.html
	git clone git://source.ffmpeg.org/ffmpeg.git ffmpeg
安装编译ffmpeg时所需要的相关工具
	sudo apt-get install yasm
生成makefile及编译
	./configure
	make
	make install
默认是安装在/user/local下，其中：头文件放在/user/local/include目录下，译好的libs放在/user/local/lib目录下，其中，在该目录下还有一个pkgconfig目录，里面存放着每个lib的配置文件，编译好的可执行文件(ffmpeg、ffprobe、ffserver)放在/user/local/bin目录下，文档在/user/local/share/man/man1目录下，同时在/user/local有一个指向此目录的链接，后面打算先学习快速使用ffmpeg，知道ffmpeg总体架构，如何将里面有用的代码提取出来。其实就是熟练使用ffmpeg、ffprobe、ffserver，ffplay

ffmpeg -i source_video.avi input -acodec aac -ab 128kb -vcodec mpeg4 -b 1200kb -mbd 2 -flags +4mv+trell -aic 2 -cmp 2 -subcmp 2 -s 320x180 -title X final_video.mp4

**ffplay命令**<br>
ffplay [选项] ['输入文件']
主要选项
	'-x width'        强制以 "width" 宽度显示
	'-y height'       强制以 "height" 高度显示
	'-an'             禁止音频
	'-vn'             禁止视频
	'-ss pos'         跳转到指定的位置(秒)
	'-t duration'     播放 "duration" 秒音/视频
	'-bytes'          按字节跳转
	'-nodisp'         禁止图像显示(只输出音频)
	'-f fmt'          强制使用 "fmt" 格式
	'-window_title title'  设置窗口标题(默认为输入文件名)
	'-loop number'    循环播放 "number" 次(0将一直循环)
	'-showmode mode'  设置显示模式
	可选的 mode ：
	'0, video'    显示视频
	'1, waves'    显示音频波形
	'2, rdft'     显示音频频带
	默认值为 'video'，你可以在播放进行时，按 "w" 键在这几种模式间切换
	'-i input_file'   指定输入文件

**ffprobe命令**<br>
- `-show_streams` 通过ffprobe -show_frames 命令可以查看视频文件中的帧信息，输出的帧信息使用STREAM标签扩起来：
- `-show_frames`  通过ffprobe -show_frames 命令可以查看视频文件中的帧信息，输出的帧信息使用FRAME标签扩起来：
- `-show_format`  通过ffprobe -show_format 命令可以查看多媒体饿封装格式，其使用FORMAT标签扩起来显示：
- `-show_packets` 查看多媒体数据包信息使用PACKET标签扩起来了
- `-show_data` 查看封装信息
- 其他搜索网上

**ffmpeg常用命令**
	https://blog.csdn.net/pyl574069214/article/details/

```cpp
ffmpeg -formats //查看所有支持的容器格式
ffmpeg -codecs //查看所有编解码器
ffmpeg -filters //查看所有可用的filter
ffmpeg -pix_fmts //查看所有支持的图片格式
ffmpeg -sample_fmts //查看所有支持的像素格式
ffprobe -i money.mp4 //查看媒体信息
ffmpeg -f v4l2 -i /dev/video0 output.mp4 //ffmpeg //获取摄像头/dev/video0并输出.mp4文件
ffplay -f rawvideo -video_size 1920x1080 a.yuv //ffplay播放yuv文件命令
ffprobe -v quiet -print_format json -show_format -show_streams  video.mp4  //获取音视频信息，以json格式返回
或ffprobe -show_format -show_streams video.mp4 //获取音视频信息
ffmpeg -i myvideo.mp4 //获取视频的信息
ffmpeg -i myvideo.mp4 image%d.jpg //将视频分解成图片序列
ffmpeg -i myvideo.mp4 -vn-ar 44100 -ac 2 -ab 192 -f mp3 sound.mp3 //从视频抽出声音，并存为Mp3
   //说明： * 源视频：myvideo.mp4  * 音频位率：192kb/s  * 输出格式：mp3  * 生成的声音：sound.mp3
ffmpeg -i sample.mp4 -vcodec copy -an output.mp4;//分离视频流
ffmpeg -i sample.mp4 -acodec copy -vn output.aac;//分离音频流
ffmpeg -i sample.mp4  -ab 32 -ar 22050 -qscale 10 -s 682*310 -r 15 output.flv //将mp4转为flv
ffmpeg -re -i test.flv -vcodec copy -acodec copy -f flv rtmp://localhost:1935/live/mystream.stream //将本地文件推流
ffmpeg -i test.ts -vcodec copy -f m4v test.264 //TS流解复用
1.ffmpeg推流：ffmpeg  -re  -i  <inputfile>  -vcodec  copy  -f  <format>  rtmp://<ipaddr>
note：-re 按照帧率推送，否则ffmpeg会以最高的速率发送数据；-vcodec copy 否则ffmpeg会重新编码输入的码流
2.ffmpeg拉流：ffplay rtmp://<ipaddr>  -fflags  nobuffer
note：-fflags  nobuffer 无缓存
3.ffmpeg使用gpu加速解码【硬解码】：ffmpeg -hwaccel <hard dirver> -c <decoder> -i <inputfile> -f null --benchmark
//note:-hwaccel cuvid选择硬件加速 -f null 强制不输出解码帧【-f null 可以换成<outfile>】 - -benchmark 测试模式
//example：ffmpeg -hwaccel cuvid -c h264_cuvid -i test.h264 -f null - -benchmark
4.ffmpe使用gpu加速编码【硬编码】
5.ffmpeg剪辑一帧数据（图片）：ffmpeg  -c  <decoder>  -i  <inputfile>  -frames  1  <outfile>
//example: ffmpeg -c h264 -i jetflow-1080p-f2949-0.h264 -frames 1 test.yuv
6.ffplay播放yuv视频 ：ffpaly  -f  rawvideo  -video_size  <width*height>  -pix_fmt <fmt> <inputfile>
//目前就设计到这两种播放格式，貌似默认播放yv21和i420格式的，nv12需要加格式说明
7.ffprobe查看视频信息：ffprobe  <inputfile>
8.ffmpeg剪辑1帧原始视频（图片）：ffmpeg -f rawvideo -video_size <width*height> -i <inputfile> -frames 1 <outfile>
9.ffmpeg裁剪视频:ffmpeg  -i  <inputfile>  -filter_complex crop=dstwidth:dstheight:x:y  -y  <outfile>
10.ffmpeg裁剪原始视频:ffmpeg  -f  rawvideo  -video_size  <width*height>  -i  <inputfile>  -filter_complex crop=dstwidth:dstheight:x:y  -y  <outfile>
```

**mp3编解码范例：**<br>
	https://github.com/rbrito/deprecated-lame-mirror
	https://github.com/lieff/minimp3

## 5. 文件格式和容器
### 5.1 wav文件格式解析
WAV文件是在PC机平台上很常见的、最经典的多媒体音频文件。编码包括了两方面内容,一是按一定格式存储数据,二是采用一定的算法压缩数据。WAV格式对音频流的编码没有硬性规定,支持非压缩的PCM(Puls Code Modulation)脉冲编码调制格式,还支持压缩型的微软自适应分脉冲编码调制Microsoft ADPCM(Adaptive Differential Puls Code Modulation)、国际电报联盟(International Telegraph Union)制定的语音压缩标准ITUG.711 a-law、ITU G.711-law、IMA ADPCM、ITU G.723 ADPCM (Yamaha)、GSM 6.10、ITU G.721 ADPCM编码和其它压缩算法。MP3编码同样也可以运用在WAV中,只要安装相应的Decode,就可以播放WAV中的MP3音乐。但一般来讲，我们只是用wav文件来保存Pcm数据流

```cpp
 private byte[] getWaveFileHeader(int sampleRate, int channels, int bitsPerSample, int bytePerSecond, long fileLenIncludeHeader)
 {
    byte[] wavHeader = new byte[44];
    long totalDataLen = fileLenIncludeHeader - 8;
    long audioDataLen = totalDataLen - 36;

    //ckid：4字节 RIFF 标志，大写
    wavHeader[0] = 'R'; wavHeader[1] = 'I'; wavHeader[2] = 'F'; wavHeader[3] = 'F';

    //cksize：4字节文件长度，这个长度不包括"RIFF"标志(4字节)和文件长度本身所占字节(4字节),即该长度等于整个文件长度 - 8
    wavHeader[4] = (byte)(totalDataLen & 0xff); wavHeader[5] = (byte)((totalDataLen >> 8) & 0xff); wavHeader[6] = (byte)((totalDataLen >> 16) & 0xff); wavHeader[7] = (byte)((totalDataLen >> 24) & 0xff);

    //fcc type：4字节 "WAVE" 类型块标识, 大写
    wavHeader[8] = 'W'; wavHeader[9] = 'A'; wavHeader[10] = 'V'; wavHeader[11] = 'E';

    //ckid：4字节 表示"fmt" chunk的开始,此块中包括文件内部格式信息，小写, 最后一个字符是空格
    wavHeader[12] = 'f'; wavHeader[13] = 'm'; wavHeader[14] = 't'; wavHeader[15] = ' ';

    //cksize：4字节，文件内部格式信息数据的大小，过滤字节（一般为00000010H）
    wavHeader[16] = 0x10; wavHeader[17] = 0; wavHeader[18] = 0; wavHeader[19] = 0;

    //FormatTag：2字节，音频数据的编码方式，1：表示是PCM 编码
    wavHeader[20] = 1; wavHeader[21] = 0;

    //Channels：2字节，声道数，单声道为1，双声道为2
    wavHeader[22] = (byte) channels; wavHeader[23] = 0;

    //SamplesPerSec：4字节，采样率，如44100
    wavHeader[24] = (byte)(sampleRate & 0xff); wavHeader[25] = (byte)((sampleRate >> 8) & 0xff); wavHeader[26] = (byte)((sampleRate >> 16) & 0xff); wavHeader[27] = (byte)((sampleRate >> 24) & 0xff);

    //BytesPerSec：4字节，音频数据传送速率, 单位是字节。其值为采样率×每次采样大小。播放软件利用此值可以估计缓冲区的大小；
    wavHeader[28] = (byte)(bytePerSecond & 0xff); wavHeader[29] = (byte)((bytePerSecond >> 8) & 0xff); wavHeader[30] = (byte)((bytePerSecond >> 16) & 0xff); wavHeader[31] = (byte)((bytePerSecond >> 24) & 0xff);

    //BlockAlign：2字节，每次采样的大小 = 采样精度*声道数/8(单位是字节); 这也是字节对齐的最小单位, 譬如 16bit 立体声在这里的值是 4 字节
    wavHeader[32] = (byte)(bitsPerSample * channels / 8); wavHeader[33] = 0;

    //BitsPerSample：2字节，每个声道的采样精度; 譬如 16bit 在这里的值就是16。如果有多个声道，则每个声道的采样精度大小都一样的；
    wavHeader[34] = (byte) bitsPerSample; wavHeader[35] = 0;

    //ckid：4字节，数据标志符（data），表示 "data" chunk的开始。此块中包含音频数据，小写；
    wavHeader[36] = 'd'; wavHeader[37] = 'a'; wavHeader[38] = 't'; wavHeader[39] = 'a';

    //cksize：音频数据的长度，4字节，audioDataLen = totalDataLen - 36 = fileLenIncludeHeader - 44
    wavHeader[40] = (byte)(audioDataLen & 0xff); wavHeader[41] = (byte)((audioDataLen >> 8) & 0xff); wavHeader[42] = (byte)((audioDataLen >> 16) & 0xff); wavHeader[43] = (byte)((audioDataLen >> 24) & 0xff); return wavHeader;
}
```
