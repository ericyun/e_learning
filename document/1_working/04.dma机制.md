# dma子系统

## 0 修订记录
| 修订说明 | 日期 | 作者 | 额外说明 |
| --- |
| 初版 | 2018/04/10 | 员清观 |  |


---
## 1 pl330模块

**dma微指令编程：**<br>
	https://blog.csdn.net/mirkerson/article/details/8818200
	https://blog.csdn.net/mirkerson/article/details/8818198

```cpp

```

### 1.1 初始化和probe


```cpp

static struct dma_pl330_platdata imap_pl330_platdata = {
	.nr_valid_peri = 23,
	.peri_id = imap_pl330_peri,
	.mcbuf_sz = 512,
};

struct amba_device imap_dma_device = {
	.dev = {
		.init_name = "dma-pl330",
		.platform_data = &imap_pl330_platdata,
		.coherent_dma_mask = ~0,
		},
	.res = {
		.start = IMAP_GDMA_BASE,
		.end = IMAP_GDMA_BASE + IMAP_GDMA_SIZE - 1,
		.flags = IORESOURCE_MEM,
		},
	.irq = {GIC_DMA0_ID, GIC_DMA1_ID,
		GIC_DMA2_ID, GIC_DMA3_ID,
		GIC_DMA4_ID, GIC_DMA5_ID,
		GIC_DMA6_ID, GIC_DMA7_ID,
		GIC_DMA8_ID, GIC_DMA9_ID,
		GIC_DMA10_ID, GIC_DMA11_ID,
		GIC_DMABT_ID, NO_IRQ},//GDMA支持最多6个通道，提供了12个中断，每个通道两个．应该是根据驱动申请的通道号，自动关联．
	.periphid = 0x00041330,
};

struct dma_pl330_chan {
	struct tasklet_struct task;/* Schedule desc completion */
	struct dma_chan chan;/* DMA-Engine Channel */
	struct list_head work_list;/* List of to be xfered descriptors */
	struct dma_pl330_dmac *dmac;
  /* Pointer to the DMAC that manages this channel, NULL if the channel is available to be acquired. As the parent, this DMAC also provides descriptors to the channel. */

	spinlock_t lock;/* To protect channel manipulation */
	enum pl330_byteswap byteswap;

	/* Token of a hardware channel thread of PL330 DMAC NULL if the channel is available to be acquired. */
	void *pl330_chid;

	/* For D-to-M and M-to-D channels */
	int burst_sz; /* the peripheral fifo width */
	int burst_len; /* the number of burst */
	dma_addr_t fifo_addr;
	enum desc_type desc_type;/* for pl330 descritor operation type */

	bool pause;
	int len;
};

struct pl330_thread {/* A DMAC Thread */
	u8 id;
	int ev;
	bool free;/* If the channel is not yet acquired by any client */
	struct pl330_dmac *dmac;/* Parent DMAC */
	struct _pl330_req req[2];/* Only two at a time */
	unsigned lstenq;/* Index of the last enqueued request */
	int req_running;/* Index of the last submitted request or -1 if the DMA is stopped */
	enum _dma_mode mode;/* dma transfer mode */
};
int pl330_update(const struct pl330_info *pi)
  void __iomem *regs = pi->base;
	struct pl330_dmac *pl330 = pi->pl330_data;
  val = readl(regs + FSM) & 0x1;//读取管理通道的fsm寄存器，判断是否需要复位管理通道
  if (val)
		pl330->dmac_tbd.reset_mngr = true;
  else
		pl330->dmac_tbd.reset_mngr = false;
  val = readl(regs + FSC) & ((1 << pi->pcfg.num_chan) - 1);  //读取普通通道的fsc寄存器，判断哪些通道需要复位
  pl330->dmac_tbd.reset_chan |= val;    i = -1;
  while (val && (++i < pi->pcfg.num_chan))  //循环复位相应的通道(线程)
    if (val & (1 << i))   _stop(&pl330->channels[i]);
  val = readl(regs + ES); //读取es寄存器，检查发生的事件
  if (pi->pcfg.num_events < 32 && val & ~((1 << pi->pcfg.num_events) - 1))
		pl330->dmac_tbd.reset_dmac = true; return;
  for (ev = 0; ev < pi->pcfg.num_events; ev++)
    if (   ( val & (1 << ev )) || ( ( ev == pl022_sync.pl022_ev_tx ) &&( pl022_sync.pl022_pages>0 ) &&(pl022_sync.pl022_pages%2 == 1) )  ) //循环处理已经发生的事件
      u32 inten = readl(regs + INTEN);
      if (inten & (1 << ev))  writel(1 << ev, regs + INTCLR); //如果中断使能并且事件发生，清除中断．
      id = pl330->events[ev]; thrd = &pl330->channels[id]; //获取事件对应的通道id和dmac通道处理线程
      active = thrd->req_running;//获取当前正在处理的请求
      rqdon//初始化pl330 dmac的struct tasklet_struct软中断处理，该操作主要处理一些dmac出现的错误e = thrd->req[active].r; //获取已经处理的请求
      mark_free(thrd, active); //将该dmac处理线程的请求标记为free
      _start(thrd);//继续该dmac处理线程
      list_add_tail(&rqdone->rqd, &pl330->req_done);//将该请求加入到已经处理完成链表
  list_for_each_entry_safe(rqdone, tmp, &pl330->req_done, rqd)//处理pl330 dmac中的已经完成处理链表的每个成员
    list_del(&rqdone->rqd);     _callback(rqdone, PL330_ERR_NONE);//调用请求中的回调处理该完成请求。
  if (pl330->dmac_tbd.reset_dmac|| pl330->dmac_tbd.reset_mngr	|| pl330->dmac_tbd.reset_chan)
    tasklet_schedule(&pl330->tasks);

void pl330_tasklet(unsigned long data)
  struct dma_pl330_chan *pch = (struct dma_pl330_chan *)data;
  struct dma_pl330_desc *desc, *_dt; //另外本函数会释放已经done的desc
  list_for_each_entry_safe(desc, _dt, &pch->work_list, node)
    if (desc->status == DONE)
			dma_cookie_complete(&desc->txd);		list_move_tail(&desc->node, &list);
  |--> fill_queue(pch);//void fill_queue(struct dma_pl330_chan *pch)
    struct dma_pl330_desc *desc;
    list_for_each_entry(desc, &pch->work_list, node)
      |--> ret = pl330_submit_req(pch->pl330_chid,&desc->req); //int pl330_submit_req(void *ch_id, struct pl330_req *r)
        if (_queue_full(thrd))  return -EAGAIN;
				idx = IS_FREE(&thrd->req[0]) ? 0 : 1;	xs.ccr = ccr;	xs.r = r;
				ret = _setup_req(1, thrd, idx, &xs);//预演
				thrd->lstenq = idx;
        |--> thrd->req[idx].mc_len = _setup_req(0, thrd, idx, &xs);//int _setup_req(unsigned dry_run, struct pl330_thread *thrd,unsigned index, struct _xfer_spec *pxs)，dry_run是演练的意思
          struct _pl330_req *req = &thrd->req[index];　　
          u8 *buf = req->mc_cpu;　//好像是微指令开始地址，因此setup函数就是填写微指令给dmac
          int off = 0; off += _emit_FLUSHP(dry_run, &buf[off], pxs->r->peri);
          off += _emit_MOV(dry_run, &buf[off], CCR, pxs->ccr);
          x = pxs->r->x;
          while (x)//从上层看，应该只包含一个，fill_px()函数中next==NULL.
            pxs->x = x; off += _setup_xfer(dry_run, &buf[off], pxs); x = x->next;
          off += _emit_SEV(dry_run, &buf[off], thrd->ev); off += _emit_END(dry_run, &buf[off]); return off;
				thrd->req[idx].r = r;
        return 0;
  |--> pl330_chan_ctrl(pch->pl330_chid, PL330_OP_START);//int pl330_chan_ctrl(void *ch_id, enum pl330_chan_op op)
    struct pl330_thread *thrd = ch_id;
    pl330 = thrd->dmac; active = thrd->req_running;
    switch (op)
      case PL330_OP_FLUSH:
      case PL330_OP_ABORT:
        _stop(thrd); ...
      case PL330_OP_START:
        _start(thrd); ...
  free_desc_list(&list);//void free_desc_list(struct list_head *list)
    list_for_each_entry(desc, list, node)
      pch = desc->pchan;
      dma_async_tx_callback callback = desc->txd.callback;
      param = desc->txd.callback_param;//驱动中调用dmaengine_prep_slave_sg()函数获取desc,然后直接赋值这两项参数
      callback(param);

void *pl330_request_channel(const struct pl330_info *pi)
  struct pl330_dmac *pl330 = pi->pl330_data;
  int chans = pi->pcfg.num_chan;
  for (i = 0; i < chans; i++)
    thrd = &pl330->channels[i];
    if ((thrd->free) && (!_manager_ns(thrd) || _chan_ns(pi, i)))
      thrd->ev = _alloc_event(thrd);
      thrd->free = false;			thrd->lstenq = 1;
      thrd->req[0].r = NULL;	mark_free(thrd, 0);			thrd->req[1].r = NULL;			mark_free(thrd, 1);
int pl330_add(struct pl330_info *pi)
  struct pl330_dmac *pl330 = kzalloc(sizeof(*pl330), GFP_KERNEL);
  pl330->pinfo = pi;  pi->pl330_data = pl330; spin_lock_init(&pl330->lock); INIT_LIST_HEAD(&pl330->req_done);
  |--> ret = dmac_alloc_resources(pl330);//int dmac_alloc_resources(struct pl330_dmac *pl330)
    struct pl330_info *pi = pl330->pinfo; int chans = pi->pcfg.num_chan;
    pl330->mcode_cpu = dma_alloc_coherent(pi->dev, chans * pi->mcbufsz, &pl330->mcode_bus, GFP_KERNEL);
    |--> ret = dmac_alloc_threads(pl330);//int dmac_alloc_threads(struct pl330_dmac *pl330)
      struct pl330_info *pi = pl330->pinfo; int chans = pi->pcfg.num_chan;
      pl330->channels = kzalloc((1 + chans) * sizeof(*thrd), GFP_KERNEL);
      for (i = 0; i < chans; i++)
        thrd = &pl330->channels[i];thrd->id = i;thrd->dmac = pl330;_reset_thread(thrd);thrd->free = true;
      thrd = &pl330->channels[chans];thrd->id = chans;thrd->dmac = pl330;thrd->free = false;pl330->manager = thrd;//MANAGER is indexed at the end
  tasklet_init(&pl330->tasks, pl330_dotask, (unsigned long) pl330);//pl330_dotask()函数主要处理一些dmac出现的错误

int add_desc(struct dma_pl330_dmac *pdmac, gfp_t flg, int count)
  struct dma_pl330_desc *desc = kmalloc(count * sizeof(*desc), flg);
  for (i = 0; i < count; i++)
    |--> _init_desc(&desc[i]);//void _init_desc(struct dma_pl330_desc *desc)
      desc->req.xfer_cb = dma_pl330_rqcb();//void dma_pl330_rqcb(void *token, enum pl330_op_err err)
          <--  void pl330_dotask(unsigned long data)
            <--  int pl330_update(const struct pl330_info *pi)//好像是启动传输：_start(thrd);
              <--  irqreturn_t pl330_irq_handler(int irq, void *data)//应该是fifo中数据满足传输要求，产生中断给dma
          tasklet_schedule(&pch->task);
      |--> desc->txd.tx_submit = pl330_tx_submit();//dma_cookie_t pl330_tx_submit(struct dma_async_tx_descriptor *tx)
        while (!list_empty(&last->node))  list_move_tail(&desc->node, &pch->work_list);//驱动将请求desc加入worklist

    list_add_tail(&desc[i].node, &pdmac->desc_pool);

static int pl330_probe(struct amba_device *adev, const struct amba_id *id)
  set_secure_para(1);
  struct clk *clk = clk_get_sys("dma-pl330","dma-pl330"); ret = clk_prepare_enable(clk);
  struct dma_pl330_dmac *pdmac = devm_kzalloc(&adev->dev, sizeof(*pdmac), GFP_KERNEL);
  struct pl330_info *pi = &pdmac->pif;  pi->base = devm_ioremap_resource(&adev->dev, res);
  amba_set_drvdata(adev, pdmac);
  while (adev->irq[j] != -1)
    ret = request_irq(adev->irq[j], pl330_irq_handler, 0,dev_name(&adev->dev), pi); j++;
  |--> ret = pl330_add(pi); //
  num_chan = max_t(int, pdat->nr_valid_peri, pi->pcfg.num_chan); //设备定义，23
  pdmac->peripherals = kzalloc(num_chan * sizeof(*pch), GFP_KERNEL);
  for (i = 0; i < num_chan; i++) //初始化所有的通道
    pch = &pdmac->peripherals[i];
    INIT_LIST_HEAD(&pch->work_list);
    spin_lock_init(&pch->lock);
    pch->pl330_chid = NULL;
    pch->chan.device = pd;
    pch->dmac = pdmac;
    list_add_tail(&pch->chan.device_node, &pd->channels);
  struct dma_device *pd;
  pd = &pdmac->ddma;  	INIT_LIST_HEAD(&pd->channels);
  //定义pl330所有的ops接口
  |--> pd->device_alloc_chan_resources = pl330_alloc_chan_resources();//申请通道 static int pl330_alloc_chan_resources(struct dma_chan *chan)
    struct dma_pl330_chan *pch = to_pchan(chan);
    dma_cookie_init(chan);  pch->desc_type = NORMAL;
    pch->pl330_chid = pl330_request_channel(&pdmac->pif);
    tasklet_init(&pch->task, pl330_tasklet, (unsigned long) pch);
	|--> pd->device_free_chan_resources = pl330_free_chan_resources();//释放通道 struct dma_pl330_chan *pch = to_pchan(chan);
    struct dma_pl330_chan *pch = to_pchan(chan);
    tasklet_kill(&pch->task);
    pl330_release_channel(pch->pl330_chid);
  |--> pd->device_prep_slave_sg = pl330_prep_slave_sg();//struct dma_async_tx_descriptor *pl330_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len, enum dma_transfer_direction direction, unsigned long flg, void *context)
    struct dma_pl330_chan *pch = to_pchan(chan);  addr = pch->fifo_addr;
    for_each_sg(sgl, sg, sg_len, i)　//每个sg一般对应一个page,对应到一个desc.
      |--> desc = pl330_get_desc(pch);//struct dma_pl330_desc *pl330_get_desc(struct dma_pl330_chan *pch)
        struct dma_pl330_desc *desc = pluck_desc(pdmac);//从desc_pool中摘下一个desc
        desc->pchan = pch;  desc->txd.cookie = 0; async_tx_ack(&desc->txd);
        desc->req.peri = peri_id ? pch->chan.chan_id : 0;
        desc->rqcfg.pcfg = &pch->dmac->pif.pcfg;
        dma_async_tx_descriptor_init(&desc->txd, &pch->chan);
      if (direction == DMA_MEM_TO_DEV)
  			desc->rqcfg.src_inc = 1; desc->rqcfg.dst_inc = 0; desc->req.rqtype = MEMTODEV;
  			|--> fill_px(&desc->px, addr, sg_dma_address(sg), sg_dma_len(sg), sg_dma_len(sg));//void fill_px(struct pl330_xfer *px, dma_addr_t dst, dma_addr_t src, size_t len, size_t total_size)
          px->next = NULL;	px->bytes = len;	px->dst_addr = dst;	px->src_addr = src;	px->total_size = total_size;
  		else
  			desc->rqcfg.src_inc = 0; desc->rqcfg.dst_inc = 1; desc->req.rqtype = DEVTOMEM;
  			fill_px(&desc->px, sg_dma_address(sg), addr, sg_dma_len(sg), sg_dma_len(sg));
      desc->rqcfg.brst_size = pch->burst_sz; desc->rqcfg.brst_len = pch->burst_len; desc->rqcfg.swap = pch->byteswap;
    desc->txd.flags = flg;    return &desc->txd;
  |--> pd->device_control = pl330_control();//int pl330_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd, unsigned long arg)
    struct dma_pl330_chan *pch = to_pchan(chan);  struct dma_pl330_dmac *pdmac = pch->dmac;
    switch (cmd)
      case DMA_TERMINATE_ALL:
        pl330_chan_ctrl(pch->pl330_chid, PL330_OP_FLUSH);
        list_for_each_entry_safe(desc, _dt, &pch->work_list , node)
          desc->status = DONE;  list_move_tail(&desc->node, &list);
        list_splice_tail_init(&list, &pdmac->desc_pool);
      case DMA_SLAVE_CONFIG:
        slave_config = (struct dma_slave_config *)arg;  pch->byteswap = slave_config->byteswap;
        //then update : pch->fifo_addr  pch->burst_sz  pch->burst_len
	|--> pd->device_issue_pending = pl330_issue_pending();//void pl330_issue_pending(struct dma_chan *chan)
    struct dma_pl330_chan *pch = to_pchan(chan);
    pl330_tasklet((unsigned long) to_pchan(chan));
  |--> pd->device_dma_getposition = pl330_dma_getposition();//int pl330_dma_getposition(struct dma_chan *chan, dma_addr_t *src, dma_addr_t *dst)
    struct dma_pl330_chan *pch = to_pchan(chan);
    int ret = pl330_chan_status(pch->pl330_chid, &status);
    *src = status.src_addr; *dst = status.dst_addr;
	pd->device_prep_dma_memcpy = pl330_prep_dma_memcpy();
	pd->device_prep_dma_cyclic = pl330_prep_dma_cyclic();
  pd->device_prep_dma_lli = pl330_prep_dma_lli();
  pd->device_prep_slave_sg_pl022 = pl330_prep_slave_sg_pl022();
  pd->device_tx_status = pl330_tx_status();
  |--> ret = dma_async_device_register(pd);//int dma_async_device_register(struct dma_device *device)
    list_for_each_entry(chan, &device->channels, device_node)
      chan->local = alloc_percpu(typeof(*chan->local));
      chan->dev = kzalloc(sizeof(*chan->dev), GFP_KERNEL);
      chan->chan_id = chancnt++;
      rc = device_register(&chan->dev->device);
    device->chancnt = chancnt;


static int pl330_remove(struct amba_device *adev)
  struct dma_pl330_dmac *pdmac = amba_get_drvdata(adev);
  dma_async_device_unregister(&pdmac->ddma);
  amba_set_drvdata(adev, NULL);
  list_for_each_entry_safe(pch, _p, &pdmac->ddma.channels, chan.device_node)
    list_del(&pch->chan.device_node);
    pl330_control(&pch->chan, DMA_TERMINATE_ALL, 0);
    |--> pl330_free_chan_resources(&pch->chan);//void pl330_free_chan_resources(struct dma_chan *chan)
      struct dma_pl330_chan *pch = to_pchan(chan);
      tasklet_kill(&pch->task);
      pl330_release_channel(pch->pl330_chid); pch->pl330_chid = NULL;
    struct pl330_info *pi = &pdmac->pif;
    |--> pl330_del(pi);//void pl330_del(struct pl330_info *pi)
      struct pl330_dmac *pl330 = pi->pl330_data;     pl330->state = UNINIT;
      tasklet_kill(&pl330->tasks);
      |--> dmac_free_resources(pl330);//void dmac_free_resources(struct pl330_dmac *pl330)
        struct pl330_info *pi = pl330->pinfo;
        |--> dmac_free_threads(pl330);//int dmac_free_threads(struct pl330_dmac *pl330)
          struct pl330_info *pi = pl330->pinfo;
          for (i = 0; i < chans; i++)
            thrd = &pl330->channels[i];
            |--> pl330_release_channel((void *)thrd);//停止线程,执行两个req回调,释放event
              struct pl330_thread *thrd = ch_id;  struct pl330_dmac *pl330 = thrd->dmac;
              _stop(thrd);
              _callback(thrd->req[1 - thrd->lstenq].r, PL330_ERR_ABORT);
              _callback(thrd->req[thrd->lstenq].r, PL330_ERR_ABORT);
              _free_event(thrd, thrd->ev);    //-->pl330->events[ev] = -1;
              thrd->free = true;
          kfree(pl330->channels);
        dma_free_coherent(pi->dev, chans * pi->mcbufsz,pl330->mcode_cpu, pl330->mcode_bus);
      kfree(pl330); pi->pl330_data = NULL;
    j = -1; while (adev->irq[++j] != -1)  free_irq(adev->irq[j], pi);

static struct amba_driver pl330_driver = {
	.drv = {
		.owner = THIS_MODULE,
		.name = "dma-pl330",
	},
	.id_table = pl330_ids,
	.probe = pl330_probe,
	.remove = pl330_remove,
};
static int __init pl330_init(void)
  return amba_driver_register(&pl330_driver);
static void __exit pl330_exit(void)
  amba_driver_unregister(&pl330_driver);
```
### 1.2 dmaengine接口
```cpp

//#define dma_request_channel(mask, x, y) __dma_request_channel(&(mask), x, y)
struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask, dma_filter_fn fn, void *fn_param)
  chan = private_candidate(mask, device, fn, fn_param);
  err = dma_chan_get(chan);//-->chan->device->device_alloc_chan_resources(chan);

void dma_release_channel(struct dma_chan *chan)

//逆向解析引用::
int dmaengine_device_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd, unsigned long arg)
  <-- dmaengine_slave_config()
  <-- dmaengine_terminate_all()
  <-- dmaengine_pause()
  <-- dmaengine_resume()
//step 1:
int imapx_spi_dma_probe(struct imapx_spi *host)
	dma_cap_mask_t mask;	dma_cap_zero(mask);	dma_cap_set(DMA_SLAVE, mask);
	host->dma_rx_channel = dma_request_channel(mask,host->master_info->dma_filter,host->master_info->dma_rx_param);
	//dma_filter这个函数主要是查找你的dma传输的设备的请求信号线，其具体是在注册时填写的。这里会根据这个函数返回的真假来判断已经注册在总线上的dma slave的
	host->dma_tx_channel = dma_request_channel(mask,host->master_info->dma_filter,host->master_info->dma_tx_param);
	host->buffer = dma_alloc_coherent(host->dev, SCATTER_SIZE, &host->buffer_dma_addr, GFP_KERNEL);
//step 2:
int imapx_spi_configure_dma(struct imapx_spi *host)
	struct dma_chan *rxchan = host->dma_rx_channel;
	struct dma_async_tx_descriptor *rxdesc;
	struct dma_slave_config rx_conf = {	.src_addr = host->phy_base + SPI_FDR,	.direction = DMA_DEV_TO_MEM,　.device_fc = false,　};
	rx_conf.src_maxburst = 32;	rx_conf.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;
	rx_conf.byteswap = DMA_SLAVE_BYTESWAP_32;
	dmaengine_slave_config(rxchan, &rx_conf);//return chan->device->device_control(chan, DMA_SLAVE_CONFIG,(unsigned long)config);
	status = sg_alloc_table(&host->sgt_rx, sgs, GFP_ATOMIC);
	imapx_spi_setup_dma_scatter(host, host->buffer, host->cur_rx_len, 0, &host->sgt_rx);
	rx_sglen = dma_map_sg(rxchan->device->dev, host->sgt_rx.sgl, host->sgt_rx.nents, DMA_FROM_DEVICE);//通过这个函数来实现虚拟地址和物理地址的映射。
	//流式DMA映射：使用的内存来自于上层kmalloc()或者__get_free_pages(),而不是dma_alloc_coherent()，增加了cache的使无效和清除操作以解决cache一致性问题
	rxdesc = dmaengine_prep_slave_sg(rxchan, host->sgt_rx.sgl,rx_sglen, DMA_DEV_TO_MEM,DMA_PREP_INTERRUPT | DMA_CTRL_ACK);//return chan->device->device_prep_slave_sg(chan, sgl, sg_len, dir, flags, NULL);
	rxdesc->callback = imapx_spi_dma_callback;
	rxdesc->callback_param = host;
	dmaengine_submit(rxdesc);//return desc->tx_submit(desc);
//step 3:
int imapx_spi_start_dma(struct imapx_spi *host)
	dma_async_issue_pending(host->dma_rx_channel);//chan->device->device_issue_pending(chan);
//step 4:
void imapx_spi_dma_callback(void *data)
	dma_unmap_sg(host->dma_rx_channel->device->dev, host->sgt_rx.sgl,	host->sgt_rx.nents, DMA_FROM_DEVICE);//访问数据之前需要先unmap,否则需要调用dma_sync_sg_for_cpu();/dma_sync_sg_for_device();
	sg_free_table(&host->sgt_rx);
	//然后可以开始下一次传输
//step 5:
void imapx_spi_dma_remove(struct imapx_spi *host)
	imapx_spi_terminate_dma(host);
	dma_release_channel(host->dma_tx_channel);
	dma_release_channel(host->dma_rx_channel);
	dma_free_coherent(host->dev, SCATTER_SIZE, host->buffer, host->buffer_dma_addr);
```
### 1.2 dma mapping & scatterlist

```cpp
struct scatterlist {
	unsigned long	page_link;
	unsigned int	offset;
	unsigned int	length;
	dma_addr_t	dma_address;
//#ifdef CONFIG_NEED_SG_DMA_LENGTH
	unsigned int	dma_length;
//#endif
};
sg_init_one()
dma_map_single()
dma_alloc_coherent()
dma_free_coherent()

```
### 1.3 CMA

**为I/O申请内存映射**<br>
```cpp
//物理地址映射为虚拟地址
//step 1:
//#define request_mem_region(start,n,name) __request_region(&iomem_resource, (start), (n), (name), 0)//struct resource * __request_region(struct resource *,resource_size_t start,resource_size_t n,const char *name, int flags);
//step 2:
//#define ioremap(cookie,size)		__arm_ioremap((cookie), (size), MT_DEVICE)//void __iomem *__arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
//step 3:
//#define iounmap				__arm_iounmap//void __arm_iounmap(volatile void __iomem *io_addr)
//#define release_mem_region(start,n)	__release_region(&iomem_resource, (start), (n))//void __release_region(struct resource *, resource_size_t,resource_size_t);
```
**mmap()函数映射**<br>
```cpp
//mmap函数可以将用户空间的内存和设备内存关联
```


```cpp
//dma_mask
//通过调用dma_set_mask()来通知内核dma访问寻址能力限制：
int dma_set_mask(struct device *dev, u64 mask);
//通过调用dma_set_coherent_mask()通知内核一致性内存分配的限制。
int dma_set_coherent_mask(struct device *dev, u64 mask);

static u64 sdmmc0_dma_mask = DMA_BIT_MASK(32);
struct platform_device imap_mmc0_device = {
	.dev = {
		.platform_data = &imap_mmc0_platdata,
		.dma_mask = &sdmmc0_dma_mask,
		.coherent_dma_mask = DMA_BIT_MASK(32),
		},

};
```
	https://lwn.net/Articles/486301/

### 1.4 DMA缓存
DMA描述符需要CPU与设备同时操作。而数据收发缓冲区分配是流式的，随用随分配，用完释放后CPU才可以操作数据！
流式DMA缓冲区是cached，在map时刷了下cache，在设备DMA完成unmap时再刷cache（根据数据流向写回或者无效），来保证了cache数据一致性，在unmap之前CPU操作缓冲区是不能保证数据一致的。因此kernel需要严格保证操作时序。
当然kernel也提供函数dma_sync_single_for_cpu与dma_sync_single_for_device，可以在未释放时操作缓冲区，很明显这2个函数实现中肯定是再次进行刷新cache的操作保证数据一致性

cma是否是双重管理,buddy提供对其他应用的管理,bitmap提供对cma的管理.

每次分配之后,如果从一个大的order中拆分,那么伙伴部分留在伙伴系统,分配到的部分并不会完全被使用,这些剩余的部分以单个page为单位留在cpu的pcp链表中
	如果外部内存用尽,需要侵犯cma,那么,第一次申请了最大快之后,后续申请看不出继续使用这个最大快,而是继续申请新的最大快.

应该改动:
- 直接从cma中获取一个合适的大小就好,因为是MOVEABLE的
- 增加碎片管理,实在缺乏内存的时候整理碎片
- 另外,如果可回收的页 挤占了 可移动的页 的空间,可移动的页 可能就要取挤占 CMA的空间.这个链表反应需要考虑到优化方案中.

**__rmqueue_fallback** <br>
感觉先分配最大order的算法,也许不适合我们的场景.

```cpp
//感觉上,所有的cma的释放都是按照order 0,调用下面函数.应该是不需要进入伙伴系统的缘故吧.
void free_hot_cold_page(struct page *page, int cold)

//map的基本过程
dma_map_single ==> __dma_map_page ==> __dma_page_cpu_to_dev ==> ___dma_page_cpu_to_dev
//unmap的基本流程
dma_unmap_single ==> __dma_unmap_page ==> __dma_page_dev_to_cpu ==> ___dma_page_dev_to_cpu ==> dmac_unmap_area ==》v7_dmac_unmap_area
//
```

## 2



## 3 timer和时钟

## 4
